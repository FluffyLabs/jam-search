Element messages list:

id:
3224
timestamp:
2025-01-01T20:29:05.948Z
sender:
@tvvkk7:matrix.org
content:
Hello, I have a problem on slot key sequence(Eq. 6.24) and the fallback mode happens. In the condition, gamma_s' will be updated by the F function. I got the same result using testvectors of W3F, but got different result using test data of jam-duna. I use eta_2 in [epoch 425523_011](https://github.com/jam-duna/jamtestnet/blob/main/fallback/state_snapshots/425523_011.json) (prior slot key sequence). I use gamma_s' in [425523_000](https://github.com/jam-duna/jamtestnet/blob/main/fallback/state_snapshots/425524_000.json) (posterior key slot sequence).
According to Eq. 6.24, in the first slot, gamma_s' will be updated using eta_1. The other slots wii be gamma_s' = gamma_s if e' = e. So, the result should be gamma_s' is finally updated using eta_1. 
Is my understanding corect ?

id:
3223
timestamp:
2025-01-02T00:39:50.310Z
sender:
@tvvkk7:matrix.org
content:
* Hello, I have a problem on slot key sequence(Eq. 6.24) and the fallback mode happens. In the condition, gamma\_s' will be updated by the F function. I got the same result using testvectors of W3F, but got different result using test data of jam-duna. I use eta\_2 in [epoch 425523\_011](https://github.com/jam-duna/jamtestnet/blob/main/fallback/state_snapshots/425523_011.json) (prior slot key sequence). I use gamma\_s' in [425523\_000](https://github.com/jam-duna/jamtestnet/blob/main/fallback/state_snapshots/425524_000.json) (posterior key slot sequence).
According to Eq. 6.24, in the first slot, gamma\_s' will be updated using eta\_1. The other slots wii be gamma\_s' = gamma\_s if e' = e. So, the result should be gamma\_s' is finally updated using eta\_1.
Is my understanding incorrect ?

id:
3222
timestamp:
2025-01-02T02:23:42.723Z
sender:
@shwchg:matrix.org
content:
the files in state_snapshots is the post_state after applying the corresponding block. If you would like to use it as the pre_state for the next block, you should use eta_0

id:
1646
timestamp:
2025-01-02T05:31:31.496Z
sender:
@clw0908:matrix.org
content:
some questions about the following opcodes:  
135:  
https://graypaper.fluffylabs.dev/#/6e1c0cd/27d70327e303  
Since **vx** belongs to **N64**, should **vx** be mod 2^32 to ensure the input of **X4** won't overflow?  

136:  
https://graypaper.fluffylabs.dev/#/6e1c0cd/27f30327fc03  
Since **vx** belongs to **N64**, should **Z4(vx)** be replaced by **Z8(vx)**?  

id:
1645
timestamp:
2025-01-02T10:05:48.405Z
sender:
@gav:polkadot.io
content:
135: v_x belongs to N_32 - it’s constructed from at most 4 octets from the instruction data. 

id:
1644
timestamp:
2025-01-02T10:06:03.337Z
sender:
@gav:polkadot.io
content:
* v_x belongs to N_32 - it’s constructed from at most 4 octets from the instruction data. 

id:
1643
timestamp:
2025-01-02T10:06:59.384Z
sender:
@gav:polkadot.io
content:
* v_x belongs to N_32 - it’s constructed from at most 4 octets from the instruction data. (l_x is `min(…, 4)`)

id:
3221
timestamp:
2025-01-02T10:11:48.320Z
sender:
@tvvkk7:matrix.org
content:
Thanks a lot 

id:
1642
timestamp:
2025-01-02T12:12:02.019Z
sender:
@clw0908:matrix.org
content:
But the output of **X_lx**(**eq A.11**) belongs to **N_R** (**N_64**) 
https://graypaper.fluffylabs.dev/#/6e1c0cd/249501249601

Did I misunderstand something?

id:
1641
timestamp:
2025-01-02T12:32:50.697Z
sender:
@gav:polkadot.io
content:
true - this is probably the correction needed, but checking with Jan Bujak also: https://github.com/gavofyork/graypaper/pull/178

id:
1640
timestamp:
2025-01-02T12:37:40.857Z
sender:
@clw0908:matrix.org
content:
> <@gav:polkadot.io> true - this is probably the correction needed, but checking with Jan Bujak also: https://github.com/gavofyork/graypaper/pull/178

Alright, thanks for your clarification

id:
1639
timestamp:
2025-01-02T19:56:46.294Z
sender:
@tomusdrw:matrix.org
content:
I'm wondering about an edge case with arguments parsing in PVM. Some instructions simply use `ζı+1` (like [here](https://graypaper.fluffylabs.dev/#/6e1c0cd/26a60326a603)), but for immediates/offsets we use `ℓ` which is mask dependent (like [here](https://graypaper.fluffylabs.dev/#/6e1c0cd/262001262001)).

ζ is zero-padded so it's fine if we go beyond the program length, but what if `ζı+1` is actually the next instruction?

AFAICT from GP we should treat the instruction byte as argument (and read registers from it) and in the next step just move according to the mask (i.e. execute that instruction).
Is that expected or should there be some special casing for that in the GP?

id:
1638
timestamp:
2025-01-02T20:04:57.822Z
sender:
@tomusdrw:matrix.org
content:
In an edge case to that edge case we may have a program that is just instructions (i.e. all bits in the mask are set), but we are still reading it correctly, because we read the next instruction bytes as arguments to the current instruction (or 0s in case we go beyond the program length). I just want to confirm that I'm understanding it correctly.

id:
1637
timestamp:
2025-01-02T20:05:13.205Z
sender:
@tomusdrw:matrix.org
content:
* In an edge case to that edge case we may have a program that is just instructions (i.e. all bits in the mask are set), but we are still executing every instruction correctly , because we read the next instruction bytes as arguments to the current instruction (or 0s in case we go beyond the program length). I just want to confirm that I'm understanding it correctly.

id:
1636
timestamp:
2025-01-02T20:05:17.416Z
sender:
@tomusdrw:matrix.org
content:
* In an edge case to that edge case we may have a program that is just instructions (i.e. all bits in the mask are set), but we are still executing every instruction correctly, because we read the next instruction bytes as arguments to the current instruction (or 0s in case we go beyond the program length). I just want to confirm that I'm understanding it correctly.

id:
1635
timestamp:
2025-01-02T23:57:50.931Z
sender:
@subotic:matrix.org
content:
Not sure if I'm correct, but I understand the definitions in the GP as how the program *must* be encoded. So if there is no argument present where one should be as per GP, then the program is incorrect and we need to `panic`.

id:
1634
timestamp:
2025-01-03T00:50:37.033Z
sender:
@gav:polkadot.io
content:
> <@tomusdrw:matrix.org> I'm wondering about an edge case with arguments parsing in PVM. Some instructions simply use `ζı+1` (like [here](https://graypaper.fluffylabs.dev/#/6e1c0cd/26a60326a603)), but for immediates/offsets we use `ℓ` which is mask dependent (like [here](https://graypaper.fluffylabs.dev/#/6e1c0cd/262001262001)).
> 
> ζ is zero-padded so it's fine if we go beyond the program length, but what if `ζı+1` is actually the next instruction?
> 
> AFAICT from GP we should treat the instruction byte as argument (and read registers from it) and in the next step just move according to the mask (i.e. execute that instruction).
> Is that expected or should there be some special casing for that in the GP?

It’s perfectly fine to concoct programs which reuse a previous instruction’s argument as the next instruction’s opcode.  

id:
1633
timestamp:
2025-01-03T00:55:53.415Z
sender:
@jan:parity.io
content:
Basically what Gav said. The program code blob is just that, a blob of bytes. You have the instruction pointer which tells you at which position to decode an instruction, and you have the skip value which tells you how to increment the instruction pointer after that instruction is executed. And the skip value doesn't necessarily have to be the same as the "length" of the instruction.

id:
1632
timestamp:
2025-01-03T07:11:02.347Z
sender:
@tomusdrw:matrix.org
content:
Perfect, thanks for the confirmation.

id:
3220
timestamp:
2025-01-03T09:33:21.002Z
sender:
@vinsystems:matrix.org
content:
Will JAM be able to support distributed validator technology?

id:
3219
timestamp:
2025-01-03T10:47:39.177Z
sender:
@gav:polkadot.io
content:
Define "distributed validator technology"

id:
3218
timestamp:
2025-01-03T10:48:17.419Z
sender:
@clearloop:matrix.org
content:
hmm, interesting, in my implementation, the validator could be local and remote

id:
3217
timestamp:
2025-01-03T10:49:14.079Z
sender:
@clearloop:matrix.org
content:
* hmm, interesting, in my implementation, the validator could be local and remote, however, I think this kind of stuff is not proper for the current stage since we need to make the network work first XD

id:
3216
timestamp:
2025-01-03T11:12:42.706Z
sender:
@vinsystems:matrix.org
content:
Basically I mean the same (or similar) approach used in Ethereum, which spreads out key management and signing responsibilities across multiple parties to reduce single points of failure. 

id:
3215
timestamp:
2025-01-03T11:18:57.057Z
sender:
@vinsystems:matrix.org
content:
* Basically I mean the same (or similar) approach used in Ethereum, which spreads out key management across multiple parties to reduce single points of failure. 

id:
3214
timestamp:
2025-01-03T11:46:29.434Z
sender:
@gav:polkadot.io
content:
JAM makes no great requirements on how validators manage/use their keys.

id:
3213
timestamp:
2025-01-03T11:47:04.467Z
sender:
@gav:polkadot.io
content:
If you want to give a better definition than "will it be like Ethereum?", I'm happy to offer a more detailed commentary.

id:
3212
timestamp:
2025-01-03T11:47:38.006Z
sender:
@gav:polkadot.io
content:
* JAM makes no great requirements on how validators manage/use their keys, only that they provide an IPv6 endpoint and set of relevant crypto keys.

id:
3211
timestamp:
2025-01-03T11:48:53.356Z
sender:
@gav:polkadot.io
content:
* JAM makes no great requirements on how validators manage/store their secret keys, only that they provide an IPv6 endpoint and set of relevant crypto keys.

id:
3210
timestamp:
2025-01-03T13:12:19.079Z
sender:
@gav:polkadot.io
content:
* JAM makes no great requirements on how validators manage/store their secret keys, only that they provide an IPv6 endpoint and set of relevant public keys.

id:
1631
timestamp:
2025-01-03T21:35:22.383Z
sender:
@charliewinston14:matrix.org
content:
Hi. Question about block sealing (by ticket). Maybe I am missing something but tickets are generated using a ring signature so there doesn't seem to be a way to determine which ring member created it. But then how do we validate that when a block is received that it was created by the correct author that matches the ticket? couldn't any other member of the ring just create the same block and set themselves as the author?

id:
1630
timestamp:
2025-01-03T21:36:53.094Z
sender:
@charliewinston14:matrix.org
content:
* Hi. Question about block sealing (by ticket). Maybe I am missing something but tickets are generated using a ring signature so there doesn't seem to be a way to determine which ring member created it. But then how do we validate that when a block is received that it was created by the correct author that matches the ticket? couldn't any other member of the ring just create the same block and set themselves as the author (pretending the ticket was theirs)?

id:
1629
timestamp:
2025-01-03T21:46:55.174Z
sender:
@davxy:matrix.org
content:
> <@charliewinston14:matrix.org> Hi. Question about block sealing (by ticket). Maybe I am missing something but tickets are generated using a ring signature so there doesn't seem to be a way to determine which ring member created it. But then how do we validate that when a block is received that it was created by the correct author that matches the ticket? couldn't any other member of the ring just create the same block and set themselves as the author (pretending the ticket was theirs)?

The block author signs the block header using a standard Bandersnatch VRF signature. The VRF output generated from this signature matches the Ring VRF output, which serves as the ticket ID. This proves the block author's ownership of the ticket, as they are the only one capable of producing this specific output. For further details, please refer to the Bandersnatch VRF specification paper and examplea https://github.com/davxy/bandersnatch-vrfs-spec.


id:
1628
timestamp:
2025-01-03T21:51:08.144Z
sender:
@davxy:matrix.org
content:
> <@charliewinston14:matrix.org> Hi. Question about block sealing (by ticket). Maybe I am missing something but tickets are generated using a ring signature so there doesn't seem to be a way to determine which ring member created it. But then how do we validate that when a block is received that it was created by the correct author that matches the ticket? couldn't any other member of the ring just create the same block and set themselves as the author?

* The block author signs the block header using a standard Bandersnatch VRF signature. The VRF output generated from this signature matches the Ring VRF output, which serves as the ticket ID. This proves the block author's ownership of the ticket, as they are the only one capable of producing this specific output. For further details, please refer to the Bandersnatch VRF specification paper and example https://github.com/davxy/bandersnatch-vrfs-spec.

id:
3209
timestamp:
2025-01-04T04:36:03.577Z
sender:
@jakechen:matrix.org
content:
Hello, we are encountering an issue with header serialization. When the test data we use does not include epoch_mark and tickets_mark, we can correctly serialize the header data and compute the parent header hash for the next time slot using BLAKE2b-256. However, when the test data includes either epoch_mark or tickets_mark, the hash we calculate does not match the test data.
In addition to handling the discriminator encoding (Eq. C.9), does the serialization of epoch_mark and tickets_mark require any additional processing for the array data, such as epoch_mark.validators? 
We are troubleshooting using the test data files [425530_000.json](https://github.com/jam-duna/jamtestnet/blob/main/safrole/blocks/425530_000.json) and [425530_001.json](https://github.com/jam-duna/jamtestnet/blob/main/safrole/blocks/425530_001.json).

id:
1627
timestamp:
2025-01-04T07:35:47.972Z
sender:
@davxy:matrix.org
content:
> <@charliewinston14:matrix.org> Hi. Question about block sealing (by ticket). Maybe I am missing something but tickets are generated using a ring signature so there doesn't seem to be a way to determine which ring member created it. But then how do we validate that when a block is received that it was created by the correct author that matches the ticket? couldn't any other member of the ring just create the same block and set themselves as the author?

* The block author signs the block header using a "standard" Bandersnatch VRF signature. The VRF output generated from this signature matches the Ring VRF output, which serves as the ticket ID. This proves the block author's ownership of the ticket, as they are the only one capable of producing this specific output. For further details, please refer to the Bandersnatch VRF specification paper and example https://github.com/davxy/bandersnatch-vrfs-spec.

id:
3208
timestamp:
2025-01-05T06:50:14.247Z
sender:
@luke_fishman:matrix.org
content:
Amrit Jain: any news on this? how you tell them apart? i am wondering the same

id:
1626
timestamp:
2025-01-05T09:10:39.998Z
sender:
@luke_fishman:matrix.org
content:
couple of question please regarding chapter 14.4 (computation of work results)
1. in Eq. 14.11 => definition of I(p,j) => invocation of refine => the function [S](https://graypaper.fluffylabs.dev/#/6e1c0cd/1a87021a8702)(import segment data) is being called with two arguments, but in [14.14](https://graypaper.fluffylabs.dev/#/6e1c0cd/1b48001b4800) is defined only with single (work item) argument.
which one is it?
2. maybe related, who/what is the bold face **s** used by the functions J and S in 14.14? where does it come from?


Thank you

id:
1625
timestamp:
2025-01-05T11:15:52.795Z
sender:
@gav:polkadot.io
content:
1. You can disregard the bold-l parameter; it is assumed to be “part of the environment” in each of the functions which utilise L. 

id:
1624
timestamp:
2025-01-05T11:16:12.933Z
sender:
@gav:polkadot.io
content:
* 1. You can disregard the bold-l parameter; it is assumed to be “part of the environment” in each of the functions which utilise L and is not passed explicitly into it. 

id:
1623
timestamp:
2025-01-05T11:17:38.499Z
sender:
@gav:polkadot.io
content:
2. Bold-s is constrained as the correct operand to the merkle root equality. 

id:
3207
timestamp:
2025-01-05T11:20:12.799Z
sender:
@gav:polkadot.io
content:
> <@amritj:matrix.org> In graypaper here https://graypaper.fluffylabs.dev/#/911af30/19e30019e600
> 
> It is mentioned that import spec could contain both segment root and work package hash and to distinguish between them the work package hash value are tagged 
> 
> 
> But here according to jam official codec it is just a hash: 
> 
> https://github.com/w3f/jamtestvectors/blob/90fcf9020fa269a3aecc23feac0d09e7fee9123b/jam-types-asn/jam-types.asn#L121
> 
> So, how do we identify, if hash is tagged or not i.e. is it a segment root or work package hash?
> 
> 
> 

There’s a proper encoding given in C.29

id:
1622
timestamp:
2025-01-05T11:29:11.513Z
sender:
@luke_fishman:matrix.org
content:
understood, thank you.
follow up question, could you shed some light on how to tell the difference between H and h⊞?

i saw your answer in the other room about encoding in C.29 but that didn't clear it up for me.
are they both Y_32 or is there any "mark" to tell them apart?

id:
1621
timestamp:
2025-01-05T11:29:44.396Z
sender:
@luke_fishman:matrix.org
content:
* understood, thank you.
follow up question, could you shed some light on how to tell the difference between H and H⊞?

i saw your answer in the other room about encoding in C.29 but that didn't clear it up for me.
are they both Y\_32 or is there any "mark" to tell them apart?

id:
1620
timestamp:
2025-01-05T11:38:44.491Z
sender:
@gav:polkadot.io
content:
There’s a mark: it’s the little window thing. 

id:
1619
timestamp:
2025-01-05T11:42:16.430Z
sender:
@gav:polkadot.io
content:
One is drawn from the set Y_32 and the other from a bijective mapping, denoted by square-plus (that little window symbol) 

id:
1618
timestamp:
2025-01-05T11:52:25.173Z
sender:
@luke_fishman:matrix.org
content:
yes indeed there is the window thingy mark. that's not what i meant.
maybe it's a silly question.
the intention was in code how do i tell them apart
if they are both a 32 octets long binary, how could i tell if i'm looking at an H or and H⊞?

id:
1617
timestamp:
2025-01-05T11:56:42.321Z
sender:
@luke_fishman:matrix.org
content:
* yes indeed there is the window thingy mark. that's not what i meant.
maybe it's a silly question.
the intention was in code how do i tell them apart
if they are both a 32 octets long binary, how could i tell if i'm looking at an H or an H⊞?

id:
1616
timestamp:
2025-01-05T11:58:21.053Z
sender:
@gav:polkadot.io
content:
Well that all depends on what language you’re using. In general, you’ll need to use some extra memory to store whether it’s with or without the mark. 

id:
1615
timestamp:
2025-01-05T11:58:41.030Z
sender:
@gav:polkadot.io
content:
In rust you’d probably use a tagged enum. 

id:
1614
timestamp:
2025-01-05T11:58:54.000Z
sender:
@gav:polkadot.io
content:
* In rust you’d probably use a tagged union

id:
1613
timestamp:
2025-01-05T13:09:50.951Z
sender:
@luke_fishman:matrix.org
content:
Thank you Gav. I think I've got it

id:
3206
timestamp:
2025-01-05T14:58:48.453Z
sender:
@tvvkk7:matrix.org
content:
Hello, I wold like to know whether the condition if the report is good, but the work report hash is not in the fault. If this happens, do we need to take any measure like append the work report hash to fault ?

id:
3205
timestamp:
2025-01-05T14:59:19.712Z
sender:
@tvvkk7:matrix.org
content:
* Hello, I wold like to know whether the condition if the report is good, but the work report hash is not in the fault.(Eq. 10.13)  If this happens, do we need to take any measure like append the work report hash to fault ?

id:
3204
timestamp:
2025-01-05T15:21:55.870Z
sender:
@tvvkk7:matrix.org
content:
* Hello, I wold like to know whether the condition will happen if the report is good, but the work report hash is not in the fault.(Eq. 10.13)  If this happens, do we need to take any measure like append the work report hash to fault ?

id:
3203
timestamp:
2025-01-05T18:00:40.330Z
sender:
@gilescope:matrix.org
content:
C.6 Are there any test vectors for the length encoding? Is number 12 really encoded as 0x30 as some test vectors suggest? Sorry for asking but something's not quite right and I'm soaking up a lot of time on C.6.

id:
1612
timestamp:
2025-01-06T06:44:02.044Z
sender:
@luke_fishman:matrix.org
content:
gav:  follow up question please.
about the construction of the  segment-root dictionary l.
initially we have:
[K(l) ≡ {h S w ∈ pw , (h⊞, n) ∈ wi} , SlS ≤ 8](https://graypaper.fluffylabs.dev/#/6e1c0cd/1af0011af601)

this is clear enough, from the work item in a work package we take up to 8 hashes, of the kind that is the work-package hash. okie dokie.

then we come to :
 ∃p, c ∈ P, NC ∶ H(p) = h in [14.13](https://graypaper.fluffylabs.dev/#/6e1c0cd/1ac7021aca02)

which means to me one of two things
1.  we have here knowledge of "all" the work packages (all in what scope?) or some subset/list of work packages and we go through it to find the work-pacakges that the work-items refers to
2.we only keep in the segment-root dictionary the hashes that refer to the work-pacakge paseed into work result computation function Ξ. but since [s](https://graypaper.fluffylabs.dev/#/6e1c0cd/1b6c001b6c00) as far as i can see is not dependent on the core index c. we would end up with  a dictionary of all identical key -> value pairs


option2 doesn't make much sense, which brings me back yo option 1, but i see no reference to any list or set of work-packages




id:
1611
timestamp:
2025-01-06T06:46:05.596Z
sender:
@luke_fishman:matrix.org
content:
* gav:  follow up question please.
about the construction of the  segment-root dictionary l.
initially we have:
[K(l) ≡ {h S w ∈ pw , (h⊞, n) ∈ wi} , SlS ≤ 8](https://graypaper.fluffylabs.dev/#/6e1c0cd/1af0011af601)

this is clear enough, from the work item in a work package we take up to 8 hashes, of the kind that is the work-package hash. okie dokie.

then we come to :
∃p, c ∈ P, NC ∶ H(p) = h in [14.13](https://graypaper.fluffylabs.dev/#/6e1c0cd/1ac7021aca02)

which means to me one of two things

1. we have here knowledge of "all" the work packages (all in what scope?) or some subset/list of work packages and we go through it to find the work-pacakges that the work-items refers to

2. we only keep in the segment-root dictionary the hashes that refer to the work-pacakge paseed into work result computation function Ξ. so all the keys will be identical, H(p), but since [s](https://graypaper.fluffylabs.dev/#/6e1c0cd/1b6c001b6c00) as far as i can see is not dependent on the core index c. we would end up with  a dictionary of all identical key -> value pairs

option 2 doesn't make much sense, which brings me back yo option 1, but i see no reference to any list or set of work-packages

id:
1610
timestamp:
2025-01-06T06:50:01.783Z
sender:
@luke_fishman:matrix.org
content:
is there a video lecture of chapter 14 as part of the JAM tour? i could not find any

id:
3202
timestamp:
2025-01-06T07:15:51.445Z
sender:
@gav:polkadot.io
content:
> <@tvvkk7:matrix.org> Hello, I wold like to know whether the condition will happen if the report is good, but the work report hash is not in the fault.(Eq. 10.13)  If this happens, do we need to take any measure like append the work report hash to fault ?

Best to provide a link with the gray paper reader. 

id:
3201
timestamp:
2025-01-06T07:16:24.270Z
sender:
@gav:polkadot.io
content:
> <@gilescope:matrix.org> C.6 Are there any test vectors for the length encoding? Is number 12 really encoded as 0x30 as some test vectors suggest? Sorry for asking but something's not quite right and I'm soaking up a lot of time on C.6.

Which test vectors imply that?

id:
3200
timestamp:
2025-01-06T10:10:59.014Z
sender:
@gilescope:matrix.org
content:
> <@gav:polkadot.io> Which test vectors imply that?

https://github.com/w3f/jamtestvectors/blob/dc20cbce7d855974aa64301a4d952e22f277010f/safrole/tiny/skip-epoch-tail-1.json#L169 - the gamma_a is a variable length and is 12 long according to the json for the pre_state. But the byte is 0x30 when I look at the hex immediately before `11da6d1f761ddf9bdb4c9d6e5303ebd41f61858d0a5647a1a7bfe089bf921be9` in the corresponding scale file.

id:
3199
timestamp:
2025-01-06T10:26:20.102Z
sender:
@gav:polkadot.io
content:
That's probably an old test vector: 0x30 is a SCALE Compact integer encoding for 12.

id:
3198
timestamp:
2025-01-06T10:28:07.466Z
sender:
@gav:polkadot.io
content:
Best to use the vectors in https://github.com/w3f/jamtestvectors/pull/28

id:
1609
timestamp:
2025-01-06T10:28:40.340Z
sender:
@gav:polkadot.io
content:
Not yet. Expect one in two months

id:
3197
timestamp:
2025-01-06T10:30:28.730Z
sender:
@xlchen:matrix.org
content:
.scale is using the old/non-gp scale codec. you should reference .bin files

id:
1608
timestamp:
2025-01-06T10:30:40.129Z
sender:
@gav:polkadot.io
content:
Yes, as the guarantor you'll need to have seen the WRs of any WP hashes mention in the 

id:
1607
timestamp:
2025-01-06T10:30:47.572Z
sender:
@gav:polkadot.io
content:
* Yes, as the guarantor you'll need to have seen the WRs of any WP hashes mentioned in the SR Loookup.

id:
1606
timestamp:
2025-01-06T10:30:49.734Z
sender:
@gav:polkadot.io
content:
* Yes, as the guarantor you'll need to have seen the WRs of any WP hashes mentioned in the SR Lookup.

id:
1605
timestamp:
2025-01-06T10:31:23.946Z
sender:
@gav:polkadot.io
content:
* Yes, as the guarantor you'll need to have seen the WRs of any WP hashes mentioned in the SR Lookup, so you can be sure that it's correct otherwise you might end up guaranteeing WP with a WR which won't be accumulated.

id:
1604
timestamp:
2025-01-06T10:34:54.430Z
sender:
@gav:polkadot.io
content:
There only a limited about of WR history you'll need have knowledge of: https://graypaper.fluffylabs.dev/#/6e1c0cd/15de01152d02

id:
1603
timestamp:
2025-01-06T10:35:20.134Z
sender:
@gav:polkadot.io
content:
Basically just what will be in the recent blocks by the time of entering accumulation.

id:
1602
timestamp:
2025-01-06T10:35:28.748Z
sender:
@gav:polkadot.io
content:
* Basically just what will be in the recent blocks by the time of becoming available.

id:
1601
timestamp:
2025-01-06T10:36:55.456Z
sender:
@gav:polkadot.io
content:
So that's basically just U+H = 13.

id:
1600
timestamp:
2025-01-06T10:37:00.198Z
sender:
@gav:polkadot.io
content:
* So that's basically just U+H = 13 slots.

id:
1599
timestamp:
2025-01-06T10:38:23.149Z
sender:
@gav:polkadot.io
content:
* So that's basically just H = 8 slots

id:
1598
timestamp:
2025-01-06T10:39:28.275Z
sender:
@gav:polkadot.io
content:
This mechanism is designed to allow pipelining when sequences of unidependent work packages are executed on the same core.

id:
1597
timestamp:
2025-01-06T10:41:04.986Z
sender:
@gav:polkadot.io
content:
Without it, it would be hard to reference data in the DA without figuring out the SR manually which may not necessarily be known at the time of authoring the importing package.

id:
3196
timestamp:
2025-01-06T11:30:42.817Z
sender:
@piotrzwolinski:matrix.org
content:
Hello JAMmers!

At Fluffy Labs, we build tools like the [Gray Paper Reader](https://graypaper.fluffylabs.dev/) and [PVM Debugger](https://pvm.fluffylabs.dev/) to support JAM development, and we’d love your feedback! 🙌

👉 Anonymous feedback: https://docs.google.com/forms/d/e/1FAIpQLSducTy9dgPd0FG0QyWH-QKuolf1Sfp8I8kI2vdkKghuu82JoA/viewform
👉 Non-anonymous testimonial: https://docs.google.com/forms/d/e/1FAIpQLSfP2Yf0cfcb5vtaNzqJb40i-H5_gVxeTpWCLactlSmPMydxiQ/viewform

Thank you for helping us improve and support the JAM community! 🚀

id:
3195
timestamp:
2025-01-06T11:33:44.591Z
sender:
@tvvkk7:matrix.org
content:
Thanks, got it!

id:
3194
timestamp:
2025-01-06T11:50:13.462Z
sender:
@tvvkk7:matrix.org
content:
* Hello, I wold like to make sure whether the condition will happen if the report is good, but the work report hash is not in the fault. As gray paper defines [good reports should be in the fault](https://graypaper.fluffylabs.dev/#/6e1c0cd/12fb0212fb02) If any good report is not in fault, do we need to take any measure like append the work report hash to fault ?

id:
3193
timestamp:
2025-01-07T04:58:44.096Z
sender:
@amritj:matrix.org
content:
https://graypaper.fluffylabs.dev/#/911af30/368602368602

id:
1596
timestamp:
2025-01-07T10:52:15.302Z
sender:
@0xjunha:matrix.org
content:
I have a question regarding the leaf node encoding function (https://graypaper.fluffylabs.dev/#/911af30/37f402371103) for state merklization.

In a Merkle trie, a leaf node will be typically placed at a depth less than 256, implying that it should represent the remaining, unconsumed bits of the `state_key` after navigating to that point - so that we can guess which `state_key` that leaf node represents.

When considering what the leaf node holds as the encoded state key (`bits(k)...248`), should this refer to:

a) "The first 248 bits of the full 256-bit state key"? Or
b) "The first 248 bits of the unconsumed path bits"?

For example, let's say we have a state key like `0b1101_1010_1001...` and a leaf node is at depth 10, meaning we've navigated the trie using the path `0b1101_1010_10` to reach this node, should the leaf node:

a) Store the first 248 bits of `0b1101_1010_1001...`? Or
b) Only store the first 248 bits of `0b01...`, skipping the first 10 bits, thus excluding the part already used for navigation (`0b1101_1010_10`)? Actually in this case the remaining part is only 246 bits, so we have 2 bits of free space.

From my interpretation of the formalism, it seems the encoding takes the "first 248 bits of the full 256-bit state key" regardless of the leaf node's position (depth), so option a), but just wanted to confirm this understanding.

id:
1595
timestamp:
2025-01-07T13:55:30.484Z
sender:
@dave:parity.io
content:
Yes, (a) is correct

id:
1594
timestamp:
2025-01-07T14:18:05.133Z
sender:
@0xjunha:matrix.org
content:
David Emett: awesome, thanks for clarifying!

id:
3192
timestamp:
2025-01-08T04:19:25.745Z
sender:
@clw0908:matrix.org
content:
Hello, I have a question regarding the **codec** part in **jamtestvector**:  

I’m wondering whether the **"extrinsic_hash"** in **block.json** is calculated using [**GP-0.5.3 equations 5.4, 5.5, and 5.6**](https://graypaper.fluffylabs.dev/#/6e1c0cd/0cc6000cf200), or if it is still calculated using the older version of the function. Since the calculation method hasn't been modified for a long time, I’d like to confirm.  

[The commit that added block.json](https://github.com/w3f/jamtestvectors/commit/7a96598b7ad7d079288a0043b0240894c9a33864#diff-81e841f65f02445b042e5186d09d489695ef99a0408535e28b72ea60b2961e61) was made on **September 18, 2024**. Since then, there have been no further modifications to **"extrinsic_hash"**.  

id:
3191
timestamp:
2025-01-08T09:17:03.536Z
sender:
@tomusdrw:matrix.org
content:
> <@clw0908:matrix.org> Hello, I have a question regarding the **codec** part in **jamtestvector**:  
> 
> I’m wondering whether the **"extrinsic_hash"** in **block.json** is calculated using [**GP-0.5.3 equations 5.4, 5.5, and 5.6**](https://graypaper.fluffylabs.dev/#/6e1c0cd/0cc6000cf200), or if it is still calculated using the older version of the function. Since the calculation method hasn't been modified for a long time, I’d like to confirm.  
> 
> [The commit that added block.json](https://github.com/w3f/jamtestvectors/commit/7a96598b7ad7d079288a0043b0240894c9a33864#diff-81e841f65f02445b042e5186d09d489695ef99a0408535e28b72ea60b2961e61) was made on **September 18, 2024**. Since then, there have been no further modifications to **"extrinsic_hash"**.  

It doesn't really matter. The codec test vectors are not meant to be semantically correct. They are just testing if you can decode the data given some schema.

id:
3190
timestamp:
2025-01-08T09:29:31.945Z
sender:
@clearloop:matrix.org
content:
Thanks to your awesome work! both of the two tools are playing important role in our development!

id:
3189
timestamp:
2025-01-08T09:29:44.238Z
sender:
@clearloop:matrix.org
content:
* Thanks for your awesome work! both of the two tools are playing important role in our development!

id:
3188
timestamp:
2025-01-08T09:29:53.214Z
sender:
@clearloop:matrix.org
content:
* Thanks for your awesome work! both of the two tools are playing important roles in our development!

id:
1593
timestamp:
2025-01-08T15:43:33.364Z
sender:
@gav:polkadot.io
content:
@room Version 0.5.4 [released](https://github.com/gavofyork/graypaper/releases/tag/v0.5.4) with some important protocol alterations.

id:
1592
timestamp:
2025-01-08T15:46:37.899Z
sender:
@gav:polkadot.io
content:
Significant alterations:
* PVM Host-calls: Host-call to inspect preimage request status by @gavofyork in https://github.com/gavofyork/graypaper/pull/177
* PVM Invocations: Yielding accumulation hash possible on OOG by @gavofyork in https://github.com/gavofyork/graypaper/pull/179
* PVM: Bit manipulation (Zbb-inspired) extensions by @gavofyork in https://github.com/gavofyork/graypaper/pull/176
* Accumulation & PVM Invocations: Buddy ejects service to allow perfect service deletion by @gavofyork in https://github.com/gavofyork/graypaper/pull/182


id:
1591
timestamp:
2025-01-08T15:47:59.776Z
sender:
@gav:polkadot.io
content:
I expect [one further substantial alteration](https://github.com/gavofyork/graypaper/issues/186) in the [0.5 series](https://github.com/gavofyork/graypaper/milestone/1) and after that I think we'll be good until the 0.7/0.8 series.

id:
3187
timestamp:
2025-01-08T16:08:57.468Z
sender:
@gav:polkadot.io
content:
I think it'd be good to request treasury support for 2 or 3 deliberately-produced JAM (services and authorizer) SDKs, aside from the SDK produced by Parity).

The best way I see this of happening is to ask teams who have passed M1 to submit proposals for an SDK framework, giving code examples for the sort of thing their SDK would allow to write. I would then get some initial treasury funding of ~$100k, possibly just from the Fellowship Treasury. From the proposals, we could fund up to 10 teams ~$10k each to make initial prototypes of their proposals and 2 or 3 of the most promising prototypes could be further developed into proper SDKs.

id:
3186
timestamp:
2025-01-08T16:09:37.423Z
sender:
@gav:polkadot.io
content:
Is there any interest from teams close to M1 to put forward such a proposal and build a prototype?

id:
3185
timestamp:
2025-01-08T16:09:50.479Z
sender:
@gav:polkadot.io
content:
* I think it'd be good to request treasury support for 2 or 3 deliberately-produced JAM (services and authorizer) SDKs, aside from the SDK produced by Parity.

The best way I see this of happening is to ask teams who have passed M1 to submit proposals for an SDK framework, giving code examples for the sort of thing their SDK would allow to write. I would then get some initial treasury funding of ~$100k, possibly just from the Fellowship Treasury. From the proposals, we could fund up to 10 teams ~$10k each to make initial prototypes of their proposals and 2 or 3 of the most promising prototypes could be further developed into proper SDKs.

id:
3184
timestamp:
2025-01-08T17:37:59.606Z
sender:
@jaymansfield:matrix.org
content:
ima_2c6a278.png

id:
3183
timestamp:
2025-01-08T17:38:00.374Z
sender:
@jaymansfield:matrix.org
content:
Happy to share a glimpse of JavaJAM’s first testnet utilizing QUIC messaging. Still a lot of tweaks to go but feels good to get to this stage.

id:
3182
timestamp:
2025-01-08T18:11:40.982Z
sender:
@clearloop:matrix.org
content:
Hi there, we just introduced a simple general test vector runner, please feel free to try it out 

https://github.com/spacejam-network/specjam

id:
3181
timestamp:
2025-01-08T18:12:01.696Z
sender:
@clearloop:matrix.org
content:
* Hi there, we just introduced a simple general testvectors runner, please feel free to try it out!

https://github.com/spacejam-network/specjam

id:
3180
timestamp:
2025-01-08T18:15:12.879Z
sender:
@clearloop:matrix.org
content:
* Hi there, we just introduced a simple general testvectors runner, please feel free to try it out!

```
cargo install specjam
specjam dummy
```

https://github.com/spacejam-network/specjam

id:
3179
timestamp:
2025-01-08T18:21:33.746Z
sender:
@clearloop:matrix.org
content:
congratulations! 

btw may I ask are you queuing empty blocks or also have the transaction pool implementations as well XD

id:
1590
timestamp:
2025-01-08T19:54:51.419Z
sender:
@danicuki:matrix.org
content:
Getting back to an old question: I still didn't figure out what C(p) means in this formula. Here:

d ↦ [join(c) ∣ c <−T[C(p) ∣ p <−unzip684(d)]]

d is a binary with size multiple of both k and 684
I unzip684 d, so I get k binaries p of size 684
then I take each one of this binaries p and apply C(p) - what is C(p) ?  

id:
3178
timestamp:
2025-01-08T21:09:28.457Z
sender:
@gav:polkadot.io
content:
> <@jaymansfield:matrix.org> Happy to share a glimpse of JavaJAM’s first testnet utilizing QUIC messaging. Still a lot of tweaks to go but feels good to get to this stage.

Could you repost with a link someone other than here? Media inclusion doesn’t seem to work for me at all:(

id:
3177
timestamp:
2025-01-08T21:46:18.370Z
sender:
@jaymansfield:matrix.org
content:
> <@gav:polkadot.io> Could you repost with a link someone other than here? Media inclusion doesn’t seem to work for me at all:(

Realizing now I probably should have made it a video, but you can find the same screenshot here: https://x.com/javajamio/status/1877069057372881158?s=46&t=WteYIn3tg7gE1pCvJh1G2Q

id:
1589
timestamp:
2025-01-09T08:17:02.944Z
sender:
@amritj:matrix.org
content:
It is Reed-Solomon erasure coding Encoding function we are allowed to use external library for this, example - https://github.com/ordian/reed-solomon-simd/blob/7def877102661817b3c2d5bcfe85118ffb535245/README.md?plain=1#L94

id:
1588
timestamp:
2025-01-10T18:08:55.139Z
sender:
@danicuki:matrix.org
content:
I got it. Thanks for sharing this. I tried to use this library, but it accepts only shards with size that are multiple of 64 bytes. In case of JAM, shards are 684 bytes. Do you know how we could deal with this? 

Another question is:  684 / 1023 configuration would be for production network. What would be the testnet / tiny / small network configuration? 

id:
1587
timestamp:
2025-01-10T20:56:47.116Z
sender:
@danicuki:matrix.org
content:
About the Erasure Code definition, if C ∶ ⟦Y2⟧342 → ⟦Y2⟧1023, and p ∈ ⟦Y684⟧, is there a formal specification missing in formula H.6 to transform p ∈ ⟦Y684⟧ into ⟦Y2⟧342 ? 

id:
1586
timestamp:
2025-01-11T03:40:20.099Z
sender:
@amritj:matrix.org
content:
I am also facing this problem right now. I have temporarily padded my data to support 64 bytes and will fix it later.

There is also some discussion about this issue on GitHub here: https://github.com/w3f/jamtestvectors/pull/4

Maybe someone else has a concrete solution.


I believe the configuration for testnet, tiny, or small networks will remain the same because the data size will still be 684k. In all these networks, the same validator holds multiple pieces of the 1023 chunks.

id:
1585
timestamp:
2025-01-11T03:48:30.882Z
sender:
@amritj:matrix.org
content:
* I am also facing this problem right now. I have temporarily padded my data to support 64 bytes and will fix it later.

There is also a discussion about this issue on GitHub here: https://github.com/w3f/jamtestvectors/pull/4

Maybe someone else has a concrete solution.


I believe the configuration for testnet, tiny, or small networks will remain the same because the data size will still be 684k. In all these networks, the same validator holds multiple pieces of the 1023 chunks.

id:
1584
timestamp:
2025-01-11T17:32:06.506Z
sender:
@weigen:matrix.org
content:
Hi, I have a question about disputes. Based on my understanding, the faults (f) in the extrinsic include validators who issued invalid judgments on a work report. Since the work report is actually valid, it should belong to the good set (Psi_g).
This logic aligns with the behavior seen in this test vector (https://github.com/davxy/jam-test-vectors/blob/polkajam-vectors/disputes/tiny/progress_with_verdicts-4.json#L82, https://github.com/davxy/jam-test-vectors/blob/polkajam-vectors/disputes/tiny/progress_with_verdicts-4.json#L191), where work reports included in faults (f) are added to the good set of the posterior Psi. However, according to the Gray Paper (https://graypaper.fluffylabs.dev/#/579bd12/128a01128a01), the reports in faults (f) should instead be placed in the bad set (Psi_b) and not in the good set.

This creates a conflict: if the work reports in faults (f) are valid and should be part of the good set, why are they assigned to the bad set instead? My understanding is that validators who issued invalid judgments on these valid reports are placed in faults (f), but the reports themselves remain valid.

Am I interpreting this correctly, or have I missed something?

id:
1583
timestamp:
2025-01-12T10:42:08.900Z
sender:
@dave:parity.io
content:
I think you are interpreting the GP incorrectly. The judgments included in faults can be either positive (ie claiming the WR is good) or negative (ie claiming it is bad). The requirement you linked (https://graypaper.fluffylabs.dev/#/579bd12/128a01128a01) just states that they must contradict a verdict

id:
1582
timestamp:
2025-01-12T10:44:32.364Z
sender:
@dave:parity.io
content:
So eg if a WR is found to be bad, then validators who supported the WR (ie produced a positive judgment) can be reported via faults

id:
1581
timestamp:
2025-01-12T13:37:27.790Z
sender:
@clearloop:matrix.org
content:
can't we just use https://github.com/paritytech/erasure-coding/tree/main directly? for the testvector, I think it is incomplete or we'd better do it after the accumulation part, if I'm not mistaken, the testvectors required a log of external logic for assembling the chunks

id:
1580
timestamp:
2025-01-12T13:37:45.142Z
sender:
@clearloop:matrix.org
content:
* can't we just use https://github.com/paritytech/erasure-coding/tree/main directly? for the testvector, I think it is incomplete or we'd better do it after the accumulation part, if I'm not mistaken, the testvectors required a lot of external logic for assembling the chunks

id:
1579
timestamp:
2025-01-12T13:39:46.609Z
sender:
@clearloop:matrix.org
content:
* can't we just use https://github.com/paritytech/erasure-coding/tree/main directly? for the testvector, I think it is incomplete or we'd better do it after the accumulation part, if I'm not mistaken, the testvectors required a lot of external logic for assembling the data

id:
1578
timestamp:
2025-01-12T13:40:17.756Z
sender:
@clearloop:matrix.org
content:
* can't we just use https://github.com/paritytech/erasure-coding/tree/main directly? for the testvector, I think it is incomplete or we'd better do it after the accumulation part, if I'm not mistaken, the testvectors required a lot of external logic for assembling the chunks

id:
1577
timestamp:
2025-01-12T13:40:27.224Z
sender:
@weigen:matrix.org
content:
Thanks! I got it

id:
1576
timestamp:
2025-01-13T07:43:35.456Z
sender:
@danicuki:matrix.org
content:
> <@clearloop:matrix.org> can't we just use https://github.com/paritytech/erasure-coding/tree/main directly? for the testvector, I think it is incomplete or we'd better do it after the accumulation part, if I'm not mistaken, the testvectors required a lot of external logic for assembling the chunks

This parity example just takes a generic byte string and splits it into chunks. It still uses the Reed Solomon library, that only accepts strings with 64 bytes multiple size

id:
1575
timestamp:
2025-01-13T10:29:11.168Z
sender:
@dakkk:matrix.org
content:
In the INVOKE host-call (https://graypaper.fluffylabs.dev/#/579bd12/362401366b01) we are apparently setting to regs[7], both values of "Host-call result constants" (https://graypaper.fluffylabs.dev/#/579bd12/2c78022ca802) and values from "inner pvm invocations result codes" (https://graypaper.fluffylabs.dev/#/579bd12/2caa022cbd02). This would be incorrect in theory, and also in practice since the value "0" is associated to two different constants (Ok and Halt)

id:
1574
timestamp:
2025-01-13T12:22:19.592Z
sender:
@gav:polkadot.io
content:
No it wouldn’t. 

id:
1573
timestamp:
2025-01-13T12:22:59.525Z
sender:
@gav:polkadot.io
content:
The two tokens `OK` and `Halt` are each representations of zero. 

id:
1572
timestamp:
2025-01-13T12:24:26.968Z
sender:
@gav:polkadot.io
content:
If `invoke` returns zero in omega_7, then it unambiguously means `Halt`.

id:
1571
timestamp:
2025-01-13T12:25:14.066Z
sender:
@gav:polkadot.io
content:
You’re probably trying to associate a _type_ with omega_7. This would be an incorrect approach. 

id:
1570
timestamp:
2025-01-13T12:30:01.121Z
sender:
@dakkk:matrix.org
content:
yep, I was trying that. Thank you for clarification

id:
1569
timestamp:
2025-01-13T16:56:15.437Z
sender:
@ycc3741:matrix.org
content:
gav: 
Does the official source provide any test data for verifying whether each class has been correctly serialized according to the specifications in Appendix [C.2](https://graypaper.fluffylabs.dev/#/5b732de/361100361100) before performing the hash operation?

id:
1568
timestamp:
2025-01-13T16:57:34.940Z
sender:
@ycc3741:matrix.org
content:
We are currently encountering some issues and would like to verify whether the serialization is correct. Having the serialized results available would greatly facilitate development. Thank you!

id:
1567
timestamp:
2025-01-13T17:08:26.228Z
sender:
@jaymansfield:matrix.org
content:
> <@ycc3741:matrix.org> We are currently encountering some issues and would like to verify whether the serialization is correct. Having the serialized results available would greatly facilitate development. Thank you!

Just a tip in the meantime until there are official full block/state serialization vectors.. I found switching to parsing all of the *.bin vectors rather than the json equivalents helped me catch a few encoding issues.

id:
1566
timestamp:
2025-01-13T17:10:50.315Z
sender:
@dave:parity.io
content:
There are some serialization test vectors here: https://github.com/w3f/jamtestvectors/tree/master/codec

id:
1565
timestamp:
2025-01-13T17:30:24.819Z
sender:
@ycc3741:matrix.org
content:
Oh I got it thanks a lot 

id:
3176
timestamp:
2025-01-13T19:02:54.970Z
sender:
@tvvkk7:matrix.org
content:
Hello, I have a question about serialization.  We have to check the data types of the serialization target to choose which kind of the serialization we should use, is that right ? 
All the int types should be serialized using [equation C.5](https://graypaper.fluffylabs.dev/#/579bd12/360102360102). 
And only the up-down arrow serialization uses equation C.6 ?

id:
3175
timestamp:
2025-01-13T19:09:59.312Z
sender:
@tvvkk7:matrix.org
content:
* Hello, I have a question about serialization.  We have to check the data types of the serialization target to choose which kind of the serialization we should use, is that right ?
All the int types should be serialized using [equation C.5](https://graypaper.fluffylabs.dev/#/579bd12/360102360102).
And only the up-down arrow serialization uses equation C.6 ?

id:
3174
timestamp:
2025-01-13T19:10:21.866Z
sender:
@tvvkk7:matrix.org
content:
* Hello, I have a question about serialization.  We have to check the data types of the serialization target to choose which kind of the serialization we should use, is that right ?
All the int types should be serialized using [equation C.5](https://graypaper.fluffylabs.dev/#/579bd12/360102360102).
And only the up-down arrow serialization uses [equation C.6](https://graypaper.fluffylabs.dev/#/579bd12/365602365602) ?

id:
3173
timestamp:
2025-01-13T19:42:13.849Z
sender:
@gav:polkadot.io
content:
> <@tvvkk7:matrix.org> Hello, I have a question about serialization.  We have to check the data types of the serialization target to choose which kind of the serialization we should use, is that right ?
> All the int types should be serialized using [equation C.5](https://graypaper.fluffylabs.dev/#/579bd12/360102360102).
> And only the up-down arrow serialization uses equation C.6 ?

Yes that’s right. We might see C.6 used a bit more in other parts of them protocol as time goes on but for now you’re reading is correct. 

id:
3172
timestamp:
2025-01-13T19:42:19.872Z
sender:
@gav:polkadot.io
content:
> <@tvvkk7:matrix.org> Hello, I have a question about serialization.  We have to check the data types of the serialization target to choose which kind of the serialization we should use, is that right ? 
> All the int types should be serialized using [equation C.5](https://graypaper.fluffylabs.dev/#/579bd12/360102360102). 
> And only the up-down arrow serialization uses equation C.6 ?

* Yes that’s right. We might see C.6 used a bit more in other parts of them protocol as time goes on but for now your reading is correct. 

id:
3171
timestamp:
2025-01-13T20:03:37.501Z
sender:
@ycc3741:matrix.org
content:
thanks a lot. We've been a bit confused for a while.

id:
3170
timestamp:
2025-01-14T10:41:25.698Z
sender:
@gav:polkadot.io
content:
> <@tvvkk7:matrix.org> Hello, I have a question about serialization.  We have to check the data types of the serialization target to choose which kind of the serialization we should use, is that right ? 
> All the int types should be serialized using [equation C.5](https://graypaper.fluffylabs.dev/#/579bd12/360102360102). 
> And only the up-down arrow serialization uses equation C.6 ?

* Yes that’s right. We might see C.6 used a bit more in other parts of the protocol as time goes on but for now your reading is correct. 

id:
3169
timestamp:
2025-01-14T17:41:44.543Z
sender:
@yu2c:matrix.org
content:
Hello, does anyone know how to join [this call](https://x.com/danicuki/status/1879180907636232527) ?
I couldn't find any public invite information

id:
3168
timestamp:
2025-01-14T17:44:07.608Z
sender:
@dakkk:matrix.org
content:
good question

id:
3167
timestamp:
2025-01-14T17:52:29.936Z
sender:
@rustybot:matrix.org
content:
> <@yu2c:matrix.org> Hello, does anyone know how to join [this call](https://x.com/danicuki/status/1879180907636232527) ?
> I couldn't find any public invite information

How was this call organized? Is there another channel dedicated to JAM that I am not aware of? I completely missed it 



id:
3166
timestamp:
2025-01-14T17:55:08.202Z
sender:
@ycc3741:matrix.org
content:
We have no idea, either.

id:
3165
timestamp:
2025-01-14T17:55:18.236Z
sender:
@jaymansfield:matrix.org
content:
Same

id:
3164
timestamp:
2025-01-14T18:01:47.419Z
sender:
@rustybot:matrix.org
content:
It seems plenty of teams were in the loop about this meetup (https://x.com/danicuki/status/1879210677216379008?t=VCKJxGk4SEqM1uRGC0IFcA&s=19). So either I've been living under a rock, or there's a communication channel that some of us aren't privy to



id:
3163
timestamp:
2025-01-14T18:15:24.832Z
sender:
@jaymansfield:matrix.org
content:
* Also was not aware of any meetup

id:
3162
timestamp:
2025-01-14T18:42:04.801Z
sender:
@prematurata:matrix.org
content:
I didn't know as well 

id:
3161
timestamp:
2025-01-14T18:51:21.421Z
sender:
@oliver.tale-yazdi:parity.io
content:
It was in the JAM0 channel since it is not really canonical "Jam", just among implementors: https://matrix.to/#/!KKOmuUpvYKPcniwOzw:matrix.org?via=matrix.org&via=parity.io  
(not sure if you need an invite)

id:
3160
timestamp:
2025-01-14T19:08:15.564Z
sender:
@yuchun:matrix.org
content:
Can you invite me? thanks

id:
3159
timestamp:
2025-01-14T19:09:08.977Z
sender:
@yuchun:matrix.org
content:
* Could you invite me? thanks

id:
3158
timestamp:
2025-01-14T19:41:04.537Z
sender:
@danicuki:matrix.org
content:
The JAM0 channel was created during the Bangkok gathering. Nothing there is private, just we didn't wanted to spam the main JAM chat with conferences and meetups organization details. The meeting video will be shared with all implementors. Anyone is welcome to join.

About the twitter thread in particular, fill free to send me your blurb and X handle, and I add it to the thread.

id:
3157
timestamp:
2025-01-14T19:43:56.406Z
sender:
@danicuki:matrix.org
content:
Summary of the Meeting Outcomes:

1/ 💼 Meeting Frequency: We agreed on holding monthly meetings to synchronize our efforts and tackle obstacles together. This will ensure that everyone stays updated on progress and challenges.

2/ 🎥 Recording Sessions: The team discussed recording our calls for transparency and sharing knowledge. These recordings will be made available privately and potentially shared publicly for broader community engagement.

3/ 🗓️ Upcoming Meetup: A second JAM0 meetup is being planned in Lisbon, likely around May, coinciding with Ethereum Lisbon. This event will focus on collaboration and knowledge sharing among JAM implementers. Waiting for gav availability to define dates.

4/ 🛠️ Community Test Vectors: We emphasized the importance of developing community test vectors to streamline our testing processes. This will help ensure compatibility and foster collaboration among different implementations.

5/ 📈 Milestones Discussion: Teams shared progress on their individual milestones, with special attention on the transition from the first milestone to the more complex second milestone, which involves networking and consensus mechanisms.

6/ 🌍 Networking Implementation: There’s a strong interest in collaborating on networking implementations across teams, ensuring that different JAM clients can communicate effectively.

7/ 🤝 Overall, the meeting reinforced our commitment to collaboration, transparency, and continuous improvement within the JAM community. Together, we aim to drive innovation and success in our projects!

id:
3156
timestamp:
2025-01-15T02:51:03.124Z
sender:
@jay_ztc:matrix.org
content:
Can someone invite me to the JAM0 channel please? Thanks!

id:
3155
timestamp:
2025-01-15T11:21:32.723Z
sender:
@gav:polkadot.io
content:
I've seen a couple of proposals for JAM node RPCs:
- https://docs.jamcha.in/basics/rpc
- https://github.com/open-web3-stack/jam-rpc-spec?tab=readme-ov-file

id:
3154
timestamp:
2025-01-15T11:22:01.809Z
sender:
@gav:polkadot.io
content:
The first thing I'd like to say is _Death to RPCs_! 🫣

id:
3153
timestamp:
2025-01-15T11:22:53.134Z
sender:
@gav:polkadot.io
content:
In general, all node interaction should happen, as with `smoldot`, through an in-process (light) node

id:
3152
timestamp:
2025-01-15T11:22:54.204Z
sender:
@gav:polkadot.io
content:
* In general, all node interaction should happen, as with `smoldot`, through an in-process (light) node.

id:
3151
timestamp:
2025-01-15T11:23:01.513Z
sender:
@gav:polkadot.io
content:
* In general, all network interaction should happen, as with `smoldot`, through an in-process (light) node.

id:
3150
timestamp:
2025-01-15T11:23:06.692Z
sender:
@gav:polkadot.io
content:
* In general, all JAM interaction should happen, as with `smoldot`, through an in-process (light) node.

id:
3149
timestamp:
2025-01-15T11:23:41.906Z
sender:
@gav:polkadot.io
content:
However, I accept it's probably helpful to have some sort of RPC defined either for local interactions or development/debugging.

id:
3148
timestamp:
2025-01-15T11:23:53.038Z
sender:
@gav:polkadot.io
content:
So, a slight critique of these two.

id:
3147
timestamp:
2025-01-15T11:24:28.502Z
sender:
@gav:polkadot.io
content:
The first suffers from `jam_getServiceStorage` and, probably, `jam_getState`.

id:
3146
timestamp:
2025-01-15T11:24:41.822Z
sender:
@gav:polkadot.io
content:
The first, and probably the second, are not generally implementable.

id:
3145
timestamp:
2025-01-15T11:26:51.579Z
sender:
@gav:polkadot.io
content:
The protocol doesn't track inserted keys; the node would have to. This requires specialised logic and removes the possibility of light-client implementations and fast-syncing. Given that block-execution will likely take a very large portion of the 6 second block time, full-syncing a chain of any significance is going to be practically impossible, doubly so if you're also having to retain a database of all inserted keys as well as everything else.

id:
3144
timestamp:
2025-01-15T11:27:04.639Z
sender:
@gav:polkadot.io
content:
* The first RPC call, and probably the second also, are not generally implementable.

id:
3143
timestamp:
2025-01-15T11:27:27.018Z
sender:
@gav:polkadot.io
content:
Otherwise the first proposal looks pretty sensible.

id:
3142
timestamp:
2025-01-15T11:29:14.054Z
sender:
@gav:polkadot.io
content:
The second proposal has typos (never a good sign) and is not especially clear in meaning. In particular, `state_getKeys` and `state_getStorage` are not 100% clear on the meaning of the term "keys".

id:
3141
timestamp:
2025-01-15T11:30:08.030Z
sender:
@gav:polkadot.io
content:
* The second proposal has typos (never a good sign) and is not especially clear in meaning. In particular, `state_getKeys` and `state_getStorage` are not 100% clear on the meaning of the term "keys". If the meaning is the key as it appears in the JAM chain Merkle tree, then the calls are fine and make sense. If the meaning is the key as appearing in the `read`/`write` host-calls, then not.

id:
3140
timestamp:
2025-01-15T11:30:54.132Z
sender:
@gav:polkadot.io
content:
* The `jam_getServiceStorage` RPC call, and probably the other also, are not generally implementable.

id:
3139
timestamp:
2025-01-15T12:04:49.504Z
sender:
@gav:polkadot.io
content:
* The protocol doesn't track inserted keys; the node would have to. This requires specialised logic and a potentially very large shadow database and removes the possibility of light-client implementations and fast-syncing. Given that block-execution will likely take a very large portion of the 6 second block time, full-syncing a chain of any significance is going to be practically impossible, doubly so if you're also having to retain a database of all inserted keys as well as everything else.

id:
3138
timestamp:
2025-01-15T13:36:55.988Z
sender:
@danicuki:matrix.org
content:
JAM Community Call #1 video is live: https://www.youtube.com/watch?v=ghjvdHzA1P0


id:
1564
timestamp:
2025-01-15T14:52:31.959Z
sender:
@prasad-kumkar:matrix.org
content:
For state merklization, is there a recommended approach for data ordering before constructing the Merkle tree? While I noticed the state key construction function C appears to provide some ordering through key generation, I'm unsure if:

1. This is indeed meant to determine the ordering before Merkleization
2. If so, would service indices (s) create well-distributed keys since they are sparsely distributed 32-bit integers?

I initially thought of splitting data in half recursively (like traditional Merkle trees), but noticed JAM's approach might be different.

id:
1563
timestamp:
2025-01-15T16:02:23.735Z
sender:
@gav:polkadot.io
content:
This is a Merkle trie, so the keys (given by C) determine exactly the node structure of the tree. 

id:
1562
timestamp:
2025-01-15T16:03:44.329Z
sender:
@gav:polkadot.io
content:
You don’t get to decide. “Ordering” is moot as we never iterate. But if you really want an order (eg for RPC) then you can apply dictionary ordering to the keys. 

id:
1561
timestamp:
2025-01-15T16:05:42.890Z
sender:
@gav:polkadot.io
content:
Service indices are sparse, as are keys generally: the function C is designed to be sparse and mostly uniform. The tree’s implied node structure (by virtue of the commitment scheme) should be able to manage this perfectly well. 

id:
1560
timestamp:
2025-01-15T16:07:10.894Z
sender:
@gav:polkadot.io
content:
One reasonable question might be whether (or to what degree) keys/values should be kept in-memory, and whether (or to what degree) the nodes of the tree should be persisted and whether that persistence should be in-memory or on-disk.  

id:
1559
timestamp:
2025-01-15T16:08:36.145Z
sender:
@gav:polkadot.io
content:
For now I’d leave this for implementers to decide. I expect that getting to M4 or M5 will almost certainly need aggressive use of RAM to store/memoize one or both of these databases. 

id:
1558
timestamp:
2025-01-15T19:52:00.731Z
sender:
@jaymansfield:matrix.org
content:
A suggestion for CE-132.. it might be a good idea to allow multiple tickets to be submitted by a proxy in a single stream. The current specification works good for the tiny chain spec but it may not be the best performance wise when using the full chain spec. The full chain spec will have 2046 tickets to be distributed to everyone within approx. 22 minutes (half of the lottery time and allowing 3 min for connectivity changes). This will result in 90+ incoming streams per minute for each validator just to consume tickets.

id:
1557
timestamp:
2025-01-15T19:56:25.871Z
sender:
@dave:parity.io
content:
Might happen for the full network protocol, don't think it's particularly important though

id:
1556
timestamp:
2025-01-15T20:23:56.458Z
sender:
@mkchung:matrix.org
content:
For CE-134 "Work-package sharing", why is "slot" not included as part of the msg?
 

```
Guarantor -> Guarantor

--> Core Index ++ Segments-Root Mappings
--> Work-Package Bundle
--> FIN
<-- Work-Report Hash ++ Ed25519 Signature
<-- FIN
```

Perhaps this can be something like
```
--> Core Index ++ slot ++ Segments-Root Mappings
```
? 

id:
3137
timestamp:
2025-01-15T20:31:11.362Z
sender:
@xlchen:matrix.org
content:
sorry about the typos. you might be the first reviewer of it. it is the merkle key, not the original key

id:
3136
timestamp:
2025-01-15T20:31:43.795Z
sender:
@xlchen:matrix.org
content:
the idea is to expose the db directly for debugging purpose

id:
3135
timestamp:
2025-01-15T20:33:22.796Z
sender:
@xlchen:matrix.org
content:
it is designed for node developers and node operators, not for end users/dapps, which as you suggested should use light clients not rpc

id:
3134
timestamp:
2025-01-15T21:26:18.254Z
sender:
@tomusdrw:matrix.org
content:
Are there any estimates when `jam-sdk` supporting `0.5.4` opcodes will be released? Or maybe there is any way to contribute to make this happen? :)

id:
3133
timestamp:
2025-01-15T21:26:33.231Z
sender:
@gav:polkadot.io
content:
Tomorrow. 

id:
3132
timestamp:
2025-01-15T21:26:49.541Z
sender:
@gav:polkadot.io
content:
Actually…

id:
3131
timestamp:
2025-01-15T21:27:16.687Z
sender:
@gav:polkadot.io
content:
Are they not already in place in the latest release from a few days ago?

id:
3130
timestamp:
2025-01-15T21:27:31.004Z
sender:
@gav:polkadot.io
content:
If not then I guess it’ll depend on [@jan:parity.io](https://matrix.to/#/@jan:parity.io)

id:
1555
timestamp:
2025-01-15T21:27:46.719Z
sender:
@dave:parity.io
content:
Not really necessary as it stands, the recipient can just accept or reject based on whether there is an appropriate core assignment or not. What an appropriate assignment is probably needs to be specified in more detail to ensure different implementations work well together, but will likely just be based on the current time and state at the head of the chain

id:
3129
timestamp:
2025-01-15T21:33:50.527Z
sender:
@tomusdrw:matrix.org
content:
let me double check, but afaict last release was 7 days ago and last time I checked it was producing pre-0.5.4 opcodes

id:
3128
timestamp:
2025-01-15T21:40:50.230Z
sender:
@tomusdrw:matrix.org
content:
yeah, `jam-pvm-build=0.1.9` seems to be producing a bunch of jumps initially and then a bunch of `113` instructions which used to be `STORE_IND_U64` and now is invalid.

id:
1554
timestamp:
2025-01-16T05:02:38.130Z
sender:
@clearloop:matrix.org
content:
thanks! I have skipped the types specified in erasure_coding after seeing `parity/erasure_coding` , will get it back these days!

id:
3127
timestamp:
2025-01-16T05:41:55.371Z
sender:
@jan:parity.io
content:
We will cut a release today.

id:
3126
timestamp:
2025-01-16T10:07:28.047Z
sender:
@clearloop:matrix.org
content:
cat input?

id:
3125
timestamp:
2025-01-16T10:07:52.723Z
sender:
@boymaas:matrix.org
content:
qwer(ty)?

id:
3124
timestamp:
2025-01-16T10:41:31.107Z
sender:
@danicuki:matrix.org
content:
cat input

id:
3123
timestamp:
2025-01-16T11:33:59.333Z
sender:
@gav:polkadot.io
content:
polkajam-sdk v0.1.10 is released on crates.io https://docs.rs/jam-pvm-common/latest/jam_pvm_common/index.html

id:
3122
timestamp:
2025-01-16T12:24:13.596Z
sender:
@gav:polkadot.io
content:
* polkajam-sdk v0.1.12 is released on crates.io https://docs.rs/jam-pvm-common/latest/jam\_pvm\_common/index.html

id:
1553
timestamp:
2025-01-17T16:29:44.924Z
sender:
@cisco:parity.io
content:
If [this function](https://graypaper.fluffylabs.dev/#/579bd12/241901241b01) returns B_{8n}, shouldn't it say "forall i in N_{8n}" instead of "...in N_{2^{8n}}"?

id:
1552
timestamp:
2025-01-17T18:15:30.261Z
sender:
@gav:polkadot.io
content:
> <@cisco:parity.io> If [this function](https://graypaper.fluffylabs.dev/#/579bd12/241901241b01) returns B_{8n}, shouldn't it say "forall i in N_{8n}" instead of "...in N_{2^{8n}}"?

Yes. Will be fixed in next revision

id:
1551
timestamp:
2025-01-17T18:22:07.433Z
sender:
@gav:polkadot.io
content:
https://github.com/gavofyork/graypaper/pull/194

id:
3121
timestamp:
2025-01-17T18:48:17.135Z
sender:
@danicuki:matrix.org
content:
I've created an open source repo with some JAM media assets. It might be helpful to use on presentations, social media posts, etc. Contributions are welcome: https://github.com/jamixir/jam-media

id:
3120
timestamp:
2025-01-17T19:14:13.352Z
sender:
@jaymansfield:matrix.org
content:
> <@danicuki:matrix.org> I've created an open source repo with some JAM media assets. It might be helpful to use on presentations, social media posts, etc. Contributions are welcome: https://github.com/jamixir/jam-media

Thanks for this. Definitely useful.

id:
1550
timestamp:
2025-01-19T10:18:05.828Z
sender:
@vinsystems:matrix.org
content:
Question about the [schema.asn](https://github.com/subotic/polkavm/blob/subotic_add_64_bit_test_programs/tools/spectool/spec/schema.asn#L52) of PVM test programs: "expected-status" is described as "the status code of the execution, i.e. the way the program is supposed to end". 

1.- Is this the GP exit reason? (halt, panic, out of gas, page fault, hostcall fault)
2.- If (1) is yes, should "trap" be swapped by "panic" and the other exit reasons added to "expected-status"?

id:
1549
timestamp:
2025-01-19T10:20:19.996Z
sender:
@vinsystems:matrix.org
content:
* Question about the [schema.asn](https://github.com/subotic/polkavm/blob/subotic_add_64_bit_test_programs/tools/spectool/spec/schema.asn#L52) of PVM test programs: "expected-status" is described as "the status code of the execution, i.e. the way the program is supposed to end".

1.- Is this the GP exit reason? (halt, panic, out of gas, page fault, hostcall fault)
2.- If (1) is yes, should "trap" be replaced by "panic" and the other exit reasons added to "expected-status"?

id:
1548
timestamp:
2025-01-19T10:21:10.569Z
sender:
@vinsystems:matrix.org
content:
* Question about the [schema.asn](https://github.com/subotic/polkavm/blob/subotic_add_64_bit_test_programs/tools/spectool/spec/schema.asn#L52) of PVM test programs: "expected-status" is described as "the status code of the execution, i.e. the way the program is supposed to end".

1.- Is this the GP exit reason? (halt, panic, out of gas, page fault, hostcall fault)
2.- If (1) is yes, should "trap" be replaced by "panic" and the other exit reasons be added to "expected-status"?

id:
3119
timestamp:
2025-01-19T17:41:58.680Z
sender:
@celadari:matrix.org
content:
Hi everyone,

I have a question regarding the Safrole test vectors. Looking at this commit:
🔗 [Commit 2d71c3f](https://github.com/davxy/jam-test-vectors/commit/2d71c3f76ac89aaf57abd892cc691841fa225c87)

I noticed that the ring root values were changed, even for test vectors without offenders.

❓ Can someone clarify what changed at that point in the computation of the ring root?

At the moment, we are using the following code (this part is from primitives, so I can share it here in the thread)

id:
3118
timestamp:
2025-01-19T17:42:38.813Z
sender:
@celadari:matrix.org
content:
```rust
let ring_size = public_keys.len();
    let ring_ctx = ring_context(ring_size, pcs_params_file_path)?;
    let padding_point = Public::from(ring_ctx.padding_point());

    let mut keys: Vec<Public<BandersnatchSha512Ell2>> = Vec::new();
    for key in public_keys {
        let mut cursor = Cursor::new(&key);
        let pub_key = Public::<BandersnatchSha512Ell2>::deserialize_compressed(&mut cursor)
            .unwrap_or_else(|_| padding_point);
        keys.push(pub_key);
    }


    let pts: Vec<_> = keys.iter().map(|pk| pk.0).collect();
    let verifier_key = ring_ctx.verifier_key(&pts);
    let commitment = verifier_key.commitment();

    let mut buf = Vec::new();
    commitment
        .serialize_compressed(&mut buf)
        .map_err(|_| VRFResult::SerializationError)?;

    Ok(buf)
```

id:
3117
timestamp:
2025-01-19T17:51:32.573Z
sender:
@vinsystems:matrix.org
content:
Maybe [this](https://github.com/davxy/jam-test-vectors/blob/polkajam-vectors/safrole/README.md?plain=1#L23)

id:
3116
timestamp:
2025-01-19T18:03:58.734Z
sender:
@celadari:matrix.org
content:
> <@vinsystems:matrix.org> Maybe [this](https://github.com/davxy/jam-test-vectors/blob/polkajam-vectors/safrole/README.md?plain=1#L23)

This is the version of ark-ec-vrfs but what changed because my code uses ark-ec-vrfs and even updating the dependency didn't do it 🥲

id:
3115
timestamp:
2025-01-19T18:17:44.594Z
sender:
@vinsystems:matrix.org
content:
Also changed [this](https://github.com/davxy/bandersnatch-vrfs-spec/blob/bdd1f4b7ccbad9227dc28c72660d438cf00b1b33/assets/example/src/main.rs#L38) data file 

id:
3114
timestamp:
2025-01-19T18:42:22.889Z
sender:
@celadari:matrix.org
content:
I took the one in Davide's repo jamtestvectors but I'm gonna try tomorrow the one in this repo you pointed out

id:
3113
timestamp:
2025-01-19T19:10:50.772Z
sender:
@danicuki:matrix.org
content:
Some people noticed that the Polkadot logo used for JAM/Polkadot mix is an old version of Polkadot logo. gav Should we update it using the new one?

id:
3112
timestamp:
2025-01-19T19:12:25.586Z
sender:
@piotrzwolinski:matrix.org
content:
Hi!

We have finalized the first "official release" of the Gray Paper Reader (https://graypaper.fluffylabs.dev), which most of you probably know.

We believe this tool will serve as a living documentation resource even beyond the JAM Contest, supporting ongoing development and understanding of JAM.

To keep the Reader freely available and open-source, we’re **submitting a proposal for retroactive funding** to the Polkadot Treasury.

We’ve started a discussion on Polkassembly to share details about the proposal and gather feedback: https://polkadot.polkassembly.io/post/2722. Your thoughts and questions are welcome!

id:
3111
timestamp:
2025-01-19T19:15:11.452Z
sender:
@boymaas:matrix.org
content:
Great work! ❤️ I’ve been using it with pleasure much appreciated.

id:
3110
timestamp:
2025-01-19T19:58:33.368Z
sender:
@celadari:matrix.org
content:
> <@vinsystems:matrix.org> Also changed [this](https://github.com/davxy/bandersnatch-vrfs-spec/blob/bdd1f4b7ccbad9227dc28c72660d438cf00b1b33/assets/example/src/main.rs#L38) data file 

What is the code you use to generate ring root commitment ?

id:
1547
timestamp:
2025-01-20T05:50:22.068Z
sender:
@jan:parity.io
content:
1. Yes.
2. "trap" is the same as  "panic"

id:
3109
timestamp:
2025-01-20T07:56:34.891Z
sender:
@vinsystems:matrix.org
content:
```
let ring_set: Vec<Public> = bandersnatch_keys;
let verifier = Verifier::new(ring_set);
verifier.commitment.serialize_compressed(&mut proof[..]).unwrap();
return proof;
```

id:
1546
timestamp:
2025-01-20T09:57:23.411Z
sender:
@vinsystems:matrix.org
content:
It's a bit confusing because "trap" is an instruction and "panic" is an exit reason.

What do you do when a page fault ocurrs? Do you store the lowest address to access in a status register before executing the trap instruction to switch from PVM mode to JAM kernel mode?

id:
1545
timestamp:
2025-01-20T12:51:20.126Z
sender:
@jan:parity.io
content:
I will align the naming in the next version of the test vectors to make it less confusing.

id:
1544
timestamp:
2025-01-20T12:55:59.200Z
sender:
@jan:parity.io
content:
For toplevel PVMs a page fault is no different than executing a "trap" instruction or any other condition which would result in a "panic" exit reason, and for those you don't even need to know the address of the page that faulted.

For inner PVMs (those are PVMs triggered with the `invoke` hostcall) a page fault will interrupt the execution of the inner PVM and should return the address of the page which triggered the fault.

id:
1543
timestamp:
2025-01-20T13:00:52.809Z
sender:
@jan:parity.io
content:
And in case you're wondering, the store instructions are atomic, so for example if the inner PVM tries to write 4 bytes into the memory, and the first two bytes end up at the 2 last bytes of page N (which was already faulted) and the last two bytes end up at the 2 first bytes of page N +1 (which was not faulted) then such a write will *only* trigger a page fault with the address of the N + 1 page and memory will not be modified.

id:
1542
timestamp:
2025-01-20T18:01:03.658Z
sender:
@sourabhniyogi:matrix.org
content:
Small Clarification question about preimages included in the genesis state ([example](https://github.com/jam-duna/jamtestnet/blob/main/fallback/state_transitions/455223_000.json#L9C24-L9C90)):  To be conformant to GP, should it include a matching a_l?  Even though no preimage was ever "requested"?





id:
1541
timestamp:
2025-01-20T19:30:57.248Z
sender:
@sourabhniyogi:matrix.org
content:
* Small Clarification question about preimages included in the genesis state ([example](https://github.com/jam-duna/jamtestnet/blob/main/fallback/state_transitions/455223_000.json#L9C24-L9C90)):  To be conformant to GP, should it include a matching [a\_l](https://graypaper.fluffylabs.dev/#/579bd12/387003387003) in the state trie _even though no preimage was ever "requested"_?  If so, what would (9.7) [link](https://graypaper.fluffylabs.dev/#/579bd12/11aa0011aa00) prescribe as its  value?

id:
1540
timestamp:
2025-01-20T19:31:57.980Z
sender:
@sourabhniyogi:matrix.org
content:
* Small Clarification question about preimages included in the genesis state ([example](https://github.com/jam-duna/jamtestnet/blob/main/fallback/state_transitions/455223_000.json#L9C24-L9C90)):  To be conformant to GP, should it include a matching [a\_l](https://graypaper.fluffylabs.dev/#/579bd12/387003387003) in the state trie _even though no preimage was ever "requested"_?  If so, what would (9.7) ([GP link](https://graypaper.fluffylabs.dev/#/579bd12/11aa0011aa00)) prescribe as its $[x]$ value?

id:
1539
timestamp:
2025-01-20T22:04:39.998Z
sender:
@gav:polkadot.io
content:
[0] perhaps?

id:
3108
timestamp:
2025-01-20T22:19:13.178Z
sender:
@decentration:matrix.org
content:
can i confirm which hashing algorithm we are using for mmr peak hashes? im assuming sha2-256 by default.


id:
3107
timestamp:
2025-01-20T22:20:56.817Z
sender:
@dave:parity.io
content:
MMR uses Keccak, most other things use Blake2b

id:
3106
timestamp:
2025-01-21T07:13:23.994Z
sender:
@celadari:matrix.org
content:
- In my case, Verifier has only method `verify`

can you tell what is your import please for Verifier ?

- Also, shouldn't you also handle padding (array of 0 is not a valid bandersnatch curve point) ?

id:
3105
timestamp:
2025-01-21T07:30:39.086Z
sender:
@vinsystems:matrix.org
content:
Im importing the bandersnatch-vrfs-spec example, the commitment method is [here](https://github.com/davxy/bandersnatch-vrfs-spec/blob/main/assets/example/src/main.rs#L120-L126)

Yes Im also handling padding and it's supposed to be in the "ring_set" vector of above pseudocode.

id:
3104
timestamp:
2025-01-21T07:35:32.260Z
sender:
@vinsystems:matrix.org
content:
* Im importing the bandersnatch-vrfs-spec example, the commitment method is [here](https://github.com/davxy/bandersnatch-vrfs-spec/blob/main/assets/example/src/main.rs#L120-L126)

Yes Im also handling padding and it's in the "ring\_set" vector of above pseudocode.

id:
3103
timestamp:
2025-01-21T07:36:20.974Z
sender:
@vinsystems:matrix.org
content:
* Im importing the bandersnatch-vrfs-spec example, the commitment method is [here](https://github.com/davxy/bandersnatch-vrfs-spec/blob/main/assets/example/src/main.rs#L120-L126)

Yes Im also handling padding and it's in the "ring\_set" vector of the above pseudocode.

id:
1538
timestamp:
2025-01-21T12:29:02.136Z
sender:
@sourabhniyogi:matrix.org
content:
Question on JAM DA throughput: How does the model referenced in [Sec 20](https://graypaper.fluffylabs.dev/#/579bd12/1f90011f9001) arrive at the distributed availability of 852MB/s?  Simplest model based on [W_B=12MB](https://graypaper.fluffylabs.dev/#/579bd12/418d00418d00) (max encoded work package size, derived from bandwidth considerations I think?) with 6s on guarantee and 6s on assurance yields 1MB/s per core x 341 cores = 341MB/s -- so what accounts for the difference?  

id:
1537
timestamp:
2025-01-21T13:24:42.451Z
sender:
@dave:parity.io
content:
JAM is pipelined so peak throughput is 341 WPs per block, not per 2 blocks. Don't know exactly where the 852MB/s number comes from, but the bundle size does not include exported segments so possibly that

id:
1536
timestamp:
2025-01-21T13:24:59.137Z
sender:
@dave:parity.io
content:
* JAM is pipelined so peak throughput is 341 WPs per block, not per 2 blocks. Don't know exactly where the 852MB/s number comes from, but the bundle size does not include exported segments so possibly that is the other missing bit

id:
1535
timestamp:
2025-01-21T13:32:38.078Z
sender:
@gav:polkadot.io
content:
That’s an old figure even WPs could be 15 mb. 

id:
1534
timestamp:
2025-01-21T13:32:53.922Z
sender:
@gav:polkadot.io
content:
Now that they’re 12, it’s 682MB/s

id:
3102
timestamp:
2025-01-21T14:00:24.543Z
sender:
@celadari:matrix.org
content:
So far that's also how we do 🤔

- do you do like this to generate the keys ?
```rust
let padding_point = Public::from(ring_ctx.padding_point());

    let mut keys: Vec<Public<BandersnatchSha512Ell2>> = Vec::new();
    for key in public_keys {
        let mut cursor = Cursor::new(&key);
        let pub_key = Public::<BandersnatchSha512Ell2>::deserialize_compressed(&mut cursor)
            .unwrap_or_else(|_| padding_point);
        keys.push(pub_key);
    }
```


id:
3101
timestamp:
2025-01-21T14:01:00.325Z
sender:
@celadari:matrix.org
content:
* So far that's also how we do 🤔

- do you do like this to generate the keys ?

```rust
let padding_point = Public::from(ring_ctx.padding_point());

let mut keys: Vec<Public<BandersnatchSha512Ell2>> = Vec::new();
for key in public_keys {
        let mut cursor = Cursor::new(&key);
        let pub_key = Public::<BandersnatchSha512Ell2>::deserialize_compressed(&mut cursor)
            .unwrap_or_else(|_| padding_point);
        keys.push(pub_key);
    }
```

id:
3100
timestamp:
2025-01-21T14:01:16.398Z
sender:
@celadari:matrix.org
content:
* So far that's also how we do 🤔

- do you do like this to generate the keys ?

```rust
let padding_point = Public::from(ring_ctx.padding_point());

let mut keys: Vec<Public<BandersnatchSha512Ell2>> = Vec::new();
for key in public_keys {
        let mut cursor = Cursor::new(&key);
        let pub_key = Public::<BandersnatchSha512Ell2>::deserialize_compressed(&mut cursor)
            .unwrap_or_else(|_| padding_point);
        keys.push(pub_key);
}
```

id:
3099
timestamp:
2025-01-21T20:28:08.029Z
sender:
@vinsystems:matrix.org
content:
It's something like (I didn't test it): 

```
let padding_point = Public::from(ring_ctx.padding_point());
let public_keys: [u8; 32] = bandersnatch_keys;
let mut ring_set: Vec<Public> = Vec::new();

for key in public_keys.iter() {
        let point = Public::deserialize_compressed(key)
            .unwrap_or_else(|_| padding_point);
        ring_set.push(point);
}
let verifier = Verifier::new(ring_set);
...
```



id:
1533
timestamp:
2025-01-22T04:52:13.986Z
sender:
@sourabhniyogi:matrix.org
content:
Ok, got it -- this 2x pipelining "easter egg" is not obvious from GP or JAMNP.   We now see the conditions for it are carefully enabled through the ordering of assurances and guarantees on "rho".   Is the idea that this performance optimization is optional for M3 "Kusama performance" but practically required  for M4 "Polkadot performance" -- yet not really part of the JAM protocol per se and thus outside of GP?  Does a hint in CE133/134 to achieve this factor of 2x make sense?

id:
3098
timestamp:
2025-01-22T09:37:04.866Z
sender:
@davxy:matrix.org
content:
> <@celadari:matrix.org> So far that's also how we do 🤔
> 
> - do you do like this to generate the keys ?
> 
> ```rust
> let padding_point = Public::from(ring_ctx.padding_point());
> 
> let mut keys: Vec<Public<BandersnatchSha512Ell2>> = Vec::new();
> for key in public_keys {
>         let mut cursor = Cursor::new(&key);
>         let pub_key = Public::<BandersnatchSha512Ell2>::deserialize_compressed(&mut cursor)
>             .unwrap_or_else(|_| padding_point);
>         keys.push(pub_key);
> }
> ```

IIRC in that test you need to use the padding point twice: once for an invalid key and once for a key in the offenders list (which is zeroed out, so it becomes invalid as well). Are you using padding for both of these? IIUC you get a different ring commitment?


id:
3097
timestamp:
2025-01-22T15:48:51.306Z
sender:
@charliewinston14:matrix.org
content:
Having some difficulty finding an erasure coding library that could adhere to the GP specs. Was previously using a branch of paritytech's erasure-coding repo (branch: cheme/test_vecs) but the PR has been closed as it was non conforment.  What library are the other teams using for this?

id:
3096
timestamp:
2025-01-22T17:28:22.056Z
sender:
@decentration:matrix.org
content:
for M1, are there any conformance test vectors that we don't need to do until M2, in the davxy/jam-test-vectors repo?

or shall we pass every (tiny) test from within assurances, authorizations, disputes, history, preimages, reports, safrole, statistics and trie?

id:
3095
timestamp:
2025-01-22T17:34:54.163Z
sender:
@decentration:matrix.org
content:
 * for M1, are there any conformance test vectors that we don't need to do until M2, in the davxy/jam-test-vectors repo?

or shall we pass every (tiny) test from within assurances, authorizations, disputes, history, preimages, reports, safrole, statistics...?

id:
3094
timestamp:
2025-01-22T17:38:46.030Z
sender:
@dakkk:matrix.org
content:
Take a look on this issue: https://github.com/w3f/jamtestvectors/issues/21 

I have doubt about erasure coding, since I'm not sure is required for M1

id:
3093
timestamp:
2025-01-22T17:38:59.695Z
sender:
@dakkk:matrix.org
content:
* Take a look on this issue: https://github.com/w3f/jamtestvectors/issues/21 

I have doubt about erasure coding, since I'm not sure it is required for M1

id:
3092
timestamp:
2025-01-22T18:01:13.074Z
sender:
@gav:polkadot.io
content:
> <@decentration:matrix.org> for M1, are there any conformance test vectors that we don't need to do until M2, in the davxy/jam-test-vectors repo?
> 
> or shall we pass every (tiny) test from within assurances, authorizations, disputes, history, preimages, reports, safrole, statistics...?

M2 is focussed only on performance, so all test vectors will likely be required for M1. 

id:
3091
timestamp:
2025-01-22T18:03:58.725Z
sender:
@dakkk:matrix.org
content:
From what I see on the website (https://jam.web3.foundation/), M2 is focused on block authoring, 3 and 4 are focused on performances

id:
3090
timestamp:
2025-01-23T08:39:25.016Z
sender:
@gav:polkadot.io
content:
> <@decentration:matrix.org> for M1, are there any conformance test vectors that we don't need to do until M2, in the davxy/jam-test-vectors repo?
> 
> or shall we pass every (tiny) test from within assurances, authorizations, disputes, history, preimages, reports, safrole, statistics and trie?

* Edit: M2 is focussed on block authoring whereas M1 on block execution. The erasure coding, for example, will not be needed for M1 as it is not used in block execution. 

id:
3089
timestamp:
2025-01-23T08:41:52.339Z
sender:
@gav:polkadot.io
content:
> <@dakkk:matrix.org> From what I see on the website (https://jam.web3.foundation/), M2 is focused on block authoring, 3 and 4 are focused on performances

Yes indeed. Most test vectors will end up being useful for M1 but some are not relevant (such as erasure coding). In any case the actual conformance tests will be done privately and using a random fuzzer by the W3F to avoid the possibility of teams gaming their implementations to any blind spots in the published vectors. 

id:
3088
timestamp:
2025-01-23T08:43:07.456Z
sender:
@gav:polkadot.io
content:
The test vectors are there as an aid only. Don’t rely on them to tell you what you should and should not implement for M1 or M2. There is no short-cut to total comprehension of the gray paper. 

id:
3087
timestamp:
2025-01-23T08:43:33.608Z
sender:
@gav:polkadot.io
content:
* The test vectors are there as an aid only. Don’t rely on them to tell you what you should and should not implement for M1 or M2. There is no short-cut to total comprehension of the Gray Paper. 

id:
3086
timestamp:
2025-01-23T08:44:16.944Z
sender:
@xlchen:matrix.org
content:
is the confirmation test fuzzer going to be open sourced? so that we don’t need to invest too much time to integrate other fuzzers

id:
1532
timestamp:
2025-01-23T08:47:30.472Z
sender:
@gav:polkadot.io
content:
Implementations are expected to author blocks in a reasonably efficient manner. Asynchrony which is possible should be exploited. This will likely be required as early as M2. 

id:
1531
timestamp:
2025-01-23T08:50:18.915Z
sender:
@gav:polkadot.io
content:
We will work to provide some basic M2 test vectors demonstrating this expectation. 

id:
3085
timestamp:
2025-01-23T08:50:59.528Z
sender:
@gav:polkadot.io
content:
The interface will be. 

id:
3084
timestamp:
2025-01-23T08:51:10.394Z
sender:
@gav:polkadot.io
content:
But probably not the block-generation aspects. 

id:
3083
timestamp:
2025-01-23T08:51:19.299Z
sender:
@gav:polkadot.io
content:
That would defeat the purpose. 

id:
1530
timestamp:
2025-01-23T08:52:21.798Z
sender:
@clearloop:matrix.org
content:
If there will be a more detailed spec for async computation provided in M2 test vectors, as we can see in the dependency graph here https://graypaper.fluffylabs.dev/#/579bd12/091b00091b00, some of the state transition are just **assign** operations, that they are actually not need async 

id:
1529
timestamp:
2025-01-23T08:53:53.222Z
sender:
@clearloop:matrix.org
content:
* If there will be a more detailed spec for async computation provided in M2 test vectors, as we can see in the dependency graph here https://graypaper.fluffylabs.dev/#/579bd12/091b00091b00, some of the state transition are just **assign** operations, that they actually not need async

id:
3082
timestamp:
2025-01-23T08:58:27.465Z
sender:
@xlchen:matrix.org
content:
ok so it will be more or less the acm contest style? we submit, get some vague error message if failed, and repeat until pass?

id:
3081
timestamp:
2025-01-23T09:07:57.951Z
sender:
@gav:polkadot.io
content:
Well, given that the W3F's throughput is not infinite, teams relying on this "grind" strategy will likely not get anywhere fast.

id:
3080
timestamp:
2025-01-23T09:08:59.904Z
sender:
@gav:polkadot.io
content:
For failed entries we will try to add a test vector to the public repository demonstrating how the candidate failed.

id:
3079
timestamp:
2025-01-23T09:09:11.617Z
sender:
@gav:polkadot.io
content:
* For failed entries we will try to add a test vector to the public repository demonstrating how the candidate failed (by giving what it should have done).

id:
3078
timestamp:
2025-01-23T09:09:26.732Z
sender:
@gav:polkadot.io
content:
* For failed entries we will try to add a test vector to the public repository demonstrating how the candidate failed (by giving what it should have done). This may not always be possible, but we'll do our best.

id:
1528
timestamp:
2025-01-23T09:11:13.750Z
sender:
@gav:polkadot.io
content:
The GP doesn't specify block production.

id:
1527
timestamp:
2025-01-23T09:11:35.440Z
sender:
@gav:polkadot.io
content:
* For state transition, asynchrony is not so relevant.

id:
1526
timestamp:
2025-01-23T09:11:50.263Z
sender:
@gav:polkadot.io
content:
* For the inner aspects of state transition (block execution), asynchrony is not so relevant.

id:
1525
timestamp:
2025-01-23T09:12:21.533Z
sender:
@gav:polkadot.io
content:
There will likely need to be some to hit M3 and M4 (e.g. signature checking, inter-block Merkle root calculation).

id:
1524
timestamp:
2025-01-23T09:12:43.344Z
sender:
@gav:polkadot.io
content:
But it's up to teams how the optimise, given the correctness-constraints of the GP.

id:
1523
timestamp:
2025-01-23T09:12:51.275Z
sender:
@gav:polkadot.io
content:
We won't be holding anyone's hand here.

id:
1522
timestamp:
2025-01-23T09:13:05.529Z
sender:
@gav:polkadot.io
content:
* For the inner aspects of state transition (block execution), asynchrony is perhaps not quite so relevant.

id:
1521
timestamp:
2025-01-23T09:13:12.166Z
sender:
@gav:polkadot.io
content:
* For the inner aspects of state transition (block execution), asynchrony is perhaps not quite so relevant as with block authoring.

id:
1520
timestamp:
2025-01-23T09:14:38.889Z
sender:
@gav:polkadot.io
content:
For block authoring (M2), skipping a block for either guaranteeing or assuring will be considered incorrect behaviour.

id:
1519
timestamp:
2025-01-23T09:14:59.368Z
sender:
@gav:polkadot.io
content:
Nodes are expected to provide (and use) information in a timely fashion.

id:
1518
timestamp:
2025-01-23T09:15:17.642Z
sender:
@gav:polkadot.io
content:
* Nodes are expected to provide (and use) information in a timely fashion; this goes for all aspects of block authoring and production.

id:
1517
timestamp:
2025-01-23T09:21:54.759Z
sender:
@gav:polkadot.io
content:
Updated my notes accordingly.

id:
1516
timestamp:
2025-01-23T09:22:03.967Z
sender:
@gav:polkadot.io
content:
* Updated my notes with an according FAQ entry.

id:
1515
timestamp:
2025-01-24T10:06:05.032Z
sender:
@dvladco:matrix.org
content:
Hi, in the GP v0.5.4 the instructions `rot_l_32` and `rot_r_32` have 3 registers but `ω_B` is never used, is this a typo? and should we rotate by `ω_B` instead?

id:
1514
timestamp:
2025-01-24T10:07:06.787Z
sender:
@jan:parity.io
content:
Yes, it's a typo.

id:
1513
timestamp:
2025-01-24T10:08:26.956Z
sender:
@qiwei:matrix.org
content:
the fix is merged: https://github.com/gavofyork/graypaper/pull/193

id:
3077
timestamp:
2025-01-24T12:05:07.440Z
sender:
@jan:parity.io
content:
I have just updated the PVM test vectors here; grab them fresh from the oven here: https://github.com/w3f/jamtestvectors/pull/3

This is the biggest update yet; changes:
   * The tests now target 64-bit PVM, in alignment with GP 0.5.4. (Thanks to Ivan Subotic for helping out with these!)
   * The `trap` exit status is now called `panic` to align with the GP.
   * Added new exit status: `page-fault`; tests which previously panicked when accessing unpaged memory now generate page faults.
   * Added 106 new tests from the RISC-V test suite transpiled into PVM.

All of the tests starting with `riscv_` are ported from an external RISC-V test suite, hence they're named based on which original RISC-V instructions they were testing. I've just transpiled them to PVM (since PVM is derived from RISC-V) and left them mostly unmodified.

As always, please do let me know if there are any issues with these test vectors.

id:
3076
timestamp:
2025-01-24T12:06:24.501Z
sender:
@boymaas:matrix.org
content:
Nice! Thank you Jan Bujak 

id:
3075
timestamp:
2025-01-24T23:07:24.950Z
sender:
@jaymansfield:matrix.org
content:
> <@jan:parity.io> I have just updated the PVM test vectors here; grab them fresh from the oven here: https://github.com/w3f/jamtestvectors/pull/3
> 
> This is the biggest update yet; changes:
>    * The tests now target 64-bit PVM, in alignment with GP 0.5.4. (Thanks to Ivan Subotic for helping out with these!)
>    * The `trap` exit status is now called `panic` to align with the GP.
>    * Added new exit status: `page-fault`; tests which previously panicked when accessing unpaged memory now generate page faults.
>    * Added 106 new tests from the RISC-V test suite transpiled into PVM.
> 
> All of the tests starting with `riscv_` are ported from an external RISC-V test suite, hence they're named based on which original RISC-V instructions they were testing. I've just transpiled them to PVM (since PVM is derived from RISC-V) and left them mostly unmodified.
> 
> As always, please do let me know if there are any issues with these test vectors.

Are the instruction codes for the risc-v tests aligned with the GP?

id:
3074
timestamp:
2025-01-25T09:25:25.498Z
sender:
@gav:polkadot.io
content:
Given the above message, I’d say the answer is “yes”

id:
3073
timestamp:
2025-01-25T12:13:18.776Z
sender:
@luke_fishman:matrix.org
content:
question about  import segments:
as a guarantor i receive a work package that includes a sequence of imported data segments

I need to import and reconstruct them, by fetching 342 shards.

looking here(https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-139140-segment-shard-request)

i see that in order to fetch a shard, i need to use the erasure root

is this a mistake in the hackamd document?
should it be fetched using the segment-root?



id:
1512
timestamp:
2025-01-25T13:18:05.536Z
sender:
@weigen:matrix.org
content:
Hi, I have a question about GP (https://graypaper.fluffylabs.dev/#/579bd12/131301131901). What is the expected range of n (segment-count)? When serializing a work report, should n be considered as 𝑛∈𝑁_32 or 𝑛∈𝑁_16? The choice would result in a difference in byte size.

id:
1511
timestamp:
2025-01-25T14:48:46.481Z
sender:
@dave:parity.io
content:
Serialisation is defined in appendix C. In particular, see C.22

id:
3072
timestamp:
2025-01-26T11:55:30.517Z
sender:
@dave:parity.io
content:
Not a mistake. You need to maintain a segment-root to erasure root mapping

id:
1510
timestamp:
2025-01-26T12:07:47.772Z
sender:
@weigen:matrix.org
content:
Maybe the constraint of 𝑛∈𝑁_16 is needed, since in C.22 the input of ε_2 should be N_16

id:
3071
timestamp:
2025-01-26T14:17:23.061Z
sender:
@luke_fishman:matrix.org
content:
is the current GP version considered complete in regards to DA specs? or is there more to come?
i find it hard to follow / understand what mapping are needed to be maintained


id:
3070
timestamp:
2025-01-26T14:18:17.253Z
sender:
@gav:polkadot.io
content:
It should be unambiguous already.

id:
3069
timestamp:
2025-01-26T14:19:48.422Z
sender:
@gav:polkadot.io
content:
It won’t be as nearly as easy as M1’s block-execution logic because it isn’t a simple closed-form spec, but more of a constraint-optimisation problem. 

id:
3068
timestamp:
2025-01-26T14:20:14.796Z
sender:
@gav:polkadot.io
content:
That’s just the reality of block authoring as opposed to block execution. 

id:
1509
timestamp:
2025-01-26T14:22:22.941Z
sender:
@gav:polkadot.io
content:
It’s not strictly needed though if N \ N_{2^16} were ever fed into E_2 the result would be undefined.   

id:
1508
timestamp:
2025-01-26T14:26:24.144Z
sender:
@gav:polkadot.io
content:
Since it is deserialised with E_2 it will always be in range for correct reserialisation. And though your node could create a segment with an out of range value, it would only break you own node since it could not be encoded and this would be needed for it to be sent to another node. 

id:
1507
timestamp:
2025-01-26T14:26:38.615Z
sender:
@gav:polkadot.io
content:
* It’s not strictly needed, though if N \ N_{2^16} were ever fed into E_2 the result would be undefined.   

id:
1506
timestamp:
2025-01-26T14:27:49.080Z
sender:
@gav:polkadot.io
content:
* Since it is deserialised with E_2, any foreign-born availability specification will always be in range for correct reserialisation. And though your node could create an availablilty spec with an out of range value, it would only break your own node since it could not be encoded and this would be needed for it to be sent to another node. 

id:
3067
timestamp:
2025-01-26T15:57:44.266Z
sender:
@luke_jamixir:matrix.org
content:
OK, maybe it's a silly or too open ended questions, but could you shed some light please on 

1.Why is the justification for a import segment is either a segment root hash hash OR a work package hash
2. Why is erasure root used to fetch segments and not the segment root? 
3.being given a wp from a builder? What is the process to find its erasure root? (seems that in order to know erasure root I need to know the imports and in order to do that I need to know the erasure root)



id:
3066
timestamp:
2025-01-26T15:58:11.539Z
sender:
@luke_jamixir:matrix.org
content:
* OK, maybe it's a silly or too open ended questions, but could you shed some light please on 

1.Why is the justification for a import segment is either a segment root hash OR a work package hash
2. Why is erasure root used to fetch segments and not the segment root? 
3.being given a wp from a builder? What is the process to find its erasure root? (seems that in order to know erasure root I need to know the imports and in order to do that I need to know the erasure root)

id:
3065
timestamp:
2025-01-26T15:58:34.794Z
sender:
@luke_jamixir:matrix.org
content:
* OK, maybe it's a silly or too open ended questions, but could you shed some light please on 

1.Why is the justification for a import segment is either a segment root hash OR a work package hash
2. Why is erasure root used to fetch segments and not the segment root? 
3.being given a wp from a builder, what is the process to find its erasure root? (seems that in order to know erasure root I need to know the imports and in order to do that I need to know the erasure root)

id:
1505
timestamp:
2025-01-26T16:52:54.556Z
sender:
@danicuki:matrix.org
content:
The formula to encode guarantees on for Hx extrinsic hash 
(https://graypaper.fluffylabs.dev/#/579bd12/0ce0000cf200) differs from the formula for encoding guarantees for block encoding (https://graypaper.fluffylabs.dev/#/579bd12/375b01377f01). 

In the former calculates hash of work report. Is this correct? 



id:
1504
timestamp:
2025-01-26T17:09:49.913Z
sender:
@danicuki:matrix.org
content:
* The formula to encode guarantees on for Hx extrinsic hash 
(https://graypaper.fluffylabs.dev/#/579bd12/0ce0000cf200) differs from the formula for encoding guarantees for block encoding (https://graypaper.fluffylabs.dev/#/579bd12/375b01377f01). 

In the former, it calculates hash of work report. Is this correct? 



id:
3064
timestamp:
2025-01-26T17:15:13.333Z
sender:
@dave:parity.io
content:
1. To allow importing of recently exported segments where the builder only knows the hash of the exporting WP (not its segment root)

id:
3063
timestamp:
2025-01-26T17:16:25.554Z
sender:
@dave:parity.io
content:
2. Because the validators providing the segment shards can verify/prove to the erasure root but not the segment root

id:
1503
timestamp:
2025-01-26T17:18:14.300Z
sender:
@gav:polkadot.io
content:
Yes. 

id:
1502
timestamp:
2025-01-26T17:18:39.778Z
sender:
@gav:polkadot.io
content:
It’s a bit fiddly but it’s so that they can be separately distributed. 

id:
1501
timestamp:
2025-01-26T17:19:05.104Z
sender:
@gav:polkadot.io
content:
* It’s a bit fiddly but it’s so that they can be separately distributed and individual items concisely proven to be correct. 

id:
3062
timestamp:
2025-01-26T17:21:30.457Z
sender:
@dave:parity.io
content:
3. Build bundle and exported segments, erasure code both, calculate the root from the shards as per the GP. For importing a segment you need the erasure root of the WP that exported it, not the root of the WP that is importing it, so there is no circularity there

id:
1500
timestamp:
2025-01-26T17:25:10.984Z
sender:
@sourabhniyogi:matrix.org
content:
With `tiny` having `rotation_period: 4` (which is not all that different from `full` having `rotation_period: 10`) we have been finding in our "run work packages back-to-back" tests that work packages are reasonable for one validator to start towards the end of one rotation but by the time a guarantee is signable by one of the three, it is the case that one or two of them have rotated out of the core.  What is a good mode of operation for
(a) don't start the work if you're towards the end according to rule R, because you won't be rewarded for it!
(b) finish it because you'll be rewarded for it!
We do believe its valuable to have this rule R or reward process specified in more detail to ensure different implementations work well together -- can we come up with a good Schelling point at this point? 

Not getting the slot in the "tiny" network (where C is just 2, R is just 4) with "big" work packages (close to 12MB) makes this issue quite prominent in regression tests.  

id:
1499
timestamp:
2025-01-26T17:26:37.982Z
sender:
@sourabhniyogi:matrix.org
content:
* With `tiny` having [`rotation_period: 4`](https://docs.jamcha.in/basics/chain-spec/tiny) (which is not all that different from `full` having [`rotation_period: 10`](https://docs.jamcha.in/basics/chain-spec/full)) we have found in our "run ~12MB work packages back-to-back" tests that work packages are reasonable for one validator to start towards the end of one rotation but by the time a guarantee is signable by one of the three, it is the case that one or two of them have rotated out of the core.  Ok, so ... what is a good mode of operation for
(a) don't start the work if you're towards the end according to rule R, because you won't be rewarded for it!
(b) finish it because you'll be rewarded for it!
We do believe its valuable to have this rule R or reward process specified in more detail to ensure different implementations work well together -- can we come up with a good Schelling point at this point?

Not getting the slot in the "tiny" network (where C is just 2, R is just 4) with "big" work packages (close to 12MB) makes this issue quite prominent in regression tests.

id:
3061
timestamp:
2025-01-26T18:14:46.913Z
sender:
@luke_jamixir:matrix.org
content:
So the WP that exported it, do I get it's erasure root from the corrodponding WR (on chain, accumulated already) or? 

id:
1498
timestamp:
2025-01-26T18:40:42.668Z
sender:
@dave:parity.io
content:
Agree this will probably need to be specified at some point for different impls to work well together. I think this rule will need to be informed by the performance characteristics of a full 1000-validator network though, with real builder nodes, and we aren't in a position to run such a network yet. I can say that at the moment our node follows a pretty simple rule: as a validator, accept a package for core C if we will be "assigned" to that core in the next slot or the slot after that. Note that it is possible for block authors to include packages using assignments from the previous rotation, so there is quite a bit of leeway.

id:
1497
timestamp:
2025-01-26T18:43:28.968Z
sender:
@dave:parity.io
content:
Given this leeway perhaps a better rule would be to allow if we're currently assigned or will be assigned in the next slot. In any case I would recommend making this sort of thing easy to tweak in your implementation!

id:
1496
timestamp:
2025-01-26T18:46:49.147Z
sender:
@dave:parity.io
content:
The rule should also probably be a bit more lenient for work packages received indirectly from another guarantor on the same assignment, to avoid a situation where the "primary" guarantor _just_ accepts a package but the guarantors it then shares it with do not.

id:
1495
timestamp:
2025-01-26T18:51:36.517Z
sender:
@dave:parity.io
content:
Of course at the end of the day none of this behaviour will be required; nodes will be free to accept or reject packages however they think will maximise their profit. A specified rule seems like a good starting point though

id:
3060
timestamp:
2025-01-26T18:54:30.740Z
sender:
@dave:parity.io
content:
Yes, you will need to get it from the WR. The assumption is that nodes will maintain a database mapping SR to ER, which they will do by monitoring WRs that are included on chain

id:
3059
timestamp:
2025-01-26T19:33:54.454Z
sender:
@tomusdrw:matrix.org
content:
FYI: I've built a web app to encode/decode JAM objects using the codec: https://papi.fluffylabs.dev it's a fork of an amazing papi-console, so most of the credit should go there - I just plugged in the JAM codec. Hope you'll find it useful.

id:
3058
timestamp:
2025-01-26T19:41:46.487Z
sender:
@tomusdrw:matrix.org
content:
* FYI: I've built a web app to encode/decode JAM objects: https://papi.fluffylabs.dev it's a fork of an amazing papi-console, so most of the credit should go there - I just plugged in the JAM codec. Hope you'll find it useful.

id:
1494
timestamp:
2025-01-26T22:45:50.865Z
sender:
@sourabhniyogi:matrix.org
content:
Alright - we'll do "to allow if we're currently assigned or will be assigned in the next slot." for now sure thing thank you

id:
3057
timestamp:
2025-01-27T00:51:44.209Z
sender:
@luke_jamixir:matrix.org
content:
> <@dave:parity.io> Yes, you will need to get it from the WR. The assumption is that nodes will maintain a database mapping SR to ER, which they will do by monitoring WRs that are included on chain

Thank you [@dave:parity.io](https://matrix.to/#/@dave:parity.io) 

id:
3056
timestamp:
2025-01-27T08:59:30.851Z
sender:
@gav:polkadot.io
content:
Just a friendly reminder on clean-room requirements: don’t look at other implementation’s code (and that includes PolkaVM). Doing this will compromise you and your implementation. It’s not hard to see where code is copy-pasted (and use of the world’s most powerful plagiarism device, AI, is no different).

The rules are clear: if you submit a code base with content determined to come from anywhere other than official specs, you’ll likely be disqualified. 

id:
3055
timestamp:
2025-01-27T10:18:33.981Z
sender:
@jan:parity.io
content:
To expand on what Gav wrote, in the context of PolkaVM (since that's a major piece of Parity's node implementation that is already open source and people may be tempted to looked at) this should be obvious but let me explicitly state it - copying *any* of these is not acceptable according to the rules (this list is *not* exhaustive; these are just examples):

- PVM program blob deserialization
- instruction parsing
- interpreter implementation of what each instruction does
- exact interpreter architecture
- machine code which the recompiler generates for each instruction
- outer PVM memory map builder

It's better to err on the side of caution and not look at the PolkaVM at all, but there are some thing I would find acceptable to look at/copy/contribute to as long as it's declared up front that you've looked at these things (note that this is not "official" and I'm only speaking in a purely personal capacity here, but I will defend anyone who does any of these):

- human readable disassembly pseudo code syntax (that is, the way instructions are printed out as seen in the TESTCASES.md in my PVM test vectors; there's no official way to disassemble PVM programs besides what they're named in the GP, but feel free to copy my pseudo code syntax if you like it),
- bindings to native Linux APIs (`polkavm-linux-raw` crate) and x86/AMD64 assembler (`polkavm-assembler` crate) - these are just general purpose libraries that only coincidentally live in the PolkaVM repository for convenience; the goal here is for people to become experts in the JAM protocol, and not become experts in how to write high performance x86 assemblers (which you need for a very fast recompiler, but by itself is not specific to the JAM protocol),
- high level API as shown on https://docs.rs/polkavm (this is just an interface, not an implementation; even PolkaVM's API was/is partially based on `wasmtime`'s API),
- high level techniques for implementing a fast parser or a recompiler, since it's unrealistic to expect 34 independent teams to rediscover these from scratch, especially since those are useful/used when implementing *any* VM, not just PVM (but I emphasize the *high level* part - just copying PolkaVM's code here is not acceptable; if there's interest eventually I'll host a workshop where I'll explain some of these independent of any particular implementation),
- the toolchain to build PVM programs (`polkavm-linker` crate, `polkavm-derive`, `polkatool`, etc.) - this takes care of translating RISC-V code to PVM; this runs offline and is not part of the GP,
- the format of the `.polkavm` blob - the on-disk format of what the toolchain emits by default; this is not part of the GP and will never be (note that this only applies to the parts of the blob that are specific to the `.polkavm` blob and are *not* specced by the GP!),
- the format of the debug info inside of `.polkavm` blob (is not and will never be specced by the GP; this is needed to be able to debug PVM programs and it's useful for production-level VMs to support some form of debug info)

In general please use some common sense, and when in doubt please first ask.

id:
3054
timestamp:
2025-01-27T10:25:49.599Z
sender:
@jan:parity.io
content:
* To expand on what Gav wrote, in the context of PolkaVM (since that's a major piece of Parity's node implementation that is already open source and people may be tempted to look at) this should be obvious but let me explicitly state it - copying _any_ of these is not acceptable according to the rules (this list is _not_ exhaustive; these are just examples):

- PVM program blob deserialization
- instruction parsing
- interpreter implementation of what each instruction does
- exact interpreter architecture
- machine code which the recompiler generates for each instruction
- outer PVM memory map builder

It's better to err on the side of caution and not look at PolkaVM at all, but there are some thing I would find acceptable to look at/copy/contribute to as long as it's declared up front that you've looked at these things (note that this is not "official" and I'm only speaking in a purely personal capacity here, but I will defend anyone who does any of these):

- human readable disassembly pseudo code syntax (that is, the way instructions are printed out as seen in the TESTCASES.md in my PVM test vectors; there's no official way to disassemble PVM programs besides what they're named in the GP, but feel free to copy my pseudo code syntax if you like it),
- bindings to native Linux APIs (`polkavm-linux-raw` crate) and x86/AMD64 assembler (`polkavm-assembler` crate) - these are just general purpose libraries that only coincidentally live in the PolkaVM repository for convenience; the goal here is for people to become experts in the JAM protocol, and not become experts in how to write high performance x86 assemblers (which you need for a very fast recompiler, but by itself is not specific to the JAM protocol),
- high level API as shown on https://docs.rs/polkavm (this is just an interface, not an implementation; even PolkaVM's API was/is partially based on `wasmtime`'s API),
- high level techniques for implementing a fast parser or a recompiler, since it's unrealistic to expect 34 independent teams to rediscover these from scratch, especially since those are useful/used when implementing _any_ VM, not just PVM (but I emphasize the _high level_ part - just copying PolkaVM's code here is not acceptable; if there's interest eventually I'll host a workshop where I'll explain some of these independent of any particular implementation),
- the toolchain to build PVM programs (`polkavm-linker` crate, `polkavm-derive`, `polkatool`, etc.) - this takes care of translating RISC-V code to PVM; this runs offline and is not part of the GP,
- the format of the `.polkavm` blob - the on-disk format of what the toolchain emits by default; this is not part of the GP and will never be (note that this only applies to the parts of the blob that are specific to the `.polkavm` blob and are _not_ specced by the GP!),
- the format of the debug info inside of `.polkavm` blob (is not and will never be specced by the GP; this is needed to be able to debug PVM programs and it's useful for production-level VMs to support some form of debug info)

In general please use some common sense, and when in doubt please first ask.

id:
3053
timestamp:
2025-01-27T13:20:17.790Z
sender:
@luke_fishman:matrix.org
content:
> if there's interest eventually I'll host a workshop where I'll explain some of these independent of any particular implementation


just stating my interest. i would be very happy to join any workshop you may host on the topic

id:
3052
timestamp:
2025-01-27T13:21:19.833Z
sender:
@boymaas:matrix.org
content:
Good point Luke | Jamixir Would be very happy as well!

id:
3051
timestamp:
2025-01-27T17:10:07.155Z
sender:
@yu2c:matrix.org
content:
Hello, I have a question about PVM implementation. Since we chose a non-Rust language to implement the JAM protocol, I feel that using a higher-level language makes PVM implementation more challenging.
Are there any language restrictions for PVM implementation?

id:
3050
timestamp:
2025-01-27T17:11:20.569Z
sender:
@dakkk:matrix.org
content:
what do you mean with "language restrictions"?

id:
3049
timestamp:
2025-01-27T17:24:25.467Z
sender:
@yu2c:matrix.org
content:
If we choose Golang to implement the JAM protocol, does that mean our PVM implementation is also restricted to Golang? Or we can implement our PVM by Rust?

id:
3048
timestamp:
2025-01-27T17:58:43.169Z
sender:
@gav:polkadot.io
content:
You are free to mix-and-match languages from the same language set. 

id:
3047
timestamp:
2025-01-27T18:01:21.370Z
sender:
@gav:polkadot.io
content:
Things get complicated with mixed-language implementations from two (or more) different language sets. For this, the Fellowship and W3F will evaluate the footprint of each language in the implementation and a fractional prize taken from the according sets. This fraction will be *entirely* at the judges discretion. 

id:
3046
timestamp:
2025-01-27T18:01:51.348Z
sender:
@gav:polkadot.io
content:
* Things get complicated with mixed-language implementations from two (or more) different language sets. For this, the Fellowship and W3F will evaluate the footprint of each language in the implementation and a fractional prize may be taken from the according sets. This fraction will be *entirely* at the judges discretion. 

id:
3045
timestamp:
2025-01-27T18:02:36.750Z
sender:
@gav:polkadot.io
content:
To avoid being at the mercy of such an arbitrary decision I’d suggest teams stick to a single language set. 

id:
3044
timestamp:
2025-01-27T18:19:03.663Z
sender:
@yu2c:matrix.org
content:
Clearly understood! But I have another question: 
if I use Rust for some low-level implementation of the PVM and construct the PVM's high-level part in Golang by leveraging FFI (Foreign Function Interface) from Rust to Go, would this operation potentially go beyond the single language set?

Let me broaden the question: for third-party libraries (e.g., VRF, erasure coding, ...) allowed under the JAM Prize rules, would using FFI to bridge two language sets also go beyond the single language restriction?


id:
3043
timestamp:
2025-01-27T18:36:01.776Z
sender:
@yu2c:matrix.org
content:
I understand your answer regarding FFI [here](https://matrix.to/#/!wBOJlzaOULZOALhaRh:polkadot.io/$ZjjyqHc0IAYY1QxedjEaHj_ZCJ5s02NpPbZ7am8PGUM?via=polkadot.io&via=matrix.org&via=parity.io), but I’d like to focus on the issue of language sets: would using FFI between two different language sets be considered mixed-language set implementation under the rules?



id:
3042
timestamp:
2025-01-27T18:49:25.989Z
sender:
@gav:polkadot.io
content:
Language is unimportant for the third-party utility libraries for those portions of the logic not covered by the prize (crypto, erasure-coding, networking).

id:
3041
timestamp:
2025-01-27T18:49:33.557Z
sender:
@gav:polkadot.io
content:
* Language is unimportant and irrelevant for the third-party utility libraries for those portions of the logic not covered by the prize (crypto, erasure-coding, networking).

id:
3040
timestamp:
2025-01-27T18:50:08.297Z
sender:
@gav:polkadot.io
content:
Language only matters for that code written and submitted against the JAM Prize.

id:
3039
timestamp:
2025-01-27T18:51:10.815Z
sender:
@gav:polkadot.io
content:
* Language only matters for that code written and submitted against the JAM Prize (commonly known as a "JAM Implementation").

id:
3038
timestamp:
2025-01-27T18:52:12.711Z
sender:
@gav:polkadot.io
content:
> if I use Rust for some low-level implementation of the PVM and construct the PVM's high-level part in Golang by leveraging FFI (Foreign Function Interface) from Rust to Go, would this operation potentially go beyond the single language set?

Yes.

id:
3037
timestamp:
2025-01-27T18:52:57.458Z
sender:
@gav:polkadot.io
content:
* > if I use Rust for some low-level implementation of the PVM and construct the PVM's high-level part in Golang by leveraging FFI (Foreign Function Interface) from Rust to Go, would this operation potentially go beyond the single language set?

Yes. It would most certainly and very obviously go beyond a single language set: your implementation is using two languages!

id:
3036
timestamp:
2025-01-27T18:53:38.736Z
sender:
@gav:polkadot.io
content:
* Language is unimportant and irrelevant for the third-party utility libraries for those portions of the logic not covered by the prize (crypto, erasure-coding, networking, ...).

id:
3035
timestamp:
2025-01-27T18:59:24.844Z
sender:
@yu2c:matrix.org
content:
Got it, thanks for the clarification!

id:
3034
timestamp:
2025-01-27T19:58:33.443Z
sender:
@tomusdrw:matrix.org
content:
Correct me if my take on this is wrong, but my understanding is that the interpreter preformance is pretty irrelevant anyway, since to reach anywhere near the rust implementation (for m3/m4) it will require some transpilation of PVM code to native code (compilation).
I have a pretty limited knowledge on this, but afaict compilation involves: a) bringing in something that can turn assembler into native code (most likely llvm) and just producing native code for the hardware you are running on. For languages with runtimes it will require FFI anyway and the language-of-choice can only orchestrate that process.

id:
3033
timestamp:
2025-01-27T20:00:52.168Z
sender:
@jan:parity.io
content:
You don't really want to use LLVM. Any implementation which uses LLVM would definitely not pass any of the performance tiers because JAM *requires* linear time O(n) recompilation (otherwise security of the network breaks) which LLVM cannot do.

id:
3032
timestamp:
2025-01-27T20:13:16.404Z
sender:
@jan:parity.io
content:
So there are essentially two major parts to a PVM recompiler VM implementation:

1) recompiler which will turn PVM into native machine code - technically can be done in *any* language, but will be too slow in some languages if we target performance anywhere close to what PolkaVM has; for some non-systems languages you might be able to pull it off if you use non-idiomatic code and carefully write it for performance, for some languages (Python, Ruby, etc.) it's most likely completely impossible without using another language (maybe you could pull it off with some "Python -> C" runtime/translator/whatever, but I'm not familiar with this space so I can't tell)

2) interacting with low level OS APIs to "load" and run that code and manage the VM state (using hardware for detecting page faults, etc.); this for the most part will need at least some FFI, because the required APIs are very low level and a lot of languages can't do this natively

id:
3031
timestamp:
2025-01-28T03:48:34.192Z
sender:
@clearloop:matrix.org
content:
tbh, super curious about if we can hit m3 with an interpreter

id:
3030
timestamp:
2025-01-28T03:49:00.543Z
sender:
@clearloop:matrix.org
content:
* tbh, super curious about if it is possible that hitting m3 with an interpreter

id:
3029
timestamp:
2025-01-28T03:49:47.767Z
sender:
@clearloop:matrix.org
content:
* curious about if it is possible to hit m3 with an interpreter

id:
3028
timestamp:
2025-01-28T03:50:54.715Z
sender:
@gav:polkadot.io
content:
> <@clearloop:matrix.org> curious about if it is possible to hit m3 with an interpreter

Highly unlikely. 

id:
3027
timestamp:
2025-01-28T07:05:08.030Z
sender:
@dakkk:matrix.org
content:
Anyway there are various techniques to improve performance even on interpreted languages; for instance, for me using python I can rewrite critical sections of jampy using cython (which is python code translated to C and compiled)

id:
3026
timestamp:
2025-01-28T08:54:47.677Z
sender:
@jan:parity.io
content:
Take a look at benchmarks here:

https://github.com/paritytech/polkavm/blob/master/BENCHMARKS.md#oneshot-execution-for-pinky

Compare "PolkaVM (64-bit, recompiler)" vs "PolkaVM (64-bit, interpreter)". My interpreter can still be optimized, but even if you look at state of art optimized-to-the-bone interpreters (e.g. wasm3) there's still an order of magnitude performance difference vs the recompiler.

So essentially I expect the M3 performance threshold will most likely be on the level of "a working but not necessarily optimal recompiler with okay compilation speed" and M4 to be "an optimized recompiler with high recompilation speed".

id:
3025
timestamp:
2025-01-28T18:03:03.753Z
sender:
@sourabhniyogi:matrix.org
content:
The "JAM requires linear time O(n) recompilation" has been an non-explicit (but deepseekly awesome) requirement it appears, for M3 -- check?  Can you explain what teams would be expected to do to achieve this O(n) linear compilation time, and how we could get started on this process in the next 2 months?

It would appear that once we go through the above transformation, the bulk of our [Appendix A](https://graypaper.fluffylabs.dev/#/579bd12)  PVM implementations are largely training wheels, save the infamous [`ecalli`](https://graypaper.fluffylabs.dev/#/579bd12/25b90025b900), check?  

id:
3024
timestamp:
2025-01-28T18:04:20.875Z
sender:
@sourabhniyogi:matrix.org
content:
* The "JAM requires linear time O(n) recompilation" has been an non-explicit (but deepseekly awesome) requirement it appears, for M3 -- check?  Can you explain what teams would be expected to do to achieve this O(n) linear compilation time, and how we could get started on this process in the next 2 months?

It would appear that once we go through the above transformation, the bulk of our [Appendix A](https://graypaper.fluffylabs.dev/#/579bd12)  PVM implementations are largely training wheels, save the infamous [`ecalli`](https://graypaper.fluffylabs.dev/#/579bd12/25b90025b900) (and thus as tomusdrw says, anyone's interpreter performance is in fact _irrelevant_), check?

id:
3023
timestamp:
2025-01-28T18:05:56.311Z
sender:
@sourabhniyogi:matrix.org
content:
* The "JAM requires linear time O(n) recompilation" has been an non-explicit (but deepseekly awesome) requirement it appears, for M3 -- check?  Can you explain what teams would be expected to do to achieve this O(n) linear compilation time, and how we could get started on this process in the next 2 months?

It would appear that once we go through the above transformation, the bulk of our [Appendix A](https://graypaper.fluffylabs.dev/#/579bd12)  PVM implementations are then expendable training wheels, save the infamous [`ecalli`](https://graypaper.fluffylabs.dev/#/579bd12/25b90025b900) (and thus as tomusdrw says, anyone's interpreter performance is in fact _irrelevant_), check?  If this is correct, we should like to understand how this ecalli / host function call process works in a post-recompilation era, soon?

id:
3022
timestamp:
2025-01-28T19:15:54.864Z
sender:
@gav:polkadot.io
content:
> as tomusdrw says, anyone's interpreter performance is in fact irrelevant

Almost certainly correct.

id:
3021
timestamp:
2025-01-28T19:17:09.609Z
sender:
@gav:polkadot.io
content:
Jan Bujak is already planning on giving talks about how to achieve a high performance O(n) recompiler. Any team interested in delivering M3 or M4 would be well-advised to pay *very* close attention.

id:
3020
timestamp:
2025-01-28T19:18:17.947Z
sender:
@gav:polkadot.io
content:
It won't just be about the recompiler though. Much performance will come with optimised databases, Merklisation and networking behaviour. M4's performance tests (which have yet to be written) will be stringent.

id:
3019
timestamp:
2025-01-28T19:19:44.153Z
sender:
@gav:polkadot.io
content:
M3 is intended to be somewhat less stringent but still requiring a recompiler and sensible implementations for all of the above. Obviously, some languages will need to be less optimised than others in order to get to M3.

id:
3018
timestamp:
2025-01-28T23:03:31.049Z
sender:
@decentration:matrix.org
content:
> <@gav:polkadot.io> Just a friendly reminder on clean-room requirements: don’t look at other implementation’s code (and that includes PolkaVM). Doing this will compromise you and your implementation. It’s not hard to see where code is copy-pasted (and use of the world’s most powerful plagiarism device, AI, is no different).
> 
> The rules are clear: if you submit a code base with content determined to come from anywhere other than official specs, you’ll likely be disqualified.

do you mean don't use AI as a tool for plagiarism, or that AI is itself plagiarism?

it is a very clear rule not to look at anything related to JAM implementation except for the GP, these two channels, the JAM videos and the conformance tests. 

But what is now less clear are AI tools use. i thought it was clear when you mentioned that AI should not be used to plagiarise someone else's work ( in [June 2024](vector://vector/webapp/#/room/!wBOJlzaOULZOALhaRh:polkadot.io/$aA1-kjwr08g74hQ46SpIIBajbxSFsZl_hg5RfRcWvpQ)), this makes sense and is an understandable concern, hence why commits must show a timestamped path of one's own coding journey. 

But it has never been mentioned that it cannot be used at all. Can you confirm? Because the "Explain" button in the Fluffy Labs GP reader, signals that it can be used, but as an obvious caveat not to be used as a tool to rearticulate someone else's work. 





id:
3017
timestamp:
2025-01-28T23:04:05.585Z
sender:
@decentration:matrix.org
content:
 * do you mean don't use AI as a tool for plagiarism, or that AI is itself plagiarism?

it is a very clear rule not to look at anything related to JAM implementation except for the GP, these two channels, the JAM videos and the conformance tests.

But what is now less clear are AI tools use. i thought it was clear when you mentioned that AI should not be used to plagiarise someone else's work, in [June 2024](vector://vector/webapp/#/room/!wBOJlzaOULZOALhaRh:polkadot.io/$aA1-kjwr08g74hQ46SpIIBajbxSFsZl_hg5RfRcWvpQ), this makes sense and is an understandable concern, hence why commits must show a timestamped path of one's own coding journey.

But it has never been mentioned that it cannot be used at all. Can you confirm? Because the "Explain" button in the Fluffy Labs GP reader, signals that it can be used, but as an obvious caveat not to be used as a tool to rearticulate someone else's work.

id:
3016
timestamp:
2025-01-28T23:16:48.670Z
sender:
@jimboj21:matrix.org
content:
Hello, I have a question regarding the MMR super peak function (GP E.10). I thought I have an understanding of how this works, but am not getting the correct output on the reporting vectors test case for this.

Lets say the input to the function is: [a, b, nil, c]
I would expect the following behavior for this example:
- empty removed, giving [a, b, c]
- msg = $peak + a + b
- msg2 = $peak + H(msg) + c
- super_peak = H(msg2)
  
Does this seem like the correct flow (i reduced the details in the example, but can elaborate on steps if needed)? I'm not understanding where I am off in my logic here but like I said I am not outputting the correct value (unless its a bug elsewhere). 

Any input is appreciated, thanks!

id:
3015
timestamp:
2025-01-29T00:33:01.416Z
sender:
@gav:polkadot.io
content:
> <@decentration:matrix.org> do you mean don't use AI as a tool for plagiarism, or that AI is itself plagiarism?
> 
> it is a very clear rule not to look at anything related to JAM implementation except for the GP, these two channels, the JAM videos and the conformance tests.
> 
> But what is now less clear are AI tools use. i thought it was clear when you mentioned that AI should not be used to plagiarise someone else's work, in [June 2024](vector://vector/webapp/#/room/!wBOJlzaOULZOALhaRh:polkadot.io/$aA1-kjwr08g74hQ46SpIIBajbxSFsZl_hg5RfRcWvpQ), this makes sense and is an understandable concern, hence why commits must show a timestamped path of one's own coding journey.
> 
> But it has never been mentioned that it cannot be used at all. Can you confirm? Because the "Explain" button in the Fluffy Labs GP reader, signals that it can be used, but as an obvious caveat not to be used as a tool to rearticulate someone else's work.

It is a slightly impetuous and controversial remark but my point was that current AI tools *are* fundamentally plagiaristic. 

id:
3014
timestamp:
2025-01-29T00:34:42.525Z
sender:
@gav:polkadot.io
content:
Generally they are shallow reformulation of the work of others and usually devoid of original material.

id:
3013
timestamp:
2025-01-29T00:36:14.384Z
sender:
@gav:polkadot.io
content:
We can’t really enforce the non-use of AI more than we can enforce not looking at, say, stackexchange. But if we see obvious similarities in two code bases we will assume at least one is plagiaristic. 

id:
3012
timestamp:
2025-01-29T00:37:21.323Z
sender:
@gav:polkadot.io
content:
So beware of this if using ChatGPT to write your algorithms for you. 

id:
3011
timestamp:
2025-01-29T05:05:26.191Z
sender:
@stanleyli:matrix.org
content:
Hello everyone,

I just created a PR to jamtestvector, providing test vectors generated based on the definitions from the Gray Paper.
https://github.com/w3f/jamtestvectors/pull/35

I hope those interested can give them a try, and I look forward to contributing to the community

id:
3010
timestamp:
2025-01-29T06:52:00.011Z
sender:
@jan:parity.io
content:
> Can you explain what teams would be expected to do to achieve this O(n) linear compilation time

I'm not really sure what there is to explain here as it should be pretty much self explanatory. The process of turning PVM into a native machine code needs to take O(n) time, that is the formula `recompilation_time = a * size_of_program + constant` must hold.

> how we could get started on this process in the next 2 months?

Write a recompiler which turns PVM bytecode into machine code in a single pass (that is, you don't go over the bytecode multiple times; you generate machine code as you read it), then write a VM that can load it and execute it.

> It would appear that once we go through the above transformation, the bulk of our Appendix A PVM implementations are then expendable training wheels

No, you want to write an interpreter first simply because recompilers are so much harder to debug. If you have a correct interpreter you can then easily crosscheck your recompiler against it. Even I wouldn't write a recompiler right off the bat and first make an interpreter.

>  If this is correct, we should like to understand how this ecalli / host function call process works in a post-recompilation era, soon?

Sorry, I don't understand this question at all. There's nothing special about ecalli nor about calling host functions?

id:
3009
timestamp:
2025-01-29T07:50:19.201Z
sender:
@dakkk:matrix.org
content:
We talked about it in this channel in September, if I remember correctly

id:
3008
timestamp:
2025-01-29T08:29:57.442Z
sender:
@dave:parity.io
content:
This was changed in the GP recently, not sure the test vectors have been updated yet

id:
3007
timestamp:
2025-01-29T08:33:15.103Z
sender:
@dave:parity.io
content:
See https://github.com/gavofyork/graypaper/commit/f9e72d79698077a2d21075f6d5555250a7c498b8

id:
3006
timestamp:
2025-01-29T08:36:11.545Z
sender:
@prematurata:matrix.org
content:
On my side they don't look like the tests have reflected the recent changes

id:
3005
timestamp:
2025-01-29T15:09:17.733Z
sender:
@jimboj21:matrix.org
content:
oh my gosh this is totally it! I was so confused, reminder to self to be careful with versions 😅 Thanks for help here

id:
1493
timestamp:
2025-01-29T16:46:50.129Z
sender:
@vinsystems:matrix.org
content:
If a panic occurs when `80 -> load_imm_jump` executes the [branch](https://graypaper.fluffylabs.dev/#/579bd12/26ef0226ef02) function, the "ωA" don't should be changed, right?

id:
1492
timestamp:
2025-01-29T16:49:10.986Z
sender:
@jan:parity.io
content:
It's always changed, regardless of whether the jump fails or not.

id:
1491
timestamp:
2025-01-29T16:49:51.068Z
sender:
@jan:parity.io
content:
(This is the equivalent to the RISC-V's `call` instruction; the usual use of this instruction is to load the return address and jump to another function.)

id:
1490
timestamp:
2025-01-29T16:50:02.138Z
sender:
@jan:parity.io
content:
* (This is the equivalent of RISC-V's `call` instruction; the usual use of this instruction is to load the return address and jump to another function.)

id:
1489
timestamp:
2025-01-29T16:52:24.159Z
sender:
@vinsystems:matrix.org
content:
Thanks! 🙂

id:
1488
timestamp:
2025-01-30T10:08:16.480Z
sender:
@carlos-romano:matrix.org
content:
Screenshot 2025-01-30 at 11.07.59.png

id:
1487
timestamp:
2025-01-30T10:09:49.198Z
sender:
@carlos-romano:matrix.org
content:
Regarding https://github.com/davxy/jam-test-vectors/tree/polkajam-vectors/reports :

 I can see there are test vectors where auth_pools are the same in both prestate and poststate, but given that we should remove the used authorizer from the pool, and that pending core authorizers are not provided by test vectors and should be empty, I don't get why this is the case. 

Specific commit where this change was introduced: https://github.com/davxy/jam-test-vectors/commit/729592cf87eb09bc34555f846c5b19f5b1453c52 .

This is related to equation 8.2, GP 0.5.4

Thanks!



id:
1486
timestamp:
2025-01-30T13:18:45.739Z
sender:
@subotic:matrix.org
content:
Regarding `page-fault` and this paragraph https://graypaper.fluffylabs.dev/#/579bd12/243c00245500, I understand that when trying to write to writable memory and I cannot, I emit a `page-fault`. What is not so clear to me based on this definition, that if I want to write to read-only memory, I should emit a `panic`. I only know that because of the test-vectors. Or did I miss a place in the GP where this is defined?

id:
1485
timestamp:
2025-01-30T13:20:38.032Z
sender:
@boymaas:matrix.org
content:
Maybe this helps: https://github.com/w3f/jamtestvectors/pull/3#issuecomment-2614547819

id:
1484
timestamp:
2025-01-30T13:25:10.678Z
sender:
@subotic:matrix.org
content:
Great, thanks for the link. As always, things are more complicated, then they seem.

id:
1483
timestamp:
2025-01-30T14:21:38.312Z
sender:
@gav:polkadot.io
content:
Version [0.6.0](https://github.com/gavofyork/graypaper/releases/tag/v0.6.0) is released!

id:
1482
timestamp:
2025-01-30T14:22:41.594Z
sender:
@gav:polkadot.io
content:
There's a few changes in this from 0.5.4 all centred around the PVM

id:
1481
timestamp:
2025-01-30T14:26:39.254Z
sender:
@gav:polkadot.io
content:
- `import` host call has been removed in favour of a new `fetch` hostcall; this reduces the amount of data placed in PVM memory up-front and provides a means of extracting data beyond just that concerning the current work-item but for other work-items too.
- All data providing host-calls now accept an offset parameter to allow any contiguous subportion of the data to be read.
- OOB has been (almost) removed. When the outer PVM has a host-call in which it is passed a memory address it cannot access, then it panics irrecoverably.


id:
1480
timestamp:
2025-01-30T14:27:02.211Z
sender:
@gav:polkadot.io
content:
The page faulting specification has also been updated and formalised.

id:
1479
timestamp:
2025-01-30T14:28:52.784Z
sender:
@gav:polkadot.io
content:
This release signals the end of the 0.5 series and, potentially (but probably not), the final protocol revision (not including the stuff which we know will need doing before the end such as gas pricing).

id:
1478
timestamp:
2025-01-30T14:29:09.402Z
sender:
@gav:polkadot.io
content:
* This release signals the end of the 0.5 series and, potentially (but probably not), the final protocol revision (not including the obvious stuff which we know will need doing before 1.0 such as gas pricing).

id:
1477
timestamp:
2025-01-30T14:30:08.471Z
sender:
@gav:polkadot.io
content:
0.6 series should contain primarily cleanups, finesse, formatting, discussion and corrections.

id:
1476
timestamp:
2025-01-30T14:30:55.495Z
sender:
@gav:polkadot.io
content:
0.7 and 0.8 will be any important tweaks or optimisations brought on through prototyping and Toaster-testing.

id:
1475
timestamp:
2025-01-30T14:31:03.655Z
sender:
@gav:polkadot.io
content:
* 0.7 and 0.8 will be any important tweaks or optimisations brought on through service-prototyping and Toaster-testing.

id:
1474
timestamp:
2025-01-30T14:31:23.890Z
sender:
@gav:polkadot.io
content:
0.9 will be auditing fixes only.

id:
1473
timestamp:
2025-02-02T11:16:26.424Z
sender:
@gav:polkadot.io
content:
Version [0.6.1](https://github.com/gavofyork/graypaper/releases/tag/v0.6.1) is released.

This is just a few small corrections and the removal of one of the more complex parts of `fetch`.

id:
1472
timestamp:
2025-02-02T11:16:50.980Z
sender:
@gav:polkadot.io
content:
* Version [0.6.1](https://github.com/gavofyork/graypaper/releases/tag/v0.6.1) is released.

This is just a few small corrections and a simplification of `fetch`.

id:
1471
timestamp:
2025-02-03T06:12:03.367Z
sender:
@clw0908:matrix.org
content:
Should **ω_A** be taken modulo **2^32** before being fed into **Χ_4**, since the input of **Χ_4** belongs to **N_32**?

https://graypaper.fluffylabs.dev/#/4bb8fd2/299c0329a303

id:
1470
timestamp:
2025-02-03T08:45:46.721Z
sender:
@gav:polkadot.io
content:
Yes, I expect so - [@jan:parity.io](https://matrix.to/#/@jan:parity.io)?

id:
1469
timestamp:
2025-02-03T09:49:11.549Z
sender:
@jan:parity.io
content:
Yes, since X_4 requires the input to be N_32 there should be a modulo there, otherwise the result would be undefined.

Side note, since I already had questions regarding this: you can think of every 32-bit instruction variant as *always* taking a modulo of its inputs, even though sometimes in the GP we omit this modulo to make the equations simpler if the result would be equivalent anyway, for example in case of `add_32` the result gets truncated anyway so truncating the inputs is unnecessary. The high-level intent is to allow every 32-bit instruction variant to be implemented as follows (to allow for an efficient recompiler implementation): 1) truncate all inputs to 32-bit, 2) do the operation, 3) sign-extend to 64-bit, so if any equation in the GP for 32-bit instruction variants would have given a result that doesn't match this please ping me and we'll correct it.

id:
1468
timestamp:
2025-02-03T09:49:53.498Z
sender:
@jan:parity.io
content:
* Yes, since X\_4 requires the input to be N\_32 there should be a modulo there, otherwise the result would be undefined.

Side note, since I already had questions regarding this: you can think of every 32-bit instruction variant as _always_ taking a modulo of its inputs, even though sometimes in the GP we omit this modulo to make the equations simpler if the result would be equivalent anyway, for example in case of `add_32` the result gets truncated anyway so truncating the inputs is unnecessary. The high-level intent is to allow every 32-bit instruction variant to be implemented as follows (to allow for an efficient recompiler implementation): 1) truncate all inputs to 32-bit, 2) do the operation, 3) sign-extend to 64-bit, so if you find any equation in the GP for 32-bit instruction variants would have given a result that doesn't match this please ping me and we'll correct it.

id:
3004
timestamp:
2025-02-03T11:08:47.370Z
sender:
@emielsebastiaan:matrix.org
content:
I found an error in testvectors for the `rem_s_64` instruction: 
https://github.com/w3f/jamtestvectors/pull/3#issuecomment-2630619549

id:
1467
timestamp:
2025-02-03T11:11:06.880Z
sender:
@emielsebastiaan:matrix.org
content:
Related to an issue we found in testvectors for PVM instruction 206.
Does GP allow for negative output of a modulo operation?
Since this is not explicitly mentioned in section 3, I assume the answer is no. 
If the answer is no, then PolkaVM probably has an incorrect implementation of GP.
https://github.com/gavofyork/graypaper/issues/222

id:
1466
timestamp:
2025-02-03T12:04:04.441Z
sender:
@jan:parity.io
content:
I just replied in the issue: https://github.com/w3f/jamtestvectors/pull/3#issuecomment-2630744952

TLDR: the test vector is correct and what we want; GP might have to be tweaked to account for this

id:
1465
timestamp:
2025-02-03T12:05:05.560Z
sender:
@emielsebastiaan:matrix.org
content:
Yes in that case GP should be adjusted to remove any ambiguity.
GP should to explicitly state that the modulo operator on a negative number yields a negative number, and not a positive number as expected by 'Maths'.

id:
3003
timestamp:
2025-02-03T12:35:03.333Z
sender:
@decentration:matrix.org
content:
do we have a tiny const for L? in GP it's:

```
L = 14, 400: The maximum age in timeslots of the lookup anchor.
```

id:
3002
timestamp:
2025-02-03T12:46:05.474Z
sender:
@decentration:matrix.org
content:
 * do we have a tiny const for `L` (re: 11.34)? In GP (I.4) Values it's:

```
L = 14, 400: The maximum age in timeslots of the lookup anchor.
```

id:
3001
timestamp:
2025-02-03T12:47:06.877Z
sender:
@oliver.tale-yazdi:parity.io
content:
I dont think there was a value proposed for this yet, otherwise i will add it to https://docs.jamcha.in/basics/chain-spec/tiny  
We can probably calculate backwards from the toaster full spec (modulo any contraints)?

id:
3000
timestamp:
2025-02-03T12:58:33.053Z
sender:
@decentration:matrix.org
content:
ah ok thanks for these docs, i think for now i can infer it from the `report_before_last_rotation` test vector

id:
2999
timestamp:
2025-02-03T13:12:48.092Z
sender:
@decentration:matrix.org
content:
oh no, actually `rotation_period` = 4 is what i need for that test vector.  

i just need to floor 22/4 - floor(6/4) = 4, which is greater than 1 rotation. 

but for `L`, I will just use a small reasonable number, i will go with 60. 

 



id:
2998
timestamp:
2025-02-03T13:13:20.304Z
sender:
@decentration:matrix.org
content:
 * oh no, actually `rotation_period` = 4 is what i need for that test vector.

i just need to floor(22/4) - floor(6/4) = 4, which is greater than 1 rotation.

but for `L`, I will just use a small reasonable number, i will go with 60.

id:
2997
timestamp:
2025-02-03T18:11:46.377Z
sender:
@gav:polkadot.io
content:
IMG_4368.jpeg

id:
2996
timestamp:
2025-02-03T18:11:51.695Z
sender:
@gav:polkadot.io
content:
IMG_4367.jpeg

id:
2995
timestamp:
2025-02-03T18:11:57.170Z
sender:
@gav:polkadot.io
content:
IMG_4366.jpeg

id:
2994
timestamp:
2025-02-03T18:12:02.001Z
sender:
@gav:polkadot.io
content:
IMG_4365.jpeg

id:
2993
timestamp:
2025-02-03T18:12:06.567Z
sender:
@gav:polkadot.io
content:
IMG_4364.jpeg

id:
2992
timestamp:
2025-02-03T18:12:11.666Z
sender:
@gav:polkadot.io
content:
IMG_4363.jpeg

id:
2991
timestamp:
2025-02-03T18:12:17.088Z
sender:
@gav:polkadot.io
content:
IMG_4362.jpeg

id:
2990
timestamp:
2025-02-03T18:12:21.587Z
sender:
@gav:polkadot.io
content:
IMG_4361.jpeg

id:
2989
timestamp:
2025-02-03T18:12:28.044Z
sender:
@gav:polkadot.io
content:
IMG_4360.jpeg

id:
2988
timestamp:
2025-02-03T18:13:03.314Z
sender:
@gav:polkadot.io
content:
Some new pics on the latest state of the Toaster.

id:
2987
timestamp:
2025-02-03T18:13:24.800Z
sender:
@gav:polkadot.io
content:
We now have around half of the machines delivered, and should have the final half in the next 2-3 weeks.

id:
2986
timestamp:
2025-02-03T18:13:56.291Z
sender:
@gav:polkadot.io
content:
The first 6 are being configured now in a temporary server room, the others will be installed and configured in the dedicated space once that's ready. 

id:
2985
timestamp:
2025-02-03T18:14:47.903Z
sender:
@gav:polkadot.io
content:
They've been installing some serious heat ducts to make sure that the jacuzzi gets toasty.

id:
2984
timestamp:
2025-02-03T18:16:18.844Z
sender:
@gav:polkadot.io
content:
* We now have around half of the machines delivered for phase 1 (41 of them), and should have the remaining 45 in the next 2-3 weeks.

id:
2983
timestamp:
2025-02-03T18:18:43.535Z
sender:
@gav:polkadot.io
content:
Once phase 1 is built out and going strong we can start to think about phase two, which will see an additional 85 machines for a total of 171 machines. That will take us to our goal of 16 cores per node.

id:
2982
timestamp:
2025-02-03T18:19:04.320Z
sender:
@gav:polkadot.io
content:
* Once phase 1 is built out and going strong we can start to think about phase two, which will see an additional 85 machines for a total of 171 machines. That will take us to our goal of 16 AMD TR cores, each with 1MB L2 cache, per node.

id:
2981
timestamp:
2025-02-03T19:12:20.810Z
sender:
@sourabhniyogi:matrix.org
content:
If you feel industrious, consider `L` (`max_age_in_timeslots` (?)) for other networks from tiny to full ?

https://docs.google.com/spreadsheets/d/1ueAisCMOx7B-m_fXMLT0FXBxfVzydJyr-udE8jKwDN8/edit?gid=615049643#gid=615049643


As you can see, since most of us can't afford a JAM Toaster but can afford a few hours of `small` to `large`, or non-stop `tiny` to `small`, the simple-minded approach taken thus far is to have the same constants for `tiny` through `large` as `tiny`, and the same constants for xlarge through `full` as `full`.  If anyone has a better idea, just go for it!

id:
2980
timestamp:
2025-02-03T22:12:16.274Z
sender:
@gav:polkadot.io
content:
* Once phase 1 is built out and going strong we can start to think about phase two, which will see an additional 85 machines for a total of 171 machines. That will take us to our goal of 16 AMD TR cores, each with 1MB L2 cache, for each of the 1023 nodes.

id:
1464
timestamp:
2025-02-04T09:06:08.041Z
sender:
@carlos-romano:matrix.org
content:
anyone please? 🙏

id:
1463
timestamp:
2025-02-04T11:31:45.786Z
sender:
@davxy:matrix.org
content:
> <@carlos-romano:matrix.org> anyone please? 🙏

I'll take a look. Could you please specify one particular test vector so I can review it directly?

id:
1462
timestamp:
2025-02-04T12:03:32.820Z
sender:
@gav:polkadot.io
content:
> <@emielsebastiaan:matrix.org> Yes in that case GP should be adjusted to remove any ambiguity.
> GP should to explicitly state that the modulo operator on a negative number yields a negative number, and not a positive number as expected by 'Maths'.

This is corrected/clarified in main. 

id:
1461
timestamp:
2025-02-04T12:44:36.684Z
sender:
@carlos-romano:matrix.org
content:
thanks a lot! All the test vectors modified here:

https://github.com/davxy/jam-test-vectors/commit/729592cf87eb09bc34555f846c5b19f5b1453c52

For example:

reports/tiny/high_work_report_gas-1.json

Probably I am missing something, but for me the right test vectors should be how they were before that commit.

id:
1460
timestamp:
2025-02-04T16:28:48.963Z
sender:
@prematurata:matrix.org
content:
I've a question about 14.13. `bold_l` is being constructed by essentially ensuring that all the workpackagehashes (special hash) in the workItems import data segments `i` have a key in the bold_l (14.11) and that the "pointing" value in the `bold_l` dictionary is the previously computed "segmentRoot" of the AVailability specification (14.13).

 - If this is correct then it means guarantors need to maintain a Datastore containing a dictionary of Previously computed WorkResults corect?
 - I don't see any limitation of imported segments referencing WorkPackage hashes which generated a report on the same core as the one we're trying to compute. This means that a workpackage p could have work items `w` whose reference in its import-segments a workpackagehash w which was computed on another core   

id:
1459
timestamp:
2025-02-04T16:28:57.739Z
sender:
@prematurata:matrix.org
content:
(0.6.1)

id:
1458
timestamp:
2025-02-04T16:29:56.162Z
sender:
@prematurata:matrix.org
content:
* I've a question about 14.13. `bold_l` is being constructed by essentially ensuring that all the workpackagehashes (special hash) in the workItems import data segments `i` have a key in the bold\_l (14.11) and that the "pointing" value in the `bold_l` dictionary is the **previously** computed "segmentRoot" of the AVailability specification (14.13).

- If this is correct then it means guarantors need to maintain a Datastore containing a dictionary of Previously computed WorkResults corect?
- I don't see any limitation of imported segments referencing WorkPackage hashes which generated a report on the same core as the one we're trying to compute. This means that a workpackage p could have work items `w` whose reference in its import-segments a workpackagehash w which was computed on another core

id:
1457
timestamp:
2025-02-04T19:28:19.484Z
sender:
@davxy:matrix.org
content:
> <@carlos-romano:matrix.org> thanks a lot! All the test vectors modified here:
> 
> https://github.com/davxy/jam-test-vectors/commit/729592cf87eb09bc34555f846c5b19f5b1453c52
> 
> For example:
> 
> reports/tiny/high_work_report_gas-1.json
> 
> Probably I am missing something, but for me the right test vectors should be how they were before that commit.

IIRC this is a change applied after a discussion with a community member. The "reports" STF exercised by these vectors do not change the content of the auth queues. The content of the queues is changed by the "authorizations" test vectors. I'll add a note to the readme to make this explicit

id:
1456
timestamp:
2025-02-04T19:29:22.199Z
sender:
@davxy:matrix.org
content:
* IIRC this is a change applied after a discussion with a community member. The "reports" STF exercised by these vectors do not change the content of the auth queues. The content of the queues is changed by the "authorizations" test vectors. I'll add a note to the readme to make this explicit

id:
1455
timestamp:
2025-02-04T19:31:51.445Z
sender:
@davxy:matrix.org
content:
* The "reports" STF exercised by these vectors do not change the content of the auth queues. The content of the queues is changed by the "authorizations" test vectors. I'll add a note to the readme to make this explicit

id:
1454
timestamp:
2025-02-04T19:32:26.079Z
sender:
@davxy:matrix.org
content:
* The "reports" STF exercised by these vectors has been modified to not change the content of the auth queues. The content of the queues is changed by the "authorizations" test vectors. I'll add a note to the readme to make this explicit

id:
2979
timestamp:
2025-02-04T20:09:56.873Z
sender:
@decentration:matrix.org
content:
> <@gav:polkadot.io> They've been installing some serious heat ducts to make sure that the jacuzzi gets toasty.

nice use case 👌

id:
1453
timestamp:
2025-02-05T07:06:42.040Z
sender:
@carlos-romano:matrix.org
content:
ahh ok thanks, so then we don't need to check them against the STF test vectors post state right? Thanks a lot  🙏

id:
1452
timestamp:
2025-02-05T09:05:02.713Z
sender:
@gav:polkadot.io
content:
> <@prematurata:matrix.org> I've a question about 14.13. `bold_l` is being constructed by essentially ensuring that all the workpackagehashes (special hash) in the workItems import data segments `i` have a key in the bold\_l (14.11) and that the "pointing" value in the `bold_l` dictionary is the **previously** computed "segmentRoot" of the AVailability specification (14.13).
> 
> - If this is correct then it means guarantors need to maintain a Datastore containing a dictionary of Previously computed WorkResults corect?
> - I don't see any limitation of imported segments referencing WorkPackage hashes which generated a report on the same core as the one we're trying to compute. This means that a workpackage p could have work items `w` whose reference in its import-segments a workpackagehash w which was computed on another core

1. Correct

2. Nodes need only keep fairly recent WPH->SR history, since it is known that they must pass the on-chain WPH->SR lookup. Ultimately guarantors are free to ignore work packages whose imports they deem unreasonable, unknowable or unlikely to result in a profitable endeavour. 

id:
1451
timestamp:
2025-02-05T09:05:28.489Z
sender:
@gav:polkadot.io
content:
> <@prematurata:matrix.org> I've a question about 14.13. `bold_l` is being constructed by essentially ensuring that all the workpackagehashes (special hash) in the workItems import data segments `i` have a key in the bold_l (14.11) and that the "pointing" value in the `bold_l` dictionary is the previously computed "segmentRoot" of the AVailability specification (14.13).
> 
>  - If this is correct then it means guarantors need to maintain a Datastore containing a dictionary of Previously computed WorkResults corect?
>  - I don't see any limitation of imported segments referencing WorkPackage hashes which generated a report on the same core as the one we're trying to compute. This means that a workpackage p could have work items `w` whose reference in its import-segments a workpackagehash w which was computed on another core   

* 1. Correct

2. Nodes need only keep fairly recent WPH->SR history, since it is known that the resultant report  must pass the on-chain WPH->SR lookup whose history is limited. Ultimately guarantors are free to ignore work packages whose imports they deem unreasonable, unknowable or unlikely to result in a profitable endeavour. 

id:
1450
timestamp:
2025-02-05T09:07:21.061Z
sender:
@gav:polkadot.io
content:
> <@prematurata:matrix.org> I've a question about 14.13. `bold_l` is being constructed by essentially ensuring that all the workpackagehashes (special hash) in the workItems import data segments `i` have a key in the bold_l (14.11) and that the "pointing" value in the `bold_l` dictionary is the previously computed "segmentRoot" of the AVailability specification (14.13).
> 
>  - If this is correct then it means guarantors need to maintain a Datastore containing a dictionary of Previously computed WorkResults corect?
>  - I don't see any limitation of imported segments referencing WorkPackage hashes which generated a report on the same core as the one we're trying to compute. This means that a workpackage p could have work items `w` whose reference in its import-segments a workpackagehash w which was computed on another core   

* 1. Correct

2. Nodes need only keep fairly recent WPH->SR history, since it is known that the resultant report  must pass the on-chain WPH->SR lookup whose history is limited. Ultimately guarantors are free to ignore work packages whose imports they deem unreasonable, unknowable or unlikely to result in a profitable endeavour. To pass the conformance tests nodes need only be able to make guarantees under reasonable conditions.  

id:
1449
timestamp:
2025-02-05T09:23:21.072Z
sender:
@prematurata:matrix.org
content:
Thanks for this. Will it ever be specified in graypaper? Especially the "fairly recent" or "reasonable conditions"?

id:
1448
timestamp:
2025-02-05T09:24:28.721Z
sender:
@prematurata:matrix.org
content:
I think the same might be applied on 14.14 which basically is implying that there is a datastore from the MerkleTreeRoot and its "preimage"

id:
1447
timestamp:
2025-02-05T09:40:07.950Z
sender:
@gav:polkadot.io
content:
> <@prematurata:matrix.org> Thanks for this. Will it ever be specified in graypaper? Especially the "fairly recent" or "reasonable conditions"?

Guaranteeing is a strategic endeavour, and the Gray Paper generally avoids dictating strategy. 

id:
1446
timestamp:
2025-02-05T09:43:03.304Z
sender:
@gav:polkadot.io
content:
However the aspect of “fairly recent” is well-specified in terms of on-chain behaviour. See equations 12.4-12.8

id:
1445
timestamp:
2025-02-05T09:43:59.333Z
sender:
@gav:polkadot.io
content:
Of course a guarantor would need to guess when their guaranteed work-report would likely make it to accumulation in order to apply this limit to the strategy of guaranteeing. 

id:
1444
timestamp:
2025-02-05T11:34:00.383Z
sender:
@gav:polkadot.io
content:
But given that guaranteers are responsible for the same core for 10 blocks at a time and receive all guarantees across (even across other cores), then it's quite possible to make a pretty decent guess on the quality and size of the backlog and how long it might be before the new report would make it on-chain.

id:
1443
timestamp:
2025-02-05T11:34:09.800Z
sender:
@gav:polkadot.io
content:
* But given that guarantors are responsible for the same core for 10 blocks at a time and receive all guarantees across (even across other cores), then it's quite possible to make a pretty decent guess on the quality and size of the backlog and how long it might be before the new report would make it on-chain.

id:
1442
timestamp:
2025-02-05T11:34:20.893Z
sender:
@gav:polkadot.io
content:
* But given that guarantors are responsible for the same core for 10 blocks at a time and receive all guarantees across (even across other cores), then it's quite possible to make a pretty decent guess on the quality and size of the core's backlog and how long it might be before the new report would make it on-chain.

id:
1441
timestamp:
2025-02-05T11:44:58.487Z
sender:
@gav:polkadot.io
content:
See also 11.40/41: https://graypaper.fluffylabs.dev/#/4bb8fd2/150302152d02

id:
2978
timestamp:
2025-02-05T13:49:20.875Z
sender:
@decentration:matrix.org
content:
i see in many_dependencies-1, that input.slot and input.guarantees[0].slot are both `14`, which could produce a false positive 

id:
2977
timestamp:
2025-02-05T13:50:17.600Z
sender:
@decentration:matrix.org
content:
 * i see in `many\_dependencies-1.json`, that `input.slot` and `input.guarantees\[0\].slot` are both `14`, which could produce a false positive

id:
2976
timestamp:
2025-02-05T13:54:03.593Z
sender:
@decentration:matrix.org
content:
Do we need to enforce a particular order for the Set of Ed25519 keys, Reporters (R)? 

from gp 11.26 i believe we don't, but just checking if we need it to go beyond and conform to the order of test vector exactly. e.g. r[eports_with_dependencies-1.json](https://github.com/davxy/jam-test-vectors/blob/553ba30d80d39f892fbd74806ba21152e01bd8a5/reports/tiny/reports_with_dependencies-1.json#L337)

id:
2975
timestamp:
2025-02-05T14:09:11.157Z
sender:
@gav:polkadot.io
content:
Do you have a GP link?

id:
2974
timestamp:
2025-02-05T14:10:47.985Z
sender:
@decentration:matrix.org
content:
> <@gav:polkadot.io> Do you have a GP link?

https://graypaper.fluffylabs.dev/#/579bd12/15760015af00

id:
2973
timestamp:
2025-02-05T14:12:53.047Z
sender:
@decentration:matrix.org
content:
> <@gav:polkadot.io> Do you have a GP link?

 * https://graypaper.fluffylabs.dev/#/579bd12/15760015af00

i ordered from ascending validator_index but seems test vector is slightly different

id:
2972
timestamp:
2025-02-05T14:15:46.350Z
sender:
@decentration:matrix.org
content:
 * https://graypaper.fluffylabs.dev/#/4bb8fd2/15b80015b800

i ordered from ascending validator\_index but seems test vector is slightly different

id:
2971
timestamp:
2025-02-05T14:15:48.316Z
sender:
@gav:polkadot.io
content:
11.26 only provides constraints for the items.

id:
2970
timestamp:
2025-02-05T14:16:02.711Z
sender:
@gav:polkadot.io
content:
11.25 constrains the ordering of the items.

id:
2969
timestamp:
2025-02-05T14:16:23.655Z
sender:
@gav:polkadot.io
content:
(by `v`).

id:
1440
timestamp:
2025-02-05T14:17:31.849Z
sender:
@prematurata:matrix.org
content:
Thanks gavin. I also have another question about the new fetch. Is it intended for μ′_{o⋅⋅⋅+l

id:
1439
timestamp:
2025-02-05T14:19:53.699Z
sender:
@prematurata:matrix.org
content:
* Thanks gavin. I also have another question about the new fetch. 

- Is it intended for μ′\_{o⋅⋅⋅+l} to be updated even in case of a panic? Ex when w9 is 5 and memory between w10 and +32 is readable.

- also what is bold x? 

id:
2968
timestamp:
2025-02-05T14:26:27.832Z
sender:
@decentration:matrix.org
content:
ok ty, yes i'm ordering by v [0,1,2,3,4,5] but seems test vector is doing something different, i.e. [1,0,4,5,3,2] in reports_with_dependendencies-1.json, will pr in the repo. 

id:
1438
timestamp:
2025-02-05T14:41:06.731Z
sender:
@gav:polkadot.io
content:
In the case of a panic, memory remains the same (though since panic at the top level is unrecoverable, it doesn’t really matter)

id:
1437
timestamp:
2025-02-05T14:43:27.487Z
sender:
@gav:polkadot.io
content:
There is a superfluous condition there - the additional omega_9 term for a panic. This will be removed in the next revision. This will then make it very clear that memory does not change in the case of a panic. 

id:
2967
timestamp:
2025-02-05T14:44:12.273Z
sender:
@decentration:matrix.org
content:
 * ok ty, yes i'm ordering by v \[0,1,2,3,4,5\] but seems test vector is doing something different, i.e. \[1,0,4,5,3,2\] in reports\_with\_dependendencies-1.json, will [pr in the repo](https://github.com/davxy/jam-test-vectors/issues/15). 

id:
2966
timestamp:
2025-02-05T15:11:53.735Z
sender:
@decentration:matrix.org
content:
ok i see now test vectors are ordered lexicographically and not by validator index, so i will go with that approach, ty. 

id:
1436
timestamp:
2025-02-06T06:51:48.112Z
sender:
@dakkk:matrix.org
content:
did you discovered what that bold x is? 

id:
1435
timestamp:
2025-02-06T08:42:26.678Z
sender:
@prematurata:matrix.org
content:
no, I have no idea. my first guess would have been A.7 but according to that it seems bold x woudld then be a set containing numbers \in N_{2^32} and that would make no sense in the fetch definition

id:
1434
timestamp:
2025-02-06T11:50:13.418Z
sender:
@gav:polkadot.io
content:
@room : [v0.6.2](https://github.com/gavofyork/graypaper/releases/tag/v0.6.2) is out - contains all the latest corrections.

id:
1433
timestamp:
2025-02-06T11:50:57.603Z
sender:
@gav:polkadot.io
content:
Bold x is any value which satisfies the various conditions. You'll find that practically speaking, there's only one which anyone could possibly know.

id:
1432
timestamp:
2025-02-06T11:51:05.708Z
sender:
@gav:polkadot.io
content:
* Bold x is any value which satisfies the various conditions on it. You'll find that practically speaking, there's only one which anyone could possibly know.

id:
1431
timestamp:
2025-02-06T11:51:15.631Z
sender:
@gav:polkadot.io
content:
* Bold x is any value which satisfies the various conditions on it. You'll find that practically speaking, there's only one which anyone could reasonably know.

id:
1430
timestamp:
2025-02-06T11:54:11.700Z
sender:
@gav:polkadot.io
content:
It's used specifically for extrinsics, where an extrinsic is specified in the WP as a hash/len. We assert that we know the preimage since we would not get to this part of the guarantee pipeline without knowing.

id:
1429
timestamp:
2025-02-06T11:55:31.655Z
sender:
@gav:polkadot.io
content:
Alternatively it could have been made explicit with an additional term being passed in to the Refine function and its context, but that would have just complicated the formulation.

id:
1428
timestamp:
2025-02-06T11:56:58.782Z
sender:
@gav:polkadot.io
content:
The GP isn't about describing a node's internal _data-logistics_; that's an implementation-specific consideration. It only concerns outward *behaviour*.

id:
2965
timestamp:
2025-02-06T13:23:46.280Z
sender:
@clearloop:matrix.org
content:
curious how many teams are now following jamnp? when will there be a stable spec for the network implementation?

id:
2964
timestamp:
2025-02-06T13:25:25.681Z
sender:
@clearloop:matrix.org
content:
* curious how many teams are now following jamnp? when will there be a stable spec for the network (p2p) implementation?

id:
2963
timestamp:
2025-02-06T13:28:50.996Z
sender:
@clearloop:matrix.org
content:
* curious how many teams are now following jamnp? when will there be a stable spec for the network (p2p) implementation? will there be an official transaction sender client for testing & debugging?

id:
2962
timestamp:
2025-02-06T13:53:50.865Z
sender:
@dave:parity.io
content:
JAM-SNP is stable modulo bugs, so I assume you mean the full protocol? Don't know the answer to that. For now you should just implement SNP. The conformance testing suite will almost certainly contain networking tests. This does not exist yet though so don't really have details.

id:
2961
timestamp:
2025-02-06T14:29:53.719Z
sender:
@clearloop:matrix.org
content:
I'm not pretty sure about some components of JAM-SNP so I was hoping those uncertain stuffs are not 100% required that I can hack or trim them 😂 

id:
1427
timestamp:
2025-02-06T16:02:53.615Z
sender:
@emielsebastiaan:matrix.org
content:
> <@gav:polkadot.io> @room : [v0.6.2](https://github.com/gavofyork/graypaper/releases/tag/v0.6.2) is out - contains all the latest corrections.

3 PRs with pvm instruction modifications are pending review at your convenience. 

id:
1426
timestamp:
2025-02-06T16:05:47.643Z
sender:
@gav:polkadot.io
content:
Merged one - will wait for Jan Bujak to take a look at the others.

id:
2960
timestamp:
2025-02-06T16:06:46.966Z
sender:
@gav:polkadot.io
content:
JAM-NP is scheduled to be added as an appendix in the 0.7/0.8 series.

id:
2959
timestamp:
2025-02-06T16:08:19.589Z
sender:
@gav:polkadot.io
content:
* JAM-NP is scheduled to be added as an appendix in the 0.7/0.8 series. We'll need a decent amount of Toaster-experimentation to gain some confidence that the network protocol is capable of delivering the sort of performance we need.

id:
1425
timestamp:
2025-02-06T16:09:45.740Z
sender:
@jan:parity.io
content:
Yeah, since in the GP we "store" the values unsigned those were definitely missing the conversions back, LGTM.

id:
1424
timestamp:
2025-02-06T16:24:26.511Z
sender:
@gav:polkadot.io
content:
All merged - will be in 0.6.3

id:
1423
timestamp:
2025-02-06T16:41:55.863Z
sender:
@emielsebastiaan:matrix.org
content:
I may have found one more for 0.6.3: https://github.com/gavofyork/graypaper/pull/231
Header / Bandersnatch related (`H_a`)

id:
1422
timestamp:
2025-02-07T14:00:36.475Z
sender:
@cisco:parity.io
content:
Should the zero host function have the same error condition as the void host function? The one about pages being inaccessible

https://graypaper.fluffylabs.dev/#/5f542d7/35a90235a902

id:
1421
timestamp:
2025-02-07T14:30:19.790Z
sender:
@cisco:parity.io
content:
The logic for getting the service in the read host function is very similar to the lookup host function: https://graypaper.fluffylabs.dev/#/5f542d7/305c00305c00

They could be unified to make it easier to read.

id:
2958
timestamp:
2025-02-07T14:45:13.883Z
sender:
@ycc3741:matrix.org
content:
Does implementing BLS Signatures require a PoPs version, or is a non-PoPs version sufficient?

id:
2957
timestamp:
2025-02-07T14:45:47.715Z
sender:
@ycc3741:matrix.org
content:
Refer to https://eprint.iacr.org/2022/1611.pdf.

id:
1420
timestamp:
2025-02-07T15:09:48.986Z
sender:
@gav:polkadot.io
content:
In the case of `void`, the pages must all be accessible because we're making them inaccessible.

id:
1419
timestamp:
2025-02-07T15:10:18.226Z
sender:
@gav:polkadot.io
content:
We wouldn't want exactly that condition for `zero` whose job is to initialize pages to being accessible and zero.

id:
1418
timestamp:
2025-02-07T15:11:46.078Z
sender:
@gav:polkadot.io
content:
We could introduce a condition to require them to be previously inaccessible, but currently we don't. This was intented, but maybe it should be changed if requiring it bring us more performance impls ( Jan Bujak ?)

id:
1417
timestamp:
2025-02-07T15:11:58.447Z
sender:
@gav:polkadot.io
content:
Feel free to make a PR :P

id:
1416
timestamp:
2025-02-07T15:12:08.154Z
sender:
@gav:polkadot.io
content:
* PRs are considered :P

id:
1415
timestamp:
2025-02-07T15:13:15.099Z
sender:
@cisco:parity.io
content:
Will do

id:
1414
timestamp:
2025-02-07T15:13:29.244Z
sender:
@cisco:parity.io
content:
* Will make a PR for that unification 

id:
1413
timestamp:
2025-02-07T15:30:03.842Z
sender:
@jan:parity.io
content:
The lack of requirement in `zero` that the pages are inaccessible is indeed intended so that e.g. this host call can be used to clear/reinitialize memory without first having to void it nor track what is already allocated. Requiring it wouldn't really change anything performance-wise since you have to (or the OS has to) iterate over the page map to find the holes to fill anyway. (And in practice not requiring this check can make things simpler because you can just ask the OS to zero allocate an address range in bulk instead of having to do this yourself.)

Hm, but now that I think about it we probably should make `void` not require the pages to be accessible either, as that could simplify its usage in certain cases (and, same as with `zero`, the page map has to be iterated over anyway by the implementation, whether it returns an error or not). Basically have it take a range of pages we want it to free, and when it returns it'll guarantee that the whole range is now free. So the `HUH` error branch could just be deleted altogether (since voiding the first 64k or out-of-bounds would be a no-op as there can't never be anything allocated there anyway).

id:
2956
timestamp:
2025-02-07T16:09:52.364Z
sender:
@wind.sixwings:matrix.org
content:
Is there a bls signature length defined in GP? We need to ensure that the bls signature length is consistent

id:
2955
timestamp:
2025-02-08T21:40:15.190Z
sender:
@decentration:matrix.org
content:
this is helpful thread, thanks. so to pass conformance test use "node" instead of "peak" as prefix. 

id:
2954
timestamp:
2025-02-08T23:22:46.607Z
sender:
@rustybot:matrix.org
content:
> <@decentration:matrix.org> this is helpful thread, thanks. so to pass conformance test use "node" instead of "peak" as prefix. 

Vectors were regenerated to use "peak". Have you tried to pull the last rev?

id:
1412
timestamp:
2025-02-09T02:00:28.626Z
sender:
@ascriv:matrix.org
content:
Is anyone aware of any Go libraries which implement bandersnatch vrf signatures? Seems the main implementations are in Rust for now

id:
1411
timestamp:
2025-02-09T03:02:06.144Z
sender:
@ascriv:matrix.org
content:
* Is anyone aware of any non-rust libraries which implement bandersnatch vrf signatures? Seems the main implementations are in Rust for now

id:
2953
timestamp:
2025-02-09T15:23:38.173Z
sender:
@decentration:matrix.org
content:
> <@rustybot:matrix.org> Vectors were regenerated to use "peak". Have you tried to pull the last rev?

oh no i didnt, ty 👍

id:
1410
timestamp:
2025-02-09T17:08:26.526Z
sender:
@jay_ztc:matrix.org
content:
Not aware of any other than davxys work on this. Have it on my list to investigate as a potential SPOF for the network.

id:
1409
timestamp:
2025-02-09T18:55:48.774Z
sender:
@davxy:matrix.org
content:
> <@ascriv:matrix.org> Is anyone aware of any non-rust libraries which implement bandersnatch vrf signatures? Seems the main implementations are in Rust for now

AFAIK, `arc-ec-vrfs` is currently the only implementation available. This is likely because the scheme is not standardized, thus any existing implementation should be tied to JAM.

For those interested in implementing it, the details of Bandersnatch VRFs are soecified here: https://github.com/davxy/bandersnatch-vrfs-spec. Implementing the "plain" VRF is **relatively** straightforward, especially with the support of a "bigint" library, making it a manageable task.

However, the complexity increases significantly when dealing with the ring-VRF variant. Even though it is thoroughly specified here: https://github.com/davxy/ring-proof-spec, implementing it requires a library that supports the underlying SNARK framework it relies on. Since there are no official standards (only de facto ones, at best), you’ll likely need to build many things from scratch. 

We have been using [arkworks](https://arkworks.rs/) for this, as it provides **most** of the tools necessary. That said, some additional components were developed by the W3F team and ourselves to fill in the gaps anyway.

In summary, if someone is inclined to implement any of these components (also by building over arkworks, for example), I’m available to provide some support. Additionally, test vectors are available for validating conformance.

id:
1408
timestamp:
2025-02-09T19:20:39.888Z
sender:
@ascriv:matrix.org
content:
Since implementing it from scratch is so challenging, it seems essentially everyone will rely on the one implementation that is available (using something like go’s FFI if they’re not implementing jam in rust), which becomes a redundancy and a failure point. It’s probably well audited and as a single point of failure still probably ok though.

id:
1407
timestamp:
2025-02-09T19:21:44.807Z
sender:
@ascriv:matrix.org
content:
I would love if people reimplement it of their own volition in their chosen language but I think we might need additional incentives if we do think relying on just this one implementation is in fact an issue 

id:
1406
timestamp:
2025-02-09T22:16:12.673Z
sender:
@davxy:matrix.org
content:
> <@ascriv:matrix.org> Is anyone aware of any Go libraries which implement bandersnatch vrf signatures? Seems the main implementations are in Rust for now

* AFAIK, `arc-ec-vrfs` is currently the only implementation available. This is likely because the scheme is not standardized, thus any existing implementation should be tied to JAM.

For those interested in implementing it, the details of Bandersnatch VRFs are soecified here: https://github.com/davxy/bandersnatch-vrfs-spec. Implementing the "plain" VRF is **relatively** straightforward, especially with the support of a "bigint" library, making it a manageable task.

However, the complexity increases significantly when dealing with the ring-VRF variant. Even though it is thoroughly specified here: https://github.com/davxy/ring-proof-spec , implementing it requires a library that supports the underlying SNARK framework it relies on. Since there are no official standards (only de facto ones, at best), you’ll likely need to build many things from scratch. 

We have been using [arkworks](https://arkworks.rs/) for this, as it provides **most** of the tools necessary. That said, some additional components were developed by the W3F team and ourselves to fill in the gaps anyway.

In summary, if someone is inclined to implement any of these components (also by building over arkworks, for example), I’m available to provide some support. Additionally, test vectors are available for validating conformance.

id:
2952
timestamp:
2025-02-10T11:38:07.364Z
sender:
@sourabhniyogi:matrix.org
content:
Here is our open source implementation of erasure coding in Go ([Appendix H](https://graypaper.fluffylabs.dev/#/5f542d7/3c11003c1100)) with a matching [erasure coding test vector](https://github.com/w3f/jamtestvectors/pull/35):
 https://github.com/jam-duna/jamtestnet/tree/main/erasurecoding
We would love others providing bug reports on this -- once its believed to be compliant by 2+ teams, we imagine other language implementations can match.


id:
1405
timestamp:
2025-02-10T18:35:06.197Z
sender:
@vinsystems:matrix.org
content:
In the preimages extrinsic:

1.- Do the service-data pairs have to be [ordered](https://graypaper.fluffylabs.dev/#/5f542d7/184800184800) *only* by service id? Or do we also have to consider the order of the data blob?
2.- The R function of the eq [(12.30)](https://graypaper.fluffylabs.dev/#/5f542d7/185e00185e00) is the same that the one defined in eq [(12.23)](https://graypaper.fluffylabs.dev/#/5f542d7/17ce0317ce03)?

id:
1404
timestamp:
2025-02-10T18:35:58.063Z
sender:
@vinsystems:matrix.org
content:
* In the preimages extrinsic:

1.- Do the service-data pairs have to be [ordered](https://graypaper.fluffylabs.dev/#/5f542d7/184800184800) _only_ by service id? Or do we also have to consider the order of the data blob?
2.- The R function of the eq [(12.30)](https://graypaper.fluffylabs.dev/#/5f542d7/185e00185e00) is the one defined in eq [(12.23)](https://graypaper.fluffylabs.dev/#/5f542d7/17ce0317ce03)?

id:
1403
timestamp:
2025-02-10T18:36:38.441Z
sender:
@vinsystems:matrix.org
content:
* In the preimages extrinsic:

1.- Do the service-data pairs have to be [ordered](https://graypaper.fluffylabs.dev/#/5f542d7/184800184800) _only_ by service id? Or do we also have to consider the order of the data blob?
2.- Is the R function of eq [(12.30)](https://graypaper.fluffylabs.dev/#/5f542d7/185e00185e00) the one defined in eq [(12.23)](https://graypaper.fluffylabs.dev/#/5f542d7/17ce0317ce03)?

id:
1402
timestamp:
2025-02-10T18:40:53.755Z
sender:
@vinsystems:matrix.org
content:
* 1.- In the preimages extrinsic, do the service-data pairs have to be [ordered](https://graypaper.fluffylabs.dev/#/5f542d7/184800184800) _only_ by service id? Or do we also have to consider the order of the data blob?
2.- Is the R function of eq [(12.30)](https://graypaper.fluffylabs.dev/#/5f542d7/185e00185e00) the one defined in eq [(12.23)](https://graypaper.fluffylabs.dev/#/5f542d7/17ce0317ce03)?

id:
2951
timestamp:
2025-02-10T19:10:30.446Z
sender:
@lucy_coulden:matrix.org
content:
Attended the first JAM Tour session at Cambridge University which was cracking!
I have a couple of questions…
- how do people apply to be involved as a builder?
- I believe there are 35 official teams. Is there a list of these teams and where they are based? We are doing a big push in the US and exploring a physical footprint which may be called a JAM space and where people could go to build and be together with others in the Polkadot ecosystem.

id:
2950
timestamp:
2025-02-10T19:12:36.350Z
sender:
@xlchen:matrix.org
content:
you can find registered teams here https://graypaper.com/clients/

id:
1401
timestamp:
2025-02-10T19:42:30.066Z
sender:
@gav:polkadot.io
content:
1. Both, primarily service ID and then data blob. Same goes for any tuple. 

id:
1400
timestamp:
2025-02-10T19:44:11.263Z
sender:
@gav:polkadot.io
content:
2. No. R in 12.30 and 12.31 are the same. R in 12.23 and 2.24 are the same. I might rename one of them to avoid the confusion. 

id:
1399
timestamp:
2025-02-10T19:45:37.102Z
sender:
@vinsystems:matrix.org
content:
got it, thanks 👍️

id:
1398
timestamp:
2025-02-10T19:58:33.040Z
sender:
@leonidas_m:matrix.org
content:
The GP assumes that we know the preimage data because there's only one value that satisfies the necessary conditions. However, since the Refine function neither allows passing the preimage explicitly as a parameter nor permits querying the state, it's unclear where this data should come from.

Should the preimage be retrieved from a node's internal data store (local database), or is it expected to be fetched from an external source (e.g., network)?

id:
2949
timestamp:
2025-02-10T20:06:34.131Z
sender:
@jay_ztc:matrix.org
content:
It's worth calling out there is a strict clean room requirement for validator implementations, love this as a long pole idea for post M5 

id:
1397
timestamp:
2025-02-10T21:23:18.207Z
sender:
@gav:polkadot.io
content:
The latter. 

id:
1396
timestamp:
2025-02-10T21:23:52.204Z
sender:
@gav:polkadot.io
content:
Validator nodes are expected to be sent preimages directly from external sources. 

id:
1395
timestamp:
2025-02-10T21:24:13.750Z
sender:
@gav:polkadot.io
content:
* Validator nodes are expected to be sent solicited preimages directly from external sources. 

id:
2948
timestamp:
2025-02-10T21:32:07.647Z
sender:
@ascriv:matrix.org
content:
+1 for jam hacker spaces post m5 

id:
2947
timestamp:
2025-02-10T21:56:23.887Z
sender:
@sourabhniyogi:matrix.org
content:
image.png

id:
2946
timestamp:
2025-02-10T21:56:27.379Z
sender:
@sourabhniyogi:matrix.org
content:
Some JAM implementers are getting together in a "JAM0 Meetup #2" call in the next 24 hours.  If you are implementing JAM, please add this to your calendar and DM me (@sourabhniyogi) and I'll you to the "JAM0" Element channel where a meeting link will be dropped.



id:
2945
timestamp:
2025-02-10T21:59:08.185Z
sender:
@jaymansfield:matrix.org
content:
> <@sourabhniyogi:matrix.org> Some JAM implementers are getting together in a "JAM0 Meetup #2" call in the next 24 hours.  If you are implementing JAM, please add this to your calendar and DM me (@sourabhniyogi) and I'll you to the "JAM0" Element channel where a meeting link will be dropped.
> 
> 

Looking forward to meeting the other teams tomorrow!

id:
2944
timestamp:
2025-02-11T01:25:10.123Z
sender:
@lucy_coulden:matrix.org
content:
Noted and thanks Jay ☺️

id:
2943
timestamp:
2025-02-11T01:25:33.713Z
sender:
@lucy_coulden:matrix.org
content:
> <@xlchen:matrix.org> you can find registered teams here https://graypaper.com/clients/

Thanks Bryan!

id:
2942
timestamp:
2025-02-11T01:29:38.221Z
sender:
@lucy_coulden:matrix.org
content:
> <@xlchen:matrix.org> you can find registered teams here https://graypaper.com/clients/

 * Thanks Bryan! Is there anything that lists where in the world the teams are based?

id:
2941
timestamp:
2025-02-11T04:41:43.576Z
sender:
@xlchen:matrix.org
content:
I am not aware of any public list with team location. Some of the teams are remote / global also.

id:
2940
timestamp:
2025-02-11T12:25:58.046Z
sender:
@alice_und_bob:matrix.org
content:
can services pull in memory pages from other services?

I am asking because I wonder how separated different applications in JAM are across the service boundary. E.g. can corechains' actions be based on the state of a coreplay agent?

id:
2939
timestamp:
2025-02-11T12:28:32.964Z
sender:
@alice_und_bob:matrix.org
content:
* can services pull in memory pages from other services?

I am asking because I wonder how separated different applications in JAM are across the service boundary. E.g. can a corechains chain's logic be based on the state of a coreplay agent?

id:
2938
timestamp:
2025-02-11T13:26:12.628Z
sender:
@bkchr:parity.io
content:
Jam doesn't know the concept of memory pages. But yeah, you can put multiple items of different services into one `WorkPackage`

id:
2937
timestamp:
2025-02-11T13:52:21.829Z
sender:
@alice_und_bob:matrix.org
content:
thanks basti.

who then knows about the memory pages? is it an optional additional service? does JAM not know about the DDL?

id:
2936
timestamp:
2025-02-11T13:52:42.294Z
sender:
@bkchr:parity.io
content:
DDL?

id:
2935
timestamp:
2025-02-11T13:53:00.184Z
sender:
@alice_und_bob:matrix.org
content:
distributed data lake

id:
2934
timestamp:
2025-02-11T13:53:09.657Z
sender:
@bkchr:parity.io
content:
Yes there will be a service that handles these memory pages etc 

id:
2933
timestamp:
2025-02-11T13:53:17.333Z
sender:
@bkchr:parity.io
content:
You mean DA?

id:
2932
timestamp:
2025-02-11T13:53:22.421Z
sender:
@bkchr:parity.io
content:
* You mean the da layer?

id:
2931
timestamp:
2025-02-11T13:53:34.788Z
sender:
@gav:polkadot.io
content:
Yes - D3L is what I'm calling JAM's DA

id:
2930
timestamp:
2025-02-11T13:54:01.360Z
sender:
@gav:polkadot.io
content:
* Yes - D3L (decentralised, distributed, data lake) is what I'm calling JAM's DA

id:
2929
timestamp:
2025-02-11T13:54:14.934Z
sender:
@alice_und_bob:matrix.org
content:
I see. So JAM is like a microkernel that is completely stateless and then D3L listens to the outputs of cores and keeps it available for 28 days?

id:
2928
timestamp:
2025-02-11T13:54:27.086Z
sender:
@alice_und_bob:matrix.org
content:
* I see. So JAM is like a microkernel that is completely stateless and then D3L is a service that listens to the outputs of cores and keeps it available for 28 days?

id:
2927
timestamp:
2025-02-11T13:54:27.618Z
sender:
@gav:polkadot.io
content:
JAM is more like the virtual hardware.

id:
2926
timestamp:
2025-02-11T13:54:55.975Z
sender:
@gav:polkadot.io
content:
And e.g. CorePlay would be more akin to the OS kernel which sits atop that.

id:
2925
timestamp:
2025-02-11T13:55:06.807Z
sender:
@gav:polkadot.io
content:
And actors in CorePlay would be the software running on the OS.

id:
2924
timestamp:
2025-02-11T13:55:46.298Z
sender:
@gav:polkadot.io
content:
the D3L has no concept of services. Once data it in there it's not associated with any service and thus any service may read it.

id:
2923
timestamp:
2025-02-11T13:56:01.642Z
sender:
@gav:polkadot.io
content:
It is temporary and immutable.

id:
2922
timestamp:
2025-02-11T13:56:24.684Z
sender:
@gav:polkadot.io
content:
* It is temporary and immutable, which means once data is added it cannot be altered and it only lasts for a limited period of time in there (28 days) before it must be re-added.

id:
2921
timestamp:
2025-02-11T13:56:51.976Z
sender:
@alice_und_bob:matrix.org
content:
and D3L as a service... who is it governed by? Is it immutable except for hardforks or is it under the control of an on-chain governance mechanism?

id:
2920
timestamp:
2025-02-11T13:57:49.120Z
sender:
@gav:polkadot.io
content:
Some services probably won't make much use of it since they will need to have data in some other domain-specific format (e.g. the parachains service will use Substrate's Merkle structure).

id:
2919
timestamp:
2025-02-11T13:58:15.427Z
sender:
@gav:polkadot.io
content:
JAM has not on-chain governance.

id:
2918
timestamp:
2025-02-11T13:58:18.915Z
sender:
@gav:polkadot.io
content:
* JAM has no on-chain governance.

id:
2917
timestamp:
2025-02-11T13:58:22.932Z
sender:
@gav:polkadot.io
content:
It's immutable.

id:
2916
timestamp:
2025-02-11T13:58:59.140Z
sender:
@gav:polkadot.io
content:
It could host on-chain governance schemes of course, but they would be powerless to do anything to JAM's internal state beyond that which they are privileged to change anyway.

id:
2915
timestamp:
2025-02-11T13:59:09.629Z
sender:
@gav:polkadot.io
content:
* It could host on-chain governance schemes of course, but they would be powerless to do anything to JAM's internal state beyond its own corner.

id:
2914
timestamp:
2025-02-11T13:59:10.834Z
sender:
@alice_und_bob:matrix.org
content:
hm, Basti refers to it as a service. As in "JAM service"?

id:
2913
timestamp:
2025-02-11T13:59:25.566Z
sender:
@gav:polkadot.io
content:
Oh - it's not a service like that.

id:
2912
timestamp:
2025-02-11T13:59:33.181Z
sender:
@gav:polkadot.io
content:
Call it a feature if you like.

id:
2911
timestamp:
2025-02-11T13:59:43.521Z
sender:
@alice_und_bob:matrix.org
content:
ah, so more low level. 

id:
2910
timestamp:
2025-02-11T13:59:49.879Z
sender:
@gav:polkadot.io
content:
Yes

id:
2909
timestamp:
2025-02-11T14:01:52.822Z
sender:
@alice_und_bob:matrix.org
content:
so in theory I could co-sequence a corechain and a coreplay actor?

id:
2908
timestamp:
2025-02-11T14:01:59.387Z
sender:
@gav:polkadot.io
content:
Yes.

id:
2907
timestamp:
2025-02-11T14:02:03.633Z
sender:
@gav:polkadot.io
content:
Services' on-chain (fully synchronous and coherent) state may be read directly and more or less synchronously by other services during accumulate. They may also be read and written by the owning service during accumulate. But services cannot synchronously interact in accumulate.

id:
2906
timestamp:
2025-02-11T14:02:11.480Z
sender:
@alice_und_bob:matrix.org
content:
lovely

id:
2905
timestamp:
2025-02-11T14:02:15.185Z
sender:
@alice_und_bob:matrix.org
content:
thx a lot!

id:
2904
timestamp:
2025-02-11T14:04:11.362Z
sender:
@alice_und_bob:matrix.org
content:
oh but so services could only compose synchronously if they have a trust-relationship?

id:
2903
timestamp:
2025-02-11T14:05:53.232Z
sender:
@gav:polkadot.io
content:
It makes little sense to do synchronous composition without trust. 

id:
2902
timestamp:
2025-02-11T14:06:16.550Z
sender:
@gav:polkadot.io
content:
You pretty much need to be able to trust that they'd do the thing you expect if you're going to interact at all.

id:
2901
timestamp:
2025-02-11T14:06:41.467Z
sender:
@gav:polkadot.io
content:
* You pretty much need to be able to trust that they'd do the thing you expect if you're going to interact (directly) at all.

id:
2900
timestamp:
2025-02-11T14:07:40.381Z
sender:
@gav:polkadot.io
content:
For synchronous composition, you need two things: to establish, in Refine, some causally-entangled effects.

id:
2899
timestamp:
2025-02-11T14:07:42.907Z
sender:
@gav:polkadot.io
content:
* For synchronous composition, you need two things: 1. to establish, in Refine, some causally-entangled effects.

id:
2898
timestamp:
2025-02-11T14:08:10.448Z
sender:
@gav:polkadot.io
content:
2. To avoid enacting those effects within Accumulate until you're certain that the counter-party is committed to doing the same.

id:
2897
timestamp:
2025-02-11T14:08:43.351Z
sender:
@gav:polkadot.io
content:
* 2. To enact those effects within Accumulate if any only if you're certain that the counter-party will, at some point, do the same.

id:
2896
timestamp:
2025-02-11T14:08:49.862Z
sender:
@gav:polkadot.io
content:
* 2. To enact those effects within Accumulate if and only if you're certain that the counter-party will, at some point, do the same.

id:
2895
timestamp:
2025-02-11T14:09:56.028Z
sender:
@gav:polkadot.io
content:
* 2. To enact those effects within Accumulate if and only if you're certain that the counter-party will, at some point, do the same. I call this the Integration Gate.

id:
2894
timestamp:
2025-02-11T14:10:10.725Z
sender:
@gav:polkadot.io
content:
Generally I'd expect that causality to be established by co-scheduling within the same Work Package, since this way you're more certain that they'll both get to the Integration Gate.

id:
2893
timestamp:
2025-02-11T14:10:30.774Z
sender:
@gav:polkadot.io
content:
* Generally I'd expect that causality to be established by co-scheduling within the same Work Package, since this way you're more certain that they'll both get to the Integration Gate. It also means that both Refines have access to exactly the same information.

id:
2892
timestamp:
2025-02-11T14:11:36.375Z
sender:
@gav:polkadot.io
content:
It could be done with these being the case, but then you'd need to extra logic to help establish what effect the Integration Gate should really be gating for (e.g. point 1 above);.

id:
2891
timestamp:
2025-02-11T14:11:37.741Z
sender:
@gav:polkadot.io
content:
* It could be done with these being the case, but then you'd need to extra logic to help establish what effect the Integration Gate should really be gating for (e.g. point 1 above).

id:
2890
timestamp:
2025-02-11T14:12:49.112Z
sender:
@gav:polkadot.io
content:
Point 2 will likely be done through each services send the other a `transfer` message establishing that they will certainly integrate their part of the entangled effects once they receive the same message from the other side.

id:
2889
timestamp:
2025-02-11T14:13:22.803Z
sender:
@gav:polkadot.io
content:
A simple way of identifying the entangled effects would be the hash of the work-package.

id:
2888
timestamp:
2025-02-11T14:13:42.505Z
sender:
@gav:polkadot.io
content:
* A simple way of identifying the entangled effects would be the hash of the work-package, which would easily fit inside the 64 bytes of the transfer memo.

id:
2887
timestamp:
2025-02-11T14:13:50.703Z
sender:
@alice_und_bob:matrix.org
content:
hm, and intuitively I would say it would create too much complexity to consider this for now

id:
2886
timestamp:
2025-02-11T14:14:21.436Z
sender:
@alice_und_bob:matrix.org
content:
does this have any impact on how we look at XCMP? or is XCMP constrained to corechains?

id:
2885
timestamp:
2025-02-11T14:14:27.436Z
sender:
@gav:polkadot.io
content:
Well it's inherently buildable on top of what we have, and I'll probably try to write a simple service which shows it in action.

id:
2884
timestamp:
2025-02-11T14:15:21.335Z
sender:
@gav:polkadot.io
content:
Unclear as yet. There's a few possibilities.

id:
2883
timestamp:
2025-02-11T14:15:58.906Z
sender:
@gav:polkadot.io
content:
XCMP could be dropped and Hyperbridge used instead.

id:
2882
timestamp:
2025-02-11T14:16:14.557Z
sender:
@gav:polkadot.io
content:
Or we could expand XCMP to a JAM-wide service.

id:
2881
timestamp:
2025-02-11T14:16:42.892Z
sender:
@gav:polkadot.io
content:
* Or we could expand XCMP to a JAM-wide service and make it be able to deliver messages not just into the parachains service but also into CorePlay and whatever else.

id:
2880
timestamp:
2025-02-11T14:17:17.230Z
sender:
@gav:polkadot.io
content:
Or we could keep it scoped as-is and potentially introduce another message-passing system specifically for inter-service messaging if it sufficiently desired.

id:
2879
timestamp:
2025-02-11T14:17:49.982Z
sender:
@gav:polkadot.io
content:
For short, expensive messages, we could also keep HRMP as a backup option.

id:
2878
timestamp:
2025-02-11T14:19:10.342Z
sender:
@gav:polkadot.io
content:
* It could be done over multiple Work Packages, but then you'd need to extra logic to help establish what effect the Integration Gate should really be gating for (e.g. point 1 above).

id:
2877
timestamp:
2025-02-11T14:27:26.001Z
sender:
@danicuki:matrix.org
content:
JAM Call link: https://meet.google.com/edd-nixn-mps?authuser=0 . Starts in 10  minutes

id:
2876
timestamp:
2025-02-11T14:31:42.726Z
sender:
@philip.poloczek:parity.io
content:
Hey Lucy, what's so interesting about the location is the contact not enough? This is often very rough as the teams just formed and not yet put a custom website up or anything which could inform about the teams location.

id:
2875
timestamp:
2025-02-11T14:32:47.158Z
sender:
@philip.poloczek:parity.io
content:
You can always start a team as well. There is artificial limit of participants or "official" teams. The list just includes teams which made it public they formed and started working on JAM. :)

id:
2874
timestamp:
2025-02-11T14:33:26.160Z
sender:
@philip.poloczek:parity.io
content:
Not all of them might be ending up implementing all milestones or even one. It's quite the challenge after all, so everything is still up for grabs! :)

id:
2873
timestamp:
2025-02-11T14:57:46.269Z
sender:
@jay_ztc:matrix.org
content:
"Artificial limit of teams"?

id:
2872
timestamp:
2025-02-11T15:24:42.920Z
sender:
@alice_und_bob:matrix.org
content:
so to summarize this conversation, JAM is a type 1 hypervisor

id:
2871
timestamp:
2025-02-11T15:26:31.356Z
sender:
@gav:polkadot.io
content:
In some sense, yeah.

id:
2870
timestamp:
2025-02-11T15:27:04.701Z
sender:
@gav:polkadot.io
content:
* In some sense.

id:
2869
timestamp:
2025-02-11T15:27:38.143Z
sender:
@gav:polkadot.io
content:
* In some sense, but it would be a bit of an odd comparison as it's not clear what the hardware on which this hypervisor runs is.

id:
2868
timestamp:
2025-02-11T15:28:11.950Z
sender:
@gav:polkadot.io
content:
I think the analogy of stating that JAM is a (virtual) machine is superior.

id:
2867
timestamp:
2025-02-11T15:30:14.634Z
sender:
@gav:polkadot.io
content:
* I think the analogy of stating that JAM is a (virtual) machine is superior, but I suppose it's reasonable to suggest that it includes elements of not only the low-level "computer" but also some functionality associated with more OS/hypervisor-level software.

id:
2866
timestamp:
2025-02-11T15:54:46.693Z
sender:
@danicuki:matrix.org
content:
Here is the link for subscription to the JAM Experience event in May. Please, subscribe only if you have high chance of coming. As it will be a free event and we need to plan foods and drinks according to number of people. 

https://lu.ma/ob0n7pdy

This link is only for JAM teams. I will create another event for the JAM Party. 

id:
2865
timestamp:
2025-02-11T16:00:32.388Z
sender:
@philip.poloczek:parity.io
content:
 * You can always start a team as well. There is no limit of participating teams or "officially" approved teams. The list just includes teams which made it public they formed and started working on JAM. :)

id:
2864
timestamp:
2025-02-11T16:00:55.651Z
sender:
@philip.poloczek:parity.io
content:
Meant to write "no artificial limiting of teams", updated my message 👍️

id:
1394
timestamp:
2025-02-11T16:04:23.040Z
sender:
@arjanz:matrix.org
content:
I believe to have corrected two small typos in appendix D, \H_0 should be \H^0: https://github.com/gavofyork/graypaper/pull/234

id:
1393
timestamp:
2025-02-11T16:10:40.553Z
sender:
@yu2c:matrix.org
content:
https://matrix.to/#/!ddsEwXlCWnreEGuqXZ:polkadot.io/$aFpJR-J3zqDbApsjZ3qMX-QjOVoZnYSYG2oTSPPCDB0?via=polkadot.io&via=matrix.org&via=parity.io

id:
2863
timestamp:
2025-02-11T16:14:29.080Z
sender:
@jay_ztc:matrix.org
content:
Hi Daniel, thanks for helping organize this event! Our team hasn't "registered" yet, but planning on having two in attendance. Will continue to keep tabs on event updates in the JAM0 channel. Thanks!

id:
2862
timestamp:
2025-02-11T16:14:45.637Z
sender:
@jay_ztc:matrix.org
content:
* Hi Daniel, thanks for helping organize this event! Our team hasn't registered yet, but planning on having two in attendance. Will continue to keep tabs on event updates in the JAM0 channel. Thanks!

id:
2861
timestamp:
2025-02-11T16:17:22.011Z
sender:
@jay_ztc:matrix.org
content:
* Hi Daniel, thanks for helping organize this event! Our team isn't registered, but planning on having two in attendance. Will continue to keep tabs on event updates in the JAM0 channel. Thanks!

id:
2860
timestamp:
2025-02-11T17:57:58.951Z
sender:
@danicuki:matrix.org
content:
> <@jay_ztc:matrix.org> Hi Daniel, thanks for helping organize this event! Our team isn't registered, but planning on having two in attendance. Will continue to keep tabs on event updates in the JAM0 channel. Thanks!

What is your team name? I will add it to the list. 

id:
2859
timestamp:
2025-02-11T18:03:07.834Z
sender:
@jay_ztc:matrix.org
content:
Thanks! "ZTC" 

id:
2858
timestamp:
2025-02-11T19:00:13.258Z
sender:
@danicuki:matrix.org
content:
> <@jay_ztc:matrix.org> Thanks! "ZTC" 

Added. You can register now! 🤗

id:
2857
timestamp:
2025-02-11T19:00:56.840Z
sender:
@danicuki:matrix.org
content:
I took the list from the official gray paper website. I suggest to open a PR to add your team there. 

id:
2856
timestamp:
2025-02-11T19:21:59.416Z
sender:
@danicuki:matrix.org
content:
For those coming to "JAM Experience" in Cascais, I highly recommend to stay a couple more days and attend to ETHLisbon. We are planning a Polkadot / JAM booth there and will probably have some funding for a Hackathon prize  for JAMers. Subscribe here:

https://lu.ma/ETH-Lisbon-2025

id:
2855
timestamp:
2025-02-11T20:01:33.153Z
sender:
@danicuki:matrix.org
content:
Please share your team / personal Twitter so I can mention on posts eventually

id:
2854
timestamp:
2025-02-11T20:05:12.169Z
sender:
@danicuki:matrix.org
content:
Today's call Highlights and video: https://x.com/danicuki/status/1889402796723609833

id:
2853
timestamp:
2025-02-11T20:53:29.783Z
sender:
@jay_ztc:matrix.org
content:
Not on socials, appreciate the thought tho

id:
1392
timestamp:
2025-02-12T00:39:14.979Z
sender:
@charliewinston14:matrix.org
content:
Hello I have a few questions I was hoping someone could help with.

1. Is there a formal definition of a work package bundle other than the textual description in 14.4.1? It mentions the attributes but not the data types of each attribute.
2. The extrinsic data in a work package bundle, is it a list of hashes and lengths? Or is this the full preimages themselves?
3. The exported segments in a work package bundle, or they the actual segments of length 4104? Or are they a list of root and indexes?
4. Is the extrinsic that’s passed in CE133 the same as what’s in a work package bundle? If so why is it included in CE133 if it can just be calculated based off the work package by the receiver using X(w) in 14.14


id:
2852
timestamp:
2025-02-12T06:15:14.030Z
sender:
@clearloop:matrix.org
content:
May I ask if the transaction pool is totally open to implement or there is sort of spec WIP?

id:
2851
timestamp:
2025-02-12T10:47:31.230Z
sender:
@decentration:matrix.org
content:
could someone confirm with me what permutation they produced in the below issue? i cannot seem to verify the validators assignment to cores. Though i was able to pass the [w3f/jamtestvectors/shuffle](https://github.com/w3f/jamtestvectors/tree/master/shuffle) test...

https://github.com/davxy/jam-test-vectors/issues/18




id:
2850
timestamp:
2025-02-12T10:48:57.433Z
sender:
@decentration:matrix.org
content:
 * could someone confirm with me what permutation they produced in this [issue](https://github.com/davxy/jam-test-vectors/issues/18)? i cannot seem to verify the validators assignment to cores. Though i was able to pass the [w3f/jamtestvectors/shuffle](https://github.com/w3f/jamtestvectors/tree/master/shuffle) test...

https://github.com/davxy/jam-test-vectors/issues/18

id:
1391
timestamp:
2025-02-12T13:38:03.535Z
sender:
@prematurata:matrix.org
content:
Hello I thought about opening an issue on graypaper repo instead of reporting here as it was/is a bit complicated matter to write here
 https://github.com/gavofyork/graypaper/issues/239 

id:
1390
timestamp:
2025-02-12T15:06:54.385Z
sender:
@gav:polkadot.io
content:
> <@charliewinston14:matrix.org> Hello I have a few questions I was hoping someone could help with.
> 
> 1. Is there a formal definition of a work package bundle other than the textual description in 14.4.1? It mentions the attributes but not the data types of each attribute.
> 2. The extrinsic data in a work package bundle, is it a list of hashes and lengths? Or is this the full preimages themselves?
> 3. The exported segments in a work package bundle, or they the actual segments of length 4104? Or are they a list of root and indexes?
> 4. Is the extrinsic that’s passed in CE133 the same as what’s in a work package bundle? If so why is it included in CE133 if it can just be calculated based off the work package by the receiver using X(w) in 14.14
> 

1. It is the second argument to A in 14.15

id:
1389
timestamp:
2025-02-12T15:07:40.597Z
sender:
@gav:polkadot.io
content:
2. A bundle has the extrinsic data; see 14.14 X. 

id:
1388
timestamp:
2025-02-12T15:08:15.220Z
sender:
@gav:polkadot.io
content:
3. A bundle has the actual segments and justifications. See 14.14 S and J. 

id:
1387
timestamp:
2025-02-12T15:09:52.608Z
sender:
@gav:polkadot.io
content:
4. 14.14 X assumes knowledge of the relevant hash preimages. The receiver may not have such knowledge therefore it makes sense to provide it. Theoretically we could make it an on-demand thing later. 

id:
2849
timestamp:
2025-02-12T15:10:21.959Z
sender:
@gav:polkadot.io
content:
> <@clearloop:matrix.org> May I ask if the transaction pool is totally open to implement or there is sort of spec WIP?

There is no transaction pool. 

id:
2848
timestamp:
2025-02-12T15:25:17.716Z
sender:
@clearloop:matrix.org
content:
hmm, if so, how we pack the extrinsics to blocks )) for example the maximum number of tickets can be submitted in a single extrinsic is 16, what if my node received 20 extrinsics, then I need to sort them and get the first 16 which has better scores, if there are references from GP for this part?

id:
2847
timestamp:
2025-02-12T15:29:21.383Z
sender:
@dave:parity.io
content:
It looks like you are permuting the wrong thing. Please read the GP formulas that you linked carefully. The input to the permutation should look like [0, 0, 0, 1, 1, 1] for the tiny configuration.

id:
2846
timestamp:
2025-02-12T15:35:25.197Z
sender:
@clearloop:matrix.org
content:
* hmm, if so, how do we pack the extrinsics to blocks )) for example the maximum number of tickets can be submitted in a single extrinsic is 16, what if my node received 20 extrinsics, then I need to sort them and get the first 16 which have better scores, if there are references from GP for this part?

also, on receiving extrinsics from other nodes, we may need to fast validate the upcoming extrinsics otherwise the node hardly can author blocks successfully once there are invalid & outdated extrinsics

id:
2845
timestamp:
2025-02-12T15:37:01.908Z
sender:
@clearloop:matrix.org
content:
* hmm, if so, how do we pack the extrinsics to blocks )) for example the maximum number of tickets can be submitted in a single extrinsic is 16, what if my node received 20 extrinsics, then I need to sort them and get the first 16 which have better scores, if there are references from GP for this part?

also, on receiving extrinsics from other nodes, we may need to fast validate the upcoming extrinsics otherwise the node hardly can author blocks successfully once there are invalid & outdated extrinsics, besides, on waiting for finalization, we can preprocess the future extrinsics based on the predicted longest chain's state

id:
2844
timestamp:
2025-02-12T15:50:09.296Z
sender:
@gav:polkadot.io
content:
B5B82264-C7ED-45C8-9076-D503D932B2ED.png

id:
2843
timestamp:
2025-02-12T15:50:19.013Z
sender:
@gav:polkadot.io
content:
First JAM toaster server room getting deployment. 

id:
2842
timestamp:
2025-02-12T15:51:14.886Z
sender:
@gav:polkadot.io
content:
The journey is arduous 

id:
2841
timestamp:
2025-02-12T15:51:44.192Z
sender:
@gav:polkadot.io
content:
300kW of power: on. 

id:
2840
timestamp:
2025-02-12T15:59:15.785Z
sender:
@oliver.tale-yazdi:parity.io
content:
Are you implying that ticket extrinsics have something to do with transactions?  
Ticket extrinsics are to acquire an authoring slot, has nothing to do with TX

id:
2839
timestamp:
2025-02-12T16:00:23.153Z
sender:
@clearloop:matrix.org
content:
oh yeah I main sort of extrinsic pool in memory comparing with db transactions

id:
2838
timestamp:
2025-02-12T16:02:04.834Z
sender:
@clearloop:matrix.org
content:
* oh yeah I mean sort of extrinsic pool in memory comparing with db transactions, just sth handling the pending extrinsics, I have a problem about how to fast validating them before packing them into blocks & simulate state transaction, so if there is a spec for this stuff, it would save my ass

id:
2837
timestamp:
2025-02-12T16:02:48.331Z
sender:
@clearloop:matrix.org
content:
* oh yeah I mean sort of extrinsic pool in memory comparing with db transactions, just sth handling the pending extrinsics, I have a problem about how to fast validate them before packing them into blocks & simulate state transaction, so if there is a spec for this stuff, it would save my ass

id:
2836
timestamp:
2025-02-12T16:03:19.919Z
sender:
@gav:polkadot.io
content:
There is not a spec for this - it is largely strategy since validators get rewarded for building blocks with full and correct extrinsic data. 

id:
2835
timestamp:
2025-02-12T16:04:31.769Z
sender:
@gav:polkadot.io
content:
Note that there is a limited amount of extrinsic slots and all slots are limited in terms of what they can host. So it’s less about building a pool (where we would assume some degree of fungibility). 

id:
2834
timestamp:
2025-02-12T16:05:12.317Z
sender:
@gav:polkadot.io
content:
In general you’ll just keep *all* reasonable extrinsic data you receive from other validators as long as it could help you build a block. 

id:
2833
timestamp:
2025-02-12T16:06:21.048Z
sender:
@gav:polkadot.io
content:
There will be a sensible block-building deadline, at which point you’ll use whatever extrinsic data you know about. 

id:
2832
timestamp:
2025-02-12T17:49:48.596Z
sender:
@decentration:matrix.org
content:
 * oh yes gotcha! 

i was shuffling validators.length amount of times instead of validators\_per\_core amount of times. and you're right the gp shuffles cores instead of validators...

though its still attainable either approach, but i will go back to shuffling cores. 

cheers.   


id:
2831
timestamp:
2025-02-13T00:51:31.489Z
sender:
@sourabhniyogi:matrix.org
content:
For pragmatic strategy choices where GP is silent, would it be useful for JAM implementers to make these parameters adjustable for users and make our (maybe singular) choices explicit amongst ourselves?

We are aware of a dozen pragmatic strategy choices we made (e.g. when and how often to send tickets, who to ask for D3L data in what order, whether to request/verify proofs when, etc.) but maybe there are 2 dozen others that we aren't aware that we made but also could justify "strategy" specified in client implementation.  Potentially having non-GP strategy default choices could be useful, so that nodes don't receive more tickets + requests + ...  than they actually need to from other implementations.  We could add a "strategy" component to docs.jamcha.in to make these 1-3 dozen strategy choices explicit and make it clear what it means to not only a GP compliant implementation but a "friendly" implementation (vs overly aggressive or overly conservative [or lazy]).   

id:
2830
timestamp:
2025-02-13T00:53:13.972Z
sender:
@sourabhniyogi:matrix.org
content:
* For pragmatic strategy choices where GP is silent, would it be useful for JAM implementers to make these strategy choices /  parameters adjustable for users and make our (maybe singular or maybe default) choices explicit amongst ourselves?

We are aware of a dozen pragmatic strategy choices we made (e.g. when and how often to send tickets, who to ask for D3L data in what order, whether to request/verify proofs when, etc.) but maybe there are 2 dozen others that we aren't aware that we made but also could justify "strategy" specified in client implementation.  Potentially having non-GP strategy default choices could be useful, so that nodes don't receive more tickets + requests + ...  than they actually need to from other implementations.  We could add a "strategy" component to docs.jamcha.in to make these 1-3 dozen strategy choices explicit and make it clear what it means to not only a GP compliant implementation but a "friendly" implementation (vs overly aggressive or overly conservative \[or lazy\]).   

id:
2829
timestamp:
2025-02-13T00:54:26.185Z
sender:
@sourabhniyogi:matrix.org
content:
* For pragmatic strategy choices where GP is silent, would it be useful for JAM implementers to make these strategy choices /  parameters adjustable for users and make our (maybe singular or maybe default) choices explicit amongst ourselves?

We are aware of a dozen pragmatic strategy choices we made (e.g. when and how often to send tickets, who to ask for D3L data in what order, whether to request/verify proofs when, etc.) but maybe there are 2 dozen others that we aren't aware that we made but also could justify "strategy" specified in client implementation.  Potentially having non-GP strategy default choices could be useful, so that nodes don't receive more tickets + requests + ...  than they actually need to from other implementations.  We could add a "strategy" component to docs.jamcha.in to make these 1-3 dozen strategy choices explicit and make it clear what it means to not only have a GP-compliant implementation but a "friendly" implementation (vs overly aggressive or overly conservative \[or lazy and maybe actually non-compliant \]).

id:
2828
timestamp:
2025-02-13T00:54:52.738Z
sender:
@xlchen:matrix.org
content:
I think it will be useful to define what is considered as "bad behaivour" (e.g. sending too much requests at once) so we can all avoid doing it. The nodes that is doing the bad behaviour should get detected and banned by other peers.

id:
2827
timestamp:
2025-02-13T06:41:10.336Z
sender:
@decentration:matrix.org
content:
 * oh yes gotcha

id:
2826
timestamp:
2025-02-13T10:16:00.910Z
sender:
@decentration:matrix.org
content:
ok yes i was thrown by the fisher yates shuffle passing tests, i will shuffle by cores and not validator indices, ty 

id:
1386
timestamp:
2025-02-13T10:16:55.722Z
sender:
@emielsebastiaan:matrix.org
content:
Add tau & tau_prime as state transition input dependencyhttps://github.com/gavofyork/graypaper/pull/241

id:
1385
timestamp:
2025-02-13T10:17:19.012Z
sender:
@emielsebastiaan:matrix.org
content:
* Adds tau & tau\_prime as state transition input dependency.

https://github.com/gavofyork/graypaper/pull/241

id:
2825
timestamp:
2025-02-13T12:46:52.998Z
sender:
@hanayukii:matrix.org
content:
For formula (A.16) https://graypaper.fluffylabs.dev/#/5f542d7/24e30224e302
under case a = 0  the formula would be -> x / (2^-1) * (2^64 -1) and seems weird, do I understand something wrong?

id:
2824
timestamp:
2025-02-13T13:55:41.778Z
sender:
@alice_und_bob:matrix.org
content:
is the JAM Bootstrap service mentioned here ( https://hackmd.io/@polkadot/jamsdk ) the thing that Gavin referred to as CoreBoot?

id:
2823
timestamp:
2025-02-13T15:45:13.572Z
sender:
@sourabhniyogi:matrix.org
content:
yes

id:
2822
timestamp:
2025-02-13T16:25:57.153Z
sender:
@gav:polkadot.io
content:
> <@hanayukii:matrix.org> For formula (A.16) https://graypaper.fluffylabs.dev/#/5f542d7/24e30224e302
> under case a = 0  the formula would be -> x / (2^-1) * (2^64 -1) and seems weird, do I understand something wrong?

No - x is always zero and since the term is multiplied by it, the result is always zero which is what we want. 

id:
2821
timestamp:
2025-02-13T16:30:48.139Z
sender:
@gav:polkadot.io
content:
> For pragmatic strategy choices where GP is silent, would it be useful for JAM implementers to make these strategy choices /  parameters adjustable for users and make our (maybe singular or maybe default) choices explicit amongst ourselves?

Most important aspects of strategy (e.g. extrinsic population, selection of WPs to guarantee) should be pretty straightforward to reason about optimal behaviour. We will likely, in due course, document strategies which we feel are sensible, and behaviour which we feel is crucial to deliver. I would invite teams to document their own approaches for discussion.

id:
2820
timestamp:
2025-02-13T16:32:51.485Z
sender:
@gav:polkadot.io
content:
Network protocol should, and ultimately will, document "polite" behaviour. Technically this, like all network behaviour is not about "correctness" but rather an attempt to make a mutually-beneficial connection.

id:
2819
timestamp:
2025-02-13T16:33:11.645Z
sender:
@gav:polkadot.io
content:
Politeness is defined as conventionally acceptable behaviour.

id:
2818
timestamp:
2025-02-13T16:34:00.438Z
sender:
@gav:polkadot.io
content:
Like all strategy, nodes are of course free to be more restrictive or permissive, but it can be very helpful to have a clear convention.

id:
2817
timestamp:
2025-02-13T19:25:24.521Z
sender:
@harshitc286:matrix.org
content:
Hello GM gm

id:
2816
timestamp:
2025-02-13T19:29:35.490Z
sender:
@harshitc286:matrix.org
content:
1000869038.jpg

id:
2815
timestamp:
2025-02-13T19:31:00.392Z
sender:
@harshitc286:matrix.org
content:
> <@harshitc286:matrix.org> sent an image.

From IITB, JAM Tour by, 
Dr. Gavin Wood
Amazing Event! 

id:
1384
timestamp:
2025-02-14T11:49:31.637Z
sender:
@sourabhniyogi:matrix.org
content:
Where does the odd number [81](https://graypaper.fluffylabs.dev/#/5f542d7/114c01114c01) from the `a_o` service storage size formula come from? 

id:
1383
timestamp:
2025-02-14T12:07:18.025Z
sender:
@sourabhniyogi:matrix.org
content:
* How is the strangely odd number [81](https://graypaper.fluffylabs.dev/#/5f542d7/114c01114c01) from the `a_o` service storage size formula derived?   The 32 within the same formula is the storage for key, but 81 is 17 more bytes than 64...

id:
2814
timestamp:
2025-02-14T15:27:10.018Z
sender:
@emielsebastiaan:matrix.org
content:
Gavin, 

In this new Parity blog (https://www.parity.io/blog/JAM-experience) the JAM Grid is mentioned. 

This Parity blog references an article explaining JAM Grid (by Permanence DAO: https://permanencedao.medium.com/jam-and-the-jam-grid-the-subsequent-phases-of-the-polkadot-cloud-a79f416e5ec5).

This referenced article seems to suggest that JAM Grid will have a shared security (by DOT).

1. Could you shed some light on this?
2. Do you envision JAMs in JAM Grid to have a shared security?
3. And if so, how would this (on a high level) be achieved technically?

id:
2813
timestamp:
2025-02-14T15:33:20.720Z
sender:
@gav:polkadot.io
content:
This is still under ideation and there are still unsolved questions. However, the main strategy is to draw all JAMs’ validators from the same set, partitioning is randomly on each epoch. 

id:
2812
timestamp:
2025-02-14T15:33:33.607Z
sender:
@gav:polkadot.io
content:
* This is still under ideation and there are still unsolved questions. However, the main strategy is to draw all JAMs’ validators from the same set, partitioning it randomly on each epoch. 

id:
2811
timestamp:
2025-02-14T15:33:55.735Z
sender:
@gav:polkadot.io
content:
With enough validators, this can be shown to keep the majority of the security. 

id:
2810
timestamp:
2025-02-14T15:35:09.914Z
sender:
@gav:polkadot.io
content:
Eg two JAMs would have a single staking system which elected 2046 validators. Every epoch the two JAMs’ entropy would be combined to partition those validators randomly into two sets of 1023 and those validators would take over validating on their corresponding network. 

id:
2809
timestamp:
2025-02-14T15:35:28.933Z
sender:
@gav:polkadot.io
content:
Communication would be achieved between JAMs via the usual bilateral bridging. 

id:
2808
timestamp:
2025-02-14T15:36:04.055Z
sender:
@gav:polkadot.io
content:
Our bridge design is light enough and there’s enough processing power and data bandwidth that we can realistically afford to bridge every pair of JAM instances. 

id:
2807
timestamp:
2025-02-14T15:37:39.591Z
sender:
@gav:polkadot.io
content:
The overall security in this model sits at 20%, which would lower the Nakamoto coefficient by around 40%. Not ideal, but Polkadot would remain well within its current practical security bracket. 

id:
2806
timestamp:
2025-02-14T15:38:03.594Z
sender:
@gav:polkadot.io
content:
* The overall security in this model sits at 20% (rather than 33% as at present), which would lower the Nakamoto coefficient by around 40%. Not ideal, but Polkadot would remain well within its current practical security bracket. 

id:
2805
timestamp:
2025-02-14T15:38:21.756Z
sender:
@gav:polkadot.io
content:
This model scales very well. 

id:
2804
timestamp:
2025-02-14T15:39:27.036Z
sender:
@gav:polkadot.io
content:
Even with 1000 JAMs and this 1023,000 validators, we’d still enjoy a somewhat similar level of both security and coherence. 

id:
2803
timestamp:
2025-02-14T15:39:34.633Z
sender:
@gav:polkadot.io
content:
* Even with 1000 JAMs and thus 1023,000 validators, we’d still enjoy a somewhat similar level of both security and coherence. 

id:
2802
timestamp:
2025-02-14T15:40:09.817Z
sender:
@gav:polkadot.io
content:
Exactly how similar is an open question. 

id:
2801
timestamp:
2025-02-14T15:41:01.652Z
sender:
@gav:polkadot.io
content:
It is conceivable that a single service could be made to scale elastically over not simple one JAM’s worth of cores but many. 

id:
2800
timestamp:
2025-02-14T15:41:11.388Z
sender:
@gav:polkadot.io
content:
* It is conceivable that a single service could be made to scale elastically over not simply one JAM’s worth of cores but many. 

id:
2799
timestamp:
2025-02-14T15:41:27.798Z
sender:
@emielsebastiaan:matrix.org
content:
Thanks great!! This gives me something to ponder about. 

id:
1382
timestamp:
2025-02-14T19:27:04.752Z
sender:
@ascriv:matrix.org
content:
For initializing the ring context for bandersnatch ring vrf stuff, there must be a common seed we’ll all be using? So that it remains deterministic 

id:
1381
timestamp:
2025-02-14T19:33:56.366Z
sender:
@ascriv:matrix.org
content:
I assume we use the parameters in 4.1 configuration in https://github.com/davxy/bandersnatch-vrfs-spec/blob/main/specification.pdf but wanted to confirm 

id:
1380
timestamp:
2025-02-14T21:58:35.844Z
sender:
@davxy:matrix.org
content:
> <@ascriv:matrix.org> I assume we use the parameters in 4.1 configuration in https://github.com/davxy/bandersnatch-vrfs-spec/blob/main/specification.pdf but wanted to confirm 

Correct. If and when anything changes, I'll update the spec and notify all JAM channels.



id:
2798
timestamp:
2025-02-15T11:34:17.962Z
sender:
@davxy:matrix.org
content:
sourabhniyogi: We'd like to try importing Jamduna vectors. Would it be possible to provide the genesis files from this link (https://github.com/jam-duna/jamtestnet/tree/main/chainspecs/rawkv) in binary format as well?

id:
2797
timestamp:
2025-02-15T14:46:04.900Z
sender:
@obnty:matrix.org
content:
Hello implementers, if any of you are in town at the same time as the JAM tour is happening in HK, Taipei, Beijing, Shanghai, Hangzhou, Shenzhen, please let me or milaswords know. (already in touch with JAM DUNA and New JAMneration team based in Taipei)

id:
2796
timestamp:
2025-02-15T14:55:16.161Z
sender:
@0xjunha:matrix.org
content:
I'll be in HK and Shanghai (team RJAM)

id:
2795
timestamp:
2025-02-15T16:11:07.112Z
sender:
@davxy:matrix.org
content:
See also: https://github.com/jam-duna/jamtestnet/issues/71

id:
2794
timestamp:
2025-02-15T16:12:15.007Z
sender:
@heyitsstanoagain:matrix.org
content:
hi davxy I think the OP's concern (and mine too) was about unrelated test cases. E.g. I was struggling with `safrole/tiny/publish-tickets-no-mark-9.json` and only now saw that its posterior gamma_z changed from 0x95f3... to 0xb375... in the mentioned commit (2d71c3f) I think there are no offenders in this case but something in the computation of the ring commitment has changed. 

id:
2793
timestamp:
2025-02-15T16:12:36.737Z
sender:
@heyitsstanoagain:matrix.org
content:
* hi davxy I think the OP's concern (and mine too) was about unrelated test cases. E.g. I was struggling with `safrole/tiny/publish-tickets-no-mark-9.json` and only now saw that its posterior gamma\_z changed from `0x95f3...` to `0xb375...` in the mentioned commit (`2d71c3f`) I think there are no offenders in this case but something in the computation of the ring commitment has changed.

id:
2792
timestamp:
2025-02-15T20:24:52.994Z
sender:
@sourabhniyogi:matrix.org
content:
Absolutely, will do!

id:
2791
timestamp:
2025-02-16T11:58:28.760Z
sender:
@sourabhniyogi:matrix.org
content:
Is [this](https://github.com/jam-duna/jamtestnet/blob/main/chainspecs/rawkv/README.md) what you had in mind?

If not, it might be easiest if you added a single array of kv codec example [https://github.com/w3f/jamtestvectors/tree/master/codec/data] that matched [this](https://github.com/jam-duna/jamtestnet/blob/main/chainspecs/rawkv/genesis-tiny.json) -- then we would publish json codec state_snapshots like [this](https://github.com/jam-duna/jamtestnet/blob/main/data/safrole/state_snapshots/5_000.bin) in the same way?

id:
2790
timestamp:
2025-02-16T12:01:41.423Z
sender:
@sourabhniyogi:matrix.org
content:
* Is [this](https://github.com/jam-duna/jamtestnet/blob/main/chainspecs/rawkv/README.md) what you had in mind?

If not, it might be easiest if you added a single array of kv codec example [here](https://github.com/w3f/jamtestvectors/tree/master/codec/data) that matched [this](https://github.com/jam-duna/jamtestnet/blob/main/chainspecs/rawkv/genesis-tiny.json) -- then we would publish json codec state\_snapshots like [this](https://github.com/jam-duna/jamtestnet/blob/main/data/safrole/state_snapshots/5_000.bin) in the same way?

id:
2789
timestamp:
2025-02-16T12:03:05.584Z
sender:
@sourabhniyogi:matrix.org
content:
* Is [this](https://github.com/jam-duna/jamtestnet/blob/main/chainspecs/rawkv/README.md) what you had in mind?

If not, it might be easiest if you added a single array of kv codec example [here](https://github.com/w3f/jamtestvectors/tree/master/codec/data) that matched [this](https://github.com/jam-duna/jamtestnet/blob/main/chainspecs/rawkv/genesis-tiny.json) -- then we would publish json codec state\_snapshots like [this](https://github.com/jam-duna/jamtestnet/blob/main/data/safrole/state_snapshots/5_000.bin) in the same way?

Can discuss [here](https://github.com/davxy/jam-test-vectors/issues/22#issuecomment-2661399256) instead as you see fit!

id:
1379
timestamp:
2025-02-16T18:27:07.432Z
sender:
@ascriv:matrix.org
content:
I think I have a correction for the state transition dependency graph: lambda prime needs to be added to the inputs for the state transition for the validator statistics, since we need to compute the reporters set which requires G* which requires lambda prime

id:
2788
timestamp:
2025-02-16T19:00:13.387Z
sender:
@davxy:matrix.org
content:
As we begin testing each other's block import vectors, it might be worth establishing clearer "standards" for these vectors to help collaboration.

I think that the format adopted by [jamduna](https://github.com/jam-duna/jamtestnet/blob/main/data/safrole/state_transitions) is quite practical (and is similar to the approach taken by [javajam](https://github.com/javajamio/javajam-trace/tree/main/state_transitions)).

Furthermore, could we explore defining community-wide values for the genesis header? For additional context, see these related issues:  
- https://github.com/jam-duna/jamtestnet/issues/71  
- https://github.com/javajamio/javajam-trace/issues/2  

It might also be beneficial to document the testing genesis header and import format outcome details here: https://docs.jamcha.in/

id:
2787
timestamp:
2025-02-17T00:26:32.618Z
sender:
@xlchen:matrix.org
content:
How are we going to deal DA/erasure coding related parameters regards to small size testnets? Are we still keep the same WG=4104 and still coded it into 1023 shards but just require each validators to hold multiple shards?

id:
1378
timestamp:
2025-02-17T08:36:30.910Z
sender:
@celadari:matrix.org
content:
Hi everyone,

I have a question about accumulation regarding the call to the accumulate function.

We call Ψ_A in equation (12.19) [GP version 0.6.1] at the accumulation stage.
I'm following the formalism of the white paper.

Should this function call alter δ, or does it return a new account (like the Ω functions) that needs to be accumulated/saved later in equation (12.21)?

Am I clear? 😅

id:
1377
timestamp:
2025-02-17T08:41:56.462Z
sender:
@gav:polkadot.io
content:
The accumulate function is used in the final definition of posterior delta. It is up to implementations to determine at what point they alter any particular internal data structure(s).

id:
1376
timestamp:
2025-02-17T08:42:25.444Z
sender:
@gav:polkadot.io
content:
* The accumulate function is used in the final definition of posterior delta. It is up to implementations to determine at what point they alter any particular internal data structure(s) which may represent delta or some partial intermediate value of it. 

id:
1375
timestamp:
2025-02-17T08:42:39.414Z
sender:
@gav:polkadot.io
content:
* The accumulate function is used in the final definition of posterior delta. It is up to implementations to determine at what point they alter any particular internal data structure(s) which may represent delta or some partial/intermediate value of it. 

id:
1374
timestamp:
2025-02-17T08:43:47.632Z
sender:
@gav:polkadot.io
content:
One thing which will be very helpful to know is that no two accumulate functions which both contribute to the same “wave” of accumulations will have contradictory changes. 

id:
1373
timestamp:
2025-02-17T08:49:38.620Z
sender:
@celadari:matrix.org
content:
thanks

id:
2786
timestamp:
2025-02-17T10:21:10.092Z
sender:
@danicuki:matrix.org
content:
I created a spreadsheet to track which teams imported blocks from each other. Feel free to edit with your own team data:

https://docs.google.com/spreadsheets/d/1JVt_1daKJWslCaP9hfggKQSN4aGuV_le0XkKcsrjXDc/edit?gid=0#gid=0

id:
2785
timestamp:
2025-02-17T12:00:19.567Z
sender:
@gav:polkadot.io
content:
> <@xlchen:matrix.org> How are we going to deal DA/erasure coding related parameters regards to small size testnets? Are we still keep the same WG=4104 and still coded it into 1023 shards but just require each validators to hold multiple shards?

We’ll want to keep segment size the same so yeah, increasing the number of shards makes most sense. 

id:
2784
timestamp:
2025-02-17T12:00:25.951Z
sender:
@gav:polkadot.io
content:
> <@xlchen:matrix.org> How are we going to deal DA/erasure coding related parameters regards to small size testnets? Are we still keep the same WG=4104 and still coded it into 1023 shards but just require each validators to hold multiple shards?

* We’ll want to keep segment size the same so yeah, increasing the number of shards stored makes most sense. 

id:
2783
timestamp:
2025-02-17T13:44:42.403Z
sender:
@dave:parity.io
content:
For the "tiny" testnet we have 2052-byte segment shards rather than 12-byte segment shards. This results in the same 4104-byte segment size (2052*2 = 12*342)

id:
2782
timestamp:
2025-02-17T13:45:26.240Z
sender:
@dave:parity.io
content:
* For the "tiny" testnet we have 2052-byte segment shards rather than 12-byte segment shards. This results in the same 4104-byte segment size (2052\*2 = 12\*342)

id:
2781
timestamp:
2025-02-17T13:46:55.824Z
sender:
@dave:parity.io
content:
EC produces 1 shard per validator, so 6 shards on the tiny testnet

id:
2780
timestamp:
2025-02-17T16:11:54.020Z
sender:
@clearloop:matrix.org
content:
process goes faster these days 🫠 still struggling with refactoring the network part though ( was implemented with libp2p XD )

id:
2779
timestamp:
2025-02-17T16:50:07.581Z
sender:
@alice_und_bob:matrix.org
content:
MEV-related question: what determines accumulation order of core work results?

id:
2778
timestamp:
2025-02-17T18:01:35.232Z
sender:
@davxy:matrix.org
content:
Proposal for a genesis header for block import interoperability testing:

https://github.com/JamBrains/jam-docs/pull/13

I'm mostly trying to achieve "consensus" on these values to make testing easier for everyone. Please feel free to share your thoughts directly on the GH issue

id:
2777
timestamp:
2025-02-17T18:03:18.689Z
sender:
@davxy:matrix.org
content:
FWW these are the values currently used by jamduna and polkajam for testing

id:
2776
timestamp:
2025-02-17T18:03:26.455Z
sender:
@davxy:matrix.org
content:
* FWIW these are the values currently used by jamduna and polkajam for testing

id:
2775
timestamp:
2025-02-17T21:03:36.692Z
sender:
@oliver.tale-yazdi:parity.io
content:
LGTM, hash also verifies. will merge tomorrow if no objections until then

id:
1372
timestamp:
2025-02-17T22:08:36.548Z
sender:
@emielsebastiaan:matrix.org
content:
ignore previous message.. mistake on our end

id:
1371
timestamp:
2025-02-17T22:19:24.101Z
sender:
@emielsebastiaan:matrix.org
content:
apologies for the inconvenience Jan Bujak 

id:
2774
timestamp:
2025-02-17T23:04:41.801Z
sender:
@gav:polkadot.io
content:
> <@alice_und_bob:matrix.org> MEV-related question: what determines accumulation order of core work results?

The service. 

id:
2773
timestamp:
2025-02-17T23:46:11.534Z
sender:
@sourabhniyogi:matrix.org
content:
Does anyone have a  polkatool or JAM Rust SDK built "null authorizer" which matches https://x.com/the_seraya/status/1891521159209824381 that we can basically standardize on amongst ourselves

id:
2772
timestamp:
2025-02-17T23:47:16.666Z
sender:
@alice_und_bob:matrix.org
content:
My understanding is that you can have work items from different services in one work package. 

Maybe a better way to phrase my question: how is the order determined in which work packages are processed?

id:
2771
timestamp:
2025-02-17T23:50:00.915Z
sender:
@dave:parity.io
content:
See https://crates.io/crates/jam-null-authorizer, though I'm not sure if the build is reproducible or not, so not sure if you will get the same hash every time

id:
2770
timestamp:
2025-02-18T00:04:34.537Z
sender:
@sourabhniyogi:matrix.org
content:
The latest "0.1.12" looks to have not changed in the last couple of months, thank you!  https://github.com/jam-duna/jamtestnet/blob/main/services/parity/jam-null-authorizer/src/main.rs
Will try with this again

id:
2769
timestamp:
2025-02-18T00:16:19.873Z
sender:
@sourabhniyogi:matrix.org
content:
* The latest "0.1.12" looks to have not changed in the last couple of months, thank you!  https://github.com/jam-duna/jamtestnet/blob/main/services/parity/jam-null-authorizer/src/main.rs
Will try with this again and attempt a polkatool equivalent to reduce byte count..

id:
2768
timestamp:
2025-02-18T01:26:35.043Z
sender:
@sourabhniyogi:matrix.org
content:
Did you put this null authorizer in the genesis state?

id:
2767
timestamp:
2025-02-18T01:36:43.936Z
sender:
@sourabhniyogi:matrix.org
content:
* Did you put this null authorizer in the genesis state (alongside the bootstrap service)?  Seems natural to do so... actually, is technically any alternative?

id:
2766
timestamp:
2025-02-18T01:36:58.794Z
sender:
@sourabhniyogi:matrix.org
content:
* Did you put this null authorizer in the genesis state (alongside the bootstrap service)?  Seems natural to do so... actually, is technically there any alternative?

id:
2765
timestamp:
2025-02-18T01:40:42.342Z
sender:
@gav:polkadot.io
content:
> <@alice_und_bob:matrix.org> My understanding is that you can have work items from different services in one work package. 
> 
> Maybe a better way to phrase my question: how is the order determined in which work packages are processed?

Work packages can declare a prerequisite work package. This will be enforced for accumulate ordering. Other than that, there are no ordering guarantees between WPs. Within the same WP, you get a guarantee that the different work items will execute contemporaneously. But if they are different services, they will execute independently; transfers are asynchronous and state-reads outside of the service which is executing are fixed at the state from when execution began. 

id:
2764
timestamp:
2025-02-18T01:41:15.298Z
sender:
@gav:polkadot.io
content:
> <@sourabhniyogi:matrix.org> Did you put this null authorizer in the genesis state (alongside the bootstrap service)?  Seems natural to do so... actually, is technically there any alternative?

Yes and no there is no alternative. 

id:
2763
timestamp:
2025-02-18T01:41:41.172Z
sender:
@gav:polkadot.io
content:
* Yes and no there is probably no alternative. 

id:
1370
timestamp:
2025-02-18T04:38:09.295Z
sender:
@sourabhniyogi:matrix.org
content:
We believe the notation ${\bf p}_{\bf c}$ used in Eq B.2 [here](https://graypaper.fluffylabs.dev/#/5f542d7/2cf8022cf802) needs to be adjusted to accommodate service $h$ and authorization code hash $u$ of 14.2 [here](https://graypaper.fluffylabs.dev/#/5f542d7/197900197900) -- that the preimage $u$ of service $h$ is the authorization code input to $\Psi_M$ of B.2.   Can something confirm this interpretation is correct?

Specifically, the genesis state with bootstrap service 0 will have a null authorizer code hash (say, 0x12344321...) and the very first work package ${\bf p}$ (to create a new service) will have $h=0$ and $u=0x12344321...$ to reference this null authorizer.

Does that make sense?

id:
1369
timestamp:
2025-02-18T04:39:20.923Z
sender:
@sourabhniyogi:matrix.org
content:
* We believe the notation ${\\bf p}\_{\\bf c}$ used in Eq B.2 [here](https://graypaper.fluffylabs.dev/#/5f542d7/2cf8022cf802) needs to be adjusted to accommodate service $h$ and authorization code hash $u$ of 14.2 [here](https://graypaper.fluffylabs.dev/#/5f542d7/197900197900) -- that the preimage $u$ of service $h$ is the authorization code input to $\\Psi\_M$ of B.2.   Can someone confirm this interpretation is correct?

Specifically, the genesis state with bootstrap service 0 will have a null authorizer code hash (say, 0x12344321...) and the very first work package ${\\bf p}$ (to create a new service) will have $h=0$ and $u=0x12344321...$ to reference this null authorizer.

Does that make sense?

id:
1368
timestamp:
2025-02-18T04:51:38.025Z
sender:
@0xjunha:matrix.org
content:
Description of accumulate queue-editing function (*E*) seems outdated -  opened a PR to update it.
https://github.com/gavofyork/graypaper/pull/246

id:
1367
timestamp:
2025-02-18T05:03:45.522Z
sender:
@gav:polkadot.io
content:
> <@sourabhniyogi:matrix.org> We believe the notation ${\\bf p}\_{\\bf c}$ used in Eq B.2 [here](https://graypaper.fluffylabs.dev/#/5f542d7/2cf8022cf802) needs to be adjusted to accommodate service $h$ and authorization code hash $u$ of 14.2 [here](https://graypaper.fluffylabs.dev/#/5f542d7/197900197900) -- that the preimage $u$ of service $h$ is the authorization code input to $\\Psi\_M$ of B.2.   Can someone confirm this interpretation is correct?
> 
> Specifically, the genesis state with bootstrap service 0 will have a null authorizer code hash (say, 0x12344321...) and the very first work package ${\\bf p}$ (to create a new service) will have $h=0$ and $u=0x12344321...$ to reference this null authorizer.
> 
> Does that make sense?

No

id:
1366
timestamp:
2025-02-18T05:05:37.781Z
sender:
@gav:polkadot.io
content:
* No - having trouble interpreting this

id:
1365
timestamp:
2025-02-18T05:07:42.621Z
sender:
@sourabhniyogi:matrix.org
content:
Sorry -- how does Eq B.2's first input of $\Psi_M$ ( which is ${\bf p}_{\bf c}$) get at a work package's authorization code?  



id:
1364
timestamp:
2025-02-18T05:08:23.716Z
sender:
@sourabhniyogi:matrix.org
content:
* Sorry -- how does Eq B.1's first input of $\\Psi\_M$ ( which is ${\\bf p}\_{\\bf c}$) get at a work package's authorization code?

id:
1363
timestamp:
2025-02-18T05:08:32.424Z
sender:
@sourabhniyogi:matrix.org
content:
* We believe the notation ${\\bf p}\_{\\bf c}$ used in Eq B.1 [here](https://graypaper.fluffylabs.dev/#/5f542d7/2cf8022cf802) needs to be adjusted to accommodate service $h$ and authorization code hash $u$ of 14.2 [here](https://graypaper.fluffylabs.dev/#/5f542d7/197900197900) -- that the preimage $u$ of service $h$ is the authorization code input to $\\Psi\_M$ of B.2.   Can someone confirm this interpretation is correct?

Specifically, the genesis state with bootstrap service 0 will have a null authorizer code hash (say, 0x12344321...) and the very first work package ${\\bf p}$ (to create a new service) will have $h=0$ and $u=0x12344321...$ to reference this null authorizer.

Does that make sense?

id:
1362
timestamp:
2025-02-18T05:09:43.188Z
sender:
@gav:polkadot.io
content:
See 14.9

id:
1361
timestamp:
2025-02-18T13:26:54.008Z
sender:
@leonidas_m:matrix.org
content:
Hey, I've noticed that some PVM tests (eg `inst_load_u8_nok`, `inst_store_u8_trap_inaccessible`) charge more than 1 extra gas when a page fault occurs even though only a single instruction is executed. In previous commits, similar tests were removed because the cost model was based on polkaVM and wasn't specified in the GP. Are these tests facing the same issue now or am I misunderstanding something?

id:
1360
timestamp:
2025-02-18T20:27:04.707Z
sender:
@jan:parity.io
content:
Yes.

id:
1359
timestamp:
2025-02-19T07:58:34.025Z
sender:
@leonidas_m:matrix.org
content:
Looks like the same issue also affects the following test vectors:
`inst_store_imm_indirect_u16_with_offset_nok` 
`inst_store_imm_indirect_u32_with_offset_nok`
`inst_store_imm_indirect_u64_with_offset_nok`
`inst_store_imm_indirect_u8_with_offset_nok`
`inst_store_imm_u8_trap_inaccessible`
`inst_store_indirect_u16_with_offset_nok`
`inst_store_indirect_u32_with_offset_nok`
`inst_store_indirect_u64_with_offset_nok`
`inst_store_indirect_u8_with_offset_nok`

id:
1358
timestamp:
2025-02-19T12:37:46.866Z
sender:
@celadari:matrix.org
content:
Hi guys,

Just some question related to safrole:

Equations (6.15), (6.16), (6.17), (6.18), (6.19), (6.20) - GP version 6.2 - refer to gamma_s' and eta_3' => so we should update gamma_s and eta_3 before ? Therefore, we should compute equations (6.23), (6.24) before checking equations (6.15), (6.16), (6.17), (6.18), (6.19), (6.20) ?

id:
1357
timestamp:
2025-02-19T13:28:55.627Z
sender:
@oliver.tale-yazdi:parity.io
content:
I had a similar Q and that is was i ended up doing. It seems that the gamma states cannot be updated at once anymore but need to be done in two steps

id:
1356
timestamp:
2025-02-19T13:33:46.423Z
sender:
@celadari:matrix.org
content:
Can you elaborate please ? 🙃
Should we update gamma then before making the header check of equation (6.15)-(6.20) ?

id:
1355
timestamp:
2025-02-19T14:21:26.696Z
sender:
@ascriv:matrix.org
content:
That’s how I interpret the equations, since they involve posterior variables, the posterior variables must be computed already 

id:
1354
timestamp:
2025-02-19T14:57:40.117Z
sender:
@oliver.tale-yazdi:parity.io
content:
We first do gamma_k & gamma_z, then check entropy marker H_e and then gamma_s & gamma_a.  
Not sure if that is optimal, it was just what i coded up first 🤷

id:
2762
timestamp:
2025-02-20T04:00:11.290Z
sender:
@alice_und_bob:matrix.org
content:
hm, so who decides on the ordering of the processing of work packages (or the resulting accumulations) in cases where there is no path-dependence constraining it?

id:
2761
timestamp:
2025-02-20T04:01:21.617Z
sender:
@alice_und_bob:matrix.org
content:
It's not very precise, but I am going to use this graphic today to explain the relation of Apps, Services, Work Packages and Cores today

id:
2760
timestamp:
2025-02-20T04:01:34.613Z
sender:
@alice_und_bob:matrix.org
content:
2025-02-20 - Winning the Scalability Wars - Hack Seasons Side Event @ Consensus HK.png

id:
2759
timestamp:
2025-02-20T05:37:22.892Z
sender:
@xlchen:matrix.org
content:
for parachains, we still going to have collators and it will still be collators to determine the tx order

id:
2758
timestamp:
2025-02-20T05:39:16.545Z
sender:
@xlchen:matrix.org
content:
we might have super collator that is responsible for multiple parachains for sync crosschain interactions and in that case, there will be some cross chain MEV 

id:
2757
timestamp:
2025-02-20T05:56:20.820Z
sender:
@gav:polkadot.io
content:
> <@alice_und_bob:matrix.org> hm, so who decides on the ordering of the processing of work packages (or the resulting accumulations) in cases where there is no path-dependence constraining it?

Refine is asynchronous, stateless and really has no order. 

id:
2756
timestamp:
2025-02-20T05:58:57.731Z
sender:
@gav:polkadot.io
content:
There are a few factors regarding the order that inter-service accumulation happens for two different WPs if they don’t have any stated dependency. No one actor controls this. 

id:
2755
timestamp:
2025-02-20T06:02:23.178Z
sender:
@gav:polkadot.io
content:
It depends on the guarantors, the builder at the time of reporting and the builder at the time of assurance. Builders have no direct control over ordering but they could hypothetically disregard a report or assurance-set at some cost to themselves which could (for the builder at report time) delay the WP compared to other WPs being reported at the time. 

id:
2754
timestamp:
2025-02-20T06:03:37.041Z
sender:
@gav:polkadot.io
content:
Builders at assurance time don’t get the same ability - the best they can do is prevent any new WPs from becoming available and thus accumulating. 

id:
2753
timestamp:
2025-02-20T06:04:39.330Z
sender:
@gav:polkadot.io
content:
> <@alice_und_bob:matrix.org> It's not very precise, but I am going to use this graphic today to explain the relation of Apps, Services, Work Packages and Cores today

Unfortunately pictures are totally broken on element/matrix right now. Is there a link?

id:
2752
timestamp:
2025-02-20T06:05:59.952Z
sender:
@gav:polkadot.io
content:
It’s worth remembering that services themselves are asynchronous with regards to each other, limiting the possibility of inter-service race conditions. 

id:
2751
timestamp:
2025-02-20T06:09:26.268Z
sender:
@mister_cole:matrix.org
content:
I see the image fine. Can you see it when I post it?


id:
2750
timestamp:
2025-02-20T06:09:30.826Z
sender:
@mister_cole:matrix.org
content:
image.png

id:
2749
timestamp:
2025-02-20T06:12:36.364Z
sender:
@gav:polkadot.io
content:
> <@mister_cole:matrix.org> I see the image fine. Can you see it when I post it?
> 

Nope

id:
2748
timestamp:
2025-02-20T06:13:33.360Z
sender:
@gav:polkadot.io
content:
I’m on Element X which might not help. [@erin:parity.io](https://matrix.to/#/@erin:parity.io)told me there were widespread difficulties with images recently. 

id:
1353
timestamp:
2025-02-20T12:10:18.778Z
sender:
@sourabhniyogi:matrix.org
content:
I want a JAM codec expression after [here](https://graypaper.fluffylabs.dev/#/5f542d7/381700381700) to fully detail [wrangled operand tuples](https://graypaper.fluffylabs.dev/#/5f542d7/17fa0117fa01) to support [the key input into single accumulate](https://graypaper.fluffylabs.dev/#/5f542d7/2e89002e8900) -- is this reasonable?

id:
1352
timestamp:
2025-02-20T12:20:24.279Z
sender:
@sourabhniyogi:matrix.org
content:
* I want a JAM codec expression after [here](https://graypaper.fluffylabs.dev/#/5f542d7/381700381700) to fully detail [wrangled operand tuples](https://graypaper.fluffylabs.dev/#/5f542d7/17fa0117fa01) to support [this ${\bf o}$ input into single accumulate](https://graypaper.fluffylabs.dev/#/5f542d7/2e89002e8900) -- is this reasonable?

id:
1351
timestamp:
2025-02-20T12:37:12.513Z
sender:
@sourabhniyogi:matrix.org
content:
Also, due to [GP Eq 4.7](https://graypaper.fluffylabs.dev/#/5f542d7/095f00095f00) it seems technically JAM departs from good old [GP Eq 4.1](https://graypaper.fluffylabs.dev/#/5f542d7/087a00087a00) in needing [accumulation result tree root $r$](https://graypaper.fluffylabs.dev/#/5f542d7/0fe4010fe401) summarizing ${\bf C}$ from the state before $\sigma$.

id:
1350
timestamp:
2025-02-20T12:38:46.911Z
sender:
@sourabhniyogi:matrix.org
content:
* Also, due to [GP Eq 4.7](https://graypaper.fluffylabs.dev/#/5f542d7/095f00095f00) it seems technically JAM departs from good old [GP Eq 4.1](https://graypaper.fluffylabs.dev/#/5f542d7/087a00087a00) in needing [accumulation result tree root $r$](https://graypaper.fluffylabs.dev/#/5f542d7/0fe4010fe401) summarizing ${\\bf C}$ from the state before $\\sigma$.  So, a "state_transition" has to include $r$  (or ${\bf C}$) like [this](https://github.com/jam-duna/jamtestnet/blob/0.6.2.3/data/assurances/state_transitions/1_005.json#L289) -- yes?

id:
1349
timestamp:
2025-02-20T12:42:49.539Z
sender:
@sourabhniyogi:matrix.org
content:
* Also, due to [GP Eq 4.7](https://graypaper.fluffylabs.dev/#/5f542d7/095f00095f00) it seems technically JAM departs from good old [GP Eq 4.1](https://graypaper.fluffylabs.dev/#/5f542d7/087a00087a00) in needing [accumulation result tree root $r$](https://graypaper.fluffylabs.dev/#/5f542d7/0fe4010fe401) summarizing ${\\bf C}$ from the state before $\\sigma$.  So, to verify a isolated JAM "state\_transition", it has to not only include two states and block, but ALSO include this $r$  (or the _whole_ [Beefy commitment map ${\\bf C}$](https://graypaper.fluffylabs.dev/#/5f542d7/172a03172a03)) like [this](https://github.com/jam-duna/jamtestnet/blob/0.6.2.3/data/assurances/state_transitions/1_005.json#L289) -- can someone confirm this?

id:
1348
timestamp:
2025-02-20T12:49:54.638Z
sender:
@gav:polkadot.io
content:
> <@sourabhniyogi:matrix.org> I want a JAM codec expression after [here](https://graypaper.fluffylabs.dev/#/5f542d7/381700381700) to fully detail [wrangled operand tuples](https://graypaper.fluffylabs.dev/#/5f542d7/17fa0117fa01) to support [this ${\bf o}$ input into single accumulate](https://graypaper.fluffylabs.dev/#/5f542d7/2e89002e8900) -- is this reasonable?

Not sure what you’re talking about. 

id:
1347
timestamp:
2025-02-20T12:50:02.977Z
sender:
@gav:polkadot.io
content:
O is defined properly. 

id:
1346
timestamp:
2025-02-20T12:50:23.432Z
sender:
@gav:polkadot.io
content:
It can easily be fed into the serialization function. 

id:
1345
timestamp:
2025-02-20T12:50:33.382Z
sender:
@gav:polkadot.io
content:
* Its result can easily be fed into the serialization function. 

id:
1344
timestamp:
2025-02-20T12:51:20.444Z
sender:
@gav:polkadot.io
content:
As used in C.23

id:
1343
timestamp:
2025-02-20T14:23:37.902Z
sender:
@sourabhniyogi:matrix.org
content:
Will refine our guess --  https://hackmd.io/@sourabhniyogi/wrangledoperandtuples

id:
1342
timestamp:
2025-02-20T14:31:06.386Z
sender:
@gav:polkadot.io
content:
Cool - looks like you’ve already done most of the work for a PR to sort this - want to submit one?

id:
2747
timestamp:
2025-02-20T17:23:50.557Z
sender:
@dvdplas:matrix.org
content:
> <@gav:polkadot.io> It’s worth remembering that services themselves are asynchronous with regards to each other, limiting the possibility of inter-service race conditions. 

Polkadot wiki is wrong then: "JAM will provide synchronous composability across heterogeneous services, enabling new kinds of interoperability."

id:
2746
timestamp:
2025-02-20T17:27:15.678Z
sender:
@dvdplas:matrix.org
content:
> <@gav:polkadot.io> It’s worth remembering that services themselves are asynchronous with regards to each other, limiting the possibility of inter-service race conditions. 

 * Polkadot wiki says: "JAM will provide synchronous composability across heterogeneous services, enabling new kinds of interoperability."

id:
2745
timestamp:
2025-02-20T17:29:05.399Z
sender:
@erin:parity.io
content:
i can take another look

id:
2744
timestamp:
2025-02-20T17:29:33.614Z
sender:
@erin:parity.io
content:
* i can take another look, perhaps they've fixed something else by now

id:
1341
timestamp:
2025-02-20T18:09:56.301Z
sender:
@sourabhniyogi:matrix.org
content:
If someone else gets our refine => accumulate, we will give it a shot!  

id:
1340
timestamp:
2025-02-20T18:10:32.824Z
sender:
@sourabhniyogi:matrix.org
content:
* If someone else agrees with it, we will give it a shot!  

id:
2743
timestamp:
2025-02-21T14:03:46.677Z
sender:
@gav:polkadot.io
content:
I didn't write the wiki,

id:
2742
timestamp:
2025-02-21T14:03:48.752Z
sender:
@gav:polkadot.io
content:
* I didn't write the wiki.

id:
2741
timestamp:
2025-02-21T14:04:03.998Z
sender:
@gav:polkadot.io
content:
* I didn't write the wiki; someone should probably clarify it.

id:
2740
timestamp:
2025-02-21T14:04:50.721Z
sender:
@gav:polkadot.io
content:
JAM certainly makes it *easier* to provide synchronous composability across services, but it does not directly offer it as a feature.

id:
2739
timestamp:
2025-02-21T14:12:07.167Z
sender:
@dvdplas:matrix.org
content:
Alright, thanks for clarifying

id:
2738
timestamp:
2025-02-21T14:12:11.299Z
sender:
@dvdplas:matrix.org
content:
Created an issue on the wiki

id:
2737
timestamp:
2025-02-21T14:16:19.025Z
sender:
@bill:web3.foundation
content:
I'll ping the tech ed team to clarify ASAP. Thanks for filing an issue.

id:
2736
timestamp:
2025-02-21T17:52:23.098Z
sender:
@danicuki:matrix.org
content:
Hey Jammers, I have bad news for us: unfortunately our JAM Experience in Cascais proposal was rejected by the events bounties committee. If you have any idea of how we can get funded, please raise your hand! I attach for your appreciation the event proposal, as well the event bounties feedback. 

id:
2735
timestamp:
2025-02-21T17:52:33.927Z
sender:
@danicuki:matrix.org
content:
JAM Experience Portugal.pdf

id:
2734
timestamp:
2025-02-21T17:52:49.675Z
sender:
@danicuki:matrix.org
content:
```
rently on the tour with Gavin. 

7. This event feels like multiple things: connecting implementers, sharing knowledge about JAM and preparing folks for ETH Lisbon. These are each an undertaking of their own, so we would propose organising only one of these formats. Additionally, aren't there around 30 teams of implementers // cca 100 people? The events bounty would not fund all these travel costs. 

8. We know that the Palace is currently under construction and don’t know when it would be open for people to come and actually go there. 

9. If you would like assistance from us on how to format your event we would be happy to help. 

Thank you.
```

id:
2733
timestamp:
2025-02-21T17:53:15.741Z
sender:
@danicuki:matrix.org
content:
* ```
Hi Daniel, 


Thank you for submitting your proposal, unfortunately it has been rejected. While we think that organising events which either support the implementers or wider reach of the technology, we are of the opinion that the budget and format of the event are not fitting. Regardless, we would still like to encourage you and even assist if necessary with planning a different type of event. 


Please find our reasons for rejection below: 


1. The Events Bounty is generally not in favour of standalone large events, as it is difficult to promote them and grow an audience. The location you choose would have also been proven difficult to attract further guests on the second day. 

2. We don’t see why you would be preparing devs for ETH Lisbon in a location outside of Lisbon, especially if the conference is centered around JAM. 

3. For this amount of attendees, the approximate agenda cost of the event is too high. 


4. It is expected that proponents use the attached budget sheet separately from the document so we can properly keep track of all entries. Every line item should be in USD, as well as the final price.  


5. We are not sure what “look and feel” entailed in the budget sheet. 

6. There is a full production team included in the budget, but it is not clear what the purposes of their roles are. Additionally, Adlib did not do the JAM Tour, nor the Grey Paper tour, this is done by Pala Labs and they are currently on the tour with Gavin. 

7. This event feels like multiple things: connecting implementers, sharing knowledge about JAM and preparing folks for ETH Lisbon. These are each an undertaking of their own, so we would propose organising only one of these formats. Additionally, aren't there around 30 teams of implementers // cca 100 people? The events bounty would not fund all these travel costs. 

8. We know that the Palace is currently under construction and don’t know when it would be open for people to come and actually go there. 

9. If you would like assistance from us on how to format your event we would be happy to help. 

Thank you
```

id:
2732
timestamp:
2025-02-21T17:53:45.328Z
sender:
@danicuki:matrix.org
content:
* ```
Hi Daniel, 


Thank you for submitting your proposal, unfortunately it has been rejected. While we think that organising events which either support the implementers or wider reach of the technology, we are of the opinion that the budget and format of the event are not fitting. Regardless, we would still like to encourage you and even assist if necessary with planning a different type of event. 

Please find our reasons for rejection below: 

1. The Events Bounty is generally not in favour of standalone large events, as it is difficult to promote them and grow an audience. The location you choose would have also been proven difficult to attract further guests on the second day. 
2. We don’t see why you would be preparing devs for ETH Lisbon in a location outside of Lisbon, especially if the conference is centered around JAM. 
3. For this amount of attendees, the approximate agenda cost of the event is too high. 
4. It is expected that proponents use the attached budget sheet separately from the document so we can properly keep track of all entries. Every line item should be in USD, as well as the final price.  
5. We are not sure what “look and feel” entailed in the budget sheet. 
6. There is a full production team included in the budget, but it is not clear what the purposes of their roles are. Additionally, Adlib did not do the JAM Tour, nor the Grey Paper tour, this is done by Pala Labs and they are currently on the tour with Gavin. 
7. This event feels like multiple things: connecting implementers, sharing knowledge about JAM and preparing folks for ETH Lisbon. These are each an undertaking of their own, so we would propose organising only one of these formats. Additionally, aren't there around 30 teams of implementers // cca 100 people? The events bounty would not fund all these travel costs. 
8. We know that the Palace is currently under construction and don’t know when it would be open for people to come and actually go there. 
9. If you would like assistance from us on how to format your event we would be happy to help. 
```

id:
2731
timestamp:
2025-02-21T17:54:36.914Z
sender:
@danicuki:matrix.org
content:
If you also have any idea of how I can improve the proposal, I am also open to collaboration


id:
2730
timestamp:
2025-02-21T17:54:37.851Z
sender:
@danicuki:matrix.org
content:
If we don

id:
2729
timestamp:
2025-02-21T17:54:56.287Z
sender:
@danicuki:matrix.org
content:
* We need to act quick, otherwise the event will be cancelled.

id:
2728
timestamp:
2025-02-21T17:55:13.108Z
sender:
@danicuki:matrix.org
content:
* ```
Hi Daniel,  Thank you for submitting your proposal, unfortunately it has been rejected. While we think that organising events which either support the implementers or wider reach of the technology, we are of the opinion that the budget and format of the event are not fitting. Regardless, we would still like to encourage you and even assist if necessary with planning a different type of event. 

Please find our reasons for rejection below: 

1. The Events Bounty is generally not in favour of standalone large events, as it is difficult to promote them and grow an audience. The location you choose would have also been proven difficult to attract further guests on the second day. 
2. We don’t see why you would be preparing devs for ETH Lisbon in a location outside of Lisbon, especially if the conference is centered around JAM. 
3. For this amount of attendees, the approximate agenda cost of the event is too high. 
4. It is expected that proponents use the attached budget sheet separately from the document so we can properly keep track of all entries. Every line item should be in USD, as well as the final price.  
5. We are not sure what “look and feel” entailed in the budget sheet. 
6. There is a full production team included in the budget, but it is not clear what the purposes of their roles are. Additionally, Adlib did not do the JAM Tour, nor the Grey Paper tour, this is done by Pala Labs and they are currently on the tour with Gavin. 
7. This event feels like multiple things: connecting implementers, sharing knowledge about JAM and preparing folks for ETH Lisbon. These are each an undertaking of their own, so we would propose organising only one of these formats. Additionally, aren't there around 30 teams of implementers // cca 100 people? The events bounty would not fund all these travel costs. 
8. We know that the Palace is currently under construction and don’t know when it would be open for people to come and actually go there. 
9. If you would like assistance from us on how to format your event we would be happy to help. 
```

id:
2727
timestamp:
2025-02-21T17:56:11.215Z
sender:
@emielsebastiaan:matrix.org
content:
I tend to agree. Focus on JAM implementers coordination. Do not focus on Ethereum event and audience. 

id:
2726
timestamp:
2025-02-21T17:57:21.244Z
sender:
@emielsebastiaan:matrix.org
content:
Scale down and focus on what is important. Palace visit would be nice, but not an absolute necessity. 

id:
2725
timestamp:
2025-02-21T18:02:06.416Z
sender:
@danicuki:matrix.org
content:
they use "head count" as a metric of success. This is the problem. We are just 30-40 

I was trying to do something more holistic 

id:
2724
timestamp:
2025-02-21T18:17:27.645Z
sender:
@danicuki:matrix.org
content:
But maybe Emiel are right. I was trying to hit two rabbits with one stone and in the end missed both. I will re-structure in a more modest format

id:
2723
timestamp:
2025-02-21T19:41:53.658Z
sender:
@gav:polkadot.io
content:
> <@emielsebastiaan:matrix.org> Scale down and focus on what is important. Palace visit would be nice, but not an absolute necessity. 

Palace visit should be no problem at all. Just wont be an especially long stay. 

id:
1339
timestamp:
2025-02-22T00:16:23.774Z
sender:
@charliewinston14:matrix.org
content:
Hello.

I’m having some difficulty understanding the erasure root formula in the GP, specifically calculating “s♣” in 14.16.

Hoping someone can point me in the right direction. 

I had no problem calculating “b♣” and have my erasure coding function C and paged proof generation function P already.

The s♣ formula has C#6 (s⌢ P(s))), where S is an array and P(S) is an array as well. Does that mean to concatenate them both together and then pass to chunking function?  I think it’s the # that is confusing me as that normally means apply to each of the sub items. There is also a # on the binary merkle call so I’m assuming that I need to call the merkle function multiple times and not just once with the results of the erasure encoding but not understanding the formula at all. Can someone give me a tip of how to proceed with it?

https://graypaper.fluffylabs.dev/#/5f542d7/1b4c011b5701


id:
2722
timestamp:
2025-02-22T00:32:32.987Z
sender:
@sourabhniyogi:matrix.org
content:
Suggestion is do this for May 7+8 for < 10K EUR
* Meeting room, 2 days + coffee breaks + dinner - Venue €6,863.00
* Celebration Dinner Day 1 Food & Drinks €1,476.00
* Transportation to Toaster (40 pax) Ext Travel €645.00

What else do you really really need over than the above.

The JAM0 in Bangkok was ~ $11-12K (sponsored by us) for ~5 days --  the above gets us 2 days for a similar price, and probably JAM teams can meet on the 6th or the 9th on their own to connect implementations.  I'd like to believe we can to real tiny=>small stuff and have a Toaster summer plan by the end of this.

I've been amazed at how much we can quickly accomplish asynchronously with github given GP clarity, but I'm sure I'm not the only one who believes that its in part because we've met each other IRL that we are able to accomplish this.  So lets be sure to make this happen.

If it doesn't happen (sad!), we can host a week at the end of the summer here in San Mateo California.  

id:
2721
timestamp:
2025-02-22T00:36:42.822Z
sender:
@sourabhniyogi:matrix.org
content:
* Suggestion is do this for May 7+8 for \< 10K EUR

- Meeting room, 2 days + coffee breaks + dinner - Venue €6,863.00
- Celebration Dinner Day 1 Food & Drinks €1,476.00
- Transportation to Toaster (40 pax) Ext Travel €645.00

What else do you really really need over than the above.

The JAM0 in Bangkok was ~ $11-12K (sponsored by us) for ~5 days --  the above gets us 2 days for a similar price, and probably JAM teams can meet on the 6th or the 9th on their own to connect implementations.  I'd like to believe we can to real tiny=>small stuff and have a Toaster summer plan by the end of this.

I've been amazed at how much we can quickly accomplish asynchronously with github given GP clarity, but I'm sure I'm not the only one who believes that its in part because we've met each other IRL that we are able to accomplish this.  So lets be sure to make this happen.

If it doesn't happen (sad!) [and also probably if it does], we can host a week at the end of the summer here in San Mateo California, though I imagine the total cost to all of you non-US people is way higher than just doing this type of event again in Europe.

id:
2720
timestamp:
2025-02-22T00:57:31.993Z
sender:
@gav:polkadot.io
content:
For Lisbon, parity office can be used. 

id:
2719
timestamp:
2025-02-22T00:58:25.826Z
sender:
@gav:polkadot.io
content:
This is big enough for 50 people to work from easily and maybe up to 100 with a little creativity. 

id:
2718
timestamp:
2025-02-22T00:58:56.906Z
sender:
@gav:polkadot.io
content:
Once the palace is completed then none of this will be a problem ofc:))

id:
1338
timestamp:
2025-02-22T01:57:46.894Z
sender:
@ascriv:matrix.org
content:
Is there a very rough estimate for v1.0.0? Trying to think if I can make milestone 1 by that time :v

id:
1337
timestamp:
2025-02-22T02:45:46.101Z
sender:
@gav:polkadot.io
content:
Latest estimate is by end of Q3. But will depend a lot on the outcome of Toaster and initial service development. 

id:
1336
timestamp:
2025-02-22T03:47:33.366Z
sender:
@ymcsabo:matrix.org
content:
Hi, in the gray paper section 15.2, it mentions advanced nodes and naive nodes. What are some of the examples of those two types of nodes?

id:
1335
timestamp:
2025-02-22T07:24:19.515Z
sender:
@gav:polkadot.io
content:
There is no clear difference. It’s more about *strategy*. 

id:
1334
timestamp:
2025-02-22T07:25:56.193Z
sender:
@gav:polkadot.io
content:
* There is no clear delineation. It’s about *strategy*. Some implementations (or node configurations) may use a more sophisticated strategy for predicting the best work package to execute and guarantee. 

id:
1333
timestamp:
2025-02-22T07:25:57.614Z
sender:
@luke_fishman:matrix.org
content:
I need some clarification regarding the advancement  of the instruction counter i in the PVM

Reading [A.1](https://graypaper.fluffylabs.dev/#/5f542d7/23ec0023ec00) and [A.7](https://graypaper.fluffylabs.dev/#/5f542d7/246600246600)

I understand the counter i` will always advance to the next instruction unless the exit reason is panic or halts

so if we have program like
ecalli ..
op1 ...
op2 ...

host call fail => continue from op1
host call succeed => continue from op2 (due to the extra skip in [A.33] (https://graypaper.fluffylabs.dev/#/5f542d7/2b70012b7001)


However, reading the [text](https://graypaper.fluffylabs.dev/#/5f542d7/2b16022b1602) below A.34 i understand that

i' is:
exit reason == continue > i + 1 +skip
out of gas => i
panic or halt => 0
page fault => i
host call => i


but this makes [A.33] (https://graypaper.fluffylabs.dev/#/5f542d7/2b70012b7001) not make sense
as now we will have

host call fail => counter stays => reinvoke into the failing host call
 


id:
1332
timestamp:
2025-02-22T07:27:25.698Z
sender:
@gav:polkadot.io
content:
* There is no clear delineation. It’s about *strategy*. Some implementations (or node configurations) may use a more sophisticated strategy for predicting the best work package to execute and guarantee. This will allow their operators to take greater rewards under some circumstances. But again, this is strategy and therefore largely out of scope for the GP. 

id:
1331
timestamp:
2025-02-22T07:32:24.999Z
sender:
@gav:polkadot.io
content:
You seem to be confusing two different conditions. 

id:
1330
timestamp:
2025-02-22T07:34:29.393Z
sender:
@gav:polkadot.io
content:
If the hostcall succeeds (where you pointed) then i’’ is used which skips past the ecalli instruction. If the hostcall results in anything other than a continue (the last condition) then Phi_H is not invoked again anyway. 

id:
1329
timestamp:
2025-02-22T07:35:26.885Z
sender:
@gav:polkadot.io
content:
* If the hostcall succeeds (where you pointed) then i’’ is used as the new instruction counter for the invocation of Phi_H which effectively skips past the ecalli instruction. If the hostcall results in anything other than a continue (the last condition) then Phi_H is not invoked again anyway. 

id:
1328
timestamp:
2025-02-22T07:43:04.055Z
sender:
@luke_fishman:matrix.org
content:
right. my code does just that. no issue

so lets talk about the case where the host call succeeds

we start with phi_1 which returned ecalli, and i' = i +1 +skip (i.e point to instruction after the the ecalli)

and so we don't need to advance again after the host call has succeeded. since we already point to the next instruction

or, the Phi_1 should not advance the counter on a ecalli exit reason, and after host call finishes with success the counter advances again

basically, why the text below A.34 says the i` point the the host call, when seems to me from A.7 that it has already progressed beyond






 

id:
1327
timestamp:
2025-02-22T07:43:15.254Z
sender:
@luke_fishman:matrix.org
content:
* right. my code does just that. no issue

so lets talk about the case where the host call succeeds

we start with phi\_1 which returned ecalli, and i' = i +1 +skip (i.e point to instruction after the the ecalli)

and so we don't need to advance again after the host call has succeeded. since we already point to the next instruction

or, the Phi\_1 should not advance the counter on a ecalli exit reason, and after host call finishes with success the counter advances again

basically, why the text below A.34 says the i\` point the the host call, when seems to me from A.7 that it has already progressed beyond

id:
1326
timestamp:
2025-02-22T07:44:23.812Z
sender:
@luke_fishman:matrix.org
content:
* right. my code does just that. no issue

so lets talk about the case where the host call succeeds

we start with phi\_1 which returned ecalli, and i' = i +1 +skip (i.e point to instruction after the the ecalli)

and so we don't need to advance again after the host call has succeeded. since we already point to the next instruction

or, the Phi\_1 should not advance the counter on a ecalli exit reason, and after host call finishes with success then the counter advances

basically, why the text below A.34 says the i\` point the the host call, when seems to me from A.7 that it has already progressed beyond

id:
1325
timestamp:
2025-02-22T08:06:07.015Z
sender:
@gav:polkadot.io
content:
I see your point, yes. i’’ needs not be defined; i’ should be used instead. Feel free to make a PR if i don’t get to it first. 

id:
1324
timestamp:
2025-02-22T08:07:15.871Z
sender:
@luke_fishman:matrix.org
content:
yep. that's what i thought , i'' is not needed
Thank you for confirming

id:
1323
timestamp:
2025-02-22T11:48:03.155Z
sender:
@luke_fishman:matrix.org
content:
regarding i in [B.9](https://graypaper.fluffylabs.dev/#/5f542d7/2e11012e1101)

```
i = check((E4−1 (H(E(s, η0′ , Ht ))) mod (232 − 29 )) + 28 )
```

does the decode_4 bytes imply that only the first(last?) bytes of the hash are to be taken?

id:
1322
timestamp:
2025-02-22T11:48:24.988Z
sender:
@luke_fishman:matrix.org
content:
* regarding i in [B.9](https://graypaper.fluffylabs.dev/#/5f542d7/2e11012e1101)

```
i = check((E4−1 (H(E(s, η0′ , Ht ))) mod (2^32 − 2^9 )) + 2^8 )
```

does the decode\_4 bytes imply that only the first(last?) bytes of the hash are to be taken?

id:
1321
timestamp:
2025-02-22T11:48:39.728Z
sender:
@luke_fishman:matrix.org
content:
* regarding `i` in [B.9](https://graypaper.fluffylabs.dev/#/5f542d7/2e11012e1101)

```
i = check((E4−1 (H(E(s, η0′ , Ht ))) mod (2^32 − 2^9 )) + 2^8 )
```

does the decode\_4 bytes imply that only the first(last?) bytes of the hash are to be taken?

id:
2717
timestamp:
2025-02-22T11:51:05.723Z
sender:
@danicuki:matrix.org
content:
Thanks all for the feedback. We will make it happen!! Will submit a less ambitious proposal soon. 

id:
2716
timestamp:
2025-02-22T11:55:55.092Z
sender:
@danicuki:matrix.org
content:
> <@gav:polkadot.io> This is big enough for 50 people to work from easily and maybe up to 100 with a little creativity. 

Great! Let’s do there then. I will coordinate with the office manager. 

id:
2715
timestamp:
2025-02-22T11:57:25.967Z
sender:
@danicuki:matrix.org
content:
Already started to create songs for the GP 1.0 JAM Party 🎉 . https://suno.com/song/44ed243a-232a-4ed6-be05-34becd213b4e

id:
2714
timestamp:
2025-02-22T12:13:31.273Z
sender:
@danicuki:matrix.org
content:
* Already started to create songs for the GP 1.0 JAM Party Playlist 🎉 . https://suno.com/song/44ed243a-232a-4ed6-be05-34becd213b4e

id:
1320
timestamp:
2025-02-22T13:50:47.527Z
sender:
@gav:polkadot.io
content:
Yes

id:
1319
timestamp:
2025-02-22T15:19:25.975Z
sender:
@sourabhniyogi:matrix.org
content:
Question on what the first value of a_t is in `new` [here](https://graypaper.fluffylabs.dev/#/293bf5a/2e81022e8102) which is defined in [9.8](https://graypaper.fluffylabs.dev/#/5f542d7/115f01115f01):

Assume the preimage of the code is 1149 bytes, and recall GP constants: B_S = 100, B_I = 10, B_L = 1

What is the value of a_i and a_o and thus a_t:

* (1): a_i = 0, a_o = 0 ==> a_t = 100
* (2): a_i = 2, a_o = 81 + 1149 = 1230 ==> a_t = 100 + 10 * 2 + 1 * 1230 = 1350

The order of operations in `new` is not clear, especially with the a_t and l "happening in the same line" here: https://graypaper.fluffylabs.dev/#/5f542d7/31b90231b902

id:
1318
timestamp:
2025-02-22T15:30:49.525Z
sender:
@gav:polkadot.io
content:
a_t is determined through the non-negotiable definition of bold-l which is fully defined. 

id:
1317
timestamp:
2025-02-22T15:31:24.504Z
sender:
@gav:polkadot.io
content:
* a_t is a dependent variable whose value is implied through the (non-negotiable) definition of bold-l which is fully defined. 

id:
1316
timestamp:
2025-02-22T15:31:59.788Z
sender:
@gav:polkadot.io
content:
balance is required to be equal to to this (dependent) variable. There exists only one solution to this statement. 

id:
1315
timestamp:
2025-02-22T15:32:09.839Z
sender:
@gav:polkadot.io
content:
* balance is required to be equal to this (dependent) variable. There exists only one solution to this statement. 

id:
1314
timestamp:
2025-02-22T15:32:33.001Z
sender:
@gav:polkadot.io
content:
* a_t is a dependent variable whose value is implied through the (non-negotiable) definition of bold-l, which is fully defined. 

id:
1313
timestamp:
2025-02-22T15:32:50.687Z
sender:
@gav:polkadot.io
content:
No order is needed. Ever. 

id:
1312
timestamp:
2025-02-22T15:33:45.226Z
sender:
@gav:polkadot.io
content:
* No specific order is *needed*. Ever. 

id:
1311
timestamp:
2025-02-22T15:33:59.378Z
sender:
@gav:polkadot.io
content:
That’s a point of implementation strategy. 

id:
1310
timestamp:
2025-02-22T15:35:21.217Z
sender:
@gav:polkadot.io
content:
* Ordering is a point of implementation strategy, for implementation languages which require the practitioner to specify it (ie imperative ones). 

id:
1309
timestamp:
2025-02-22T15:36:37.246Z
sender:
@gav:polkadot.io
content:
Plenty of languages, like formal logic, don’t generally insist on specifying a solution in terms of ordered mutations. 

id:
1308
timestamp:
2025-02-22T15:37:21.754Z
sender:
@gav:polkadot.io
content:
If this is a new concept, I’d suggest reading some undergrad computer science texts such as “structure and interpretation of computer languages”. 

id:
1307
timestamp:
2025-02-22T15:37:38.182Z
sender:
@gav:polkadot.io
content:
* If this is a new concept, I’d suggest reading some undergrad computer science texts such as “structure and interpretation of computer programmes”. 

id:
1306
timestamp:
2025-02-22T15:38:56.771Z
sender:
@gav:polkadot.io
content:
* a_t is a dependent variable whose value is implied through the (non-negotiable) definition of bold-l, which is fully defined as c and l are both fixed values. 

id:
2713
timestamp:
2025-02-22T16:32:14.470Z
sender:
@davxy:matrix.org
content:
@danikuki, I noticed [here](https://docs.google.com/spreadsheets/d/1JVt_1daKJWslCaP9hfggKQSN4aGuV_le0XkKcsrjXDc/edit?gid=0#gid=0) that Jamixir provides test vectors. Are these vectors the same as the ones listed in this repository: https://github.com/jamixir/jamtestnet? If so, how do they differ from the ones provided by JamDuna?

id:
2712
timestamp:
2025-02-22T16:32:25.889Z
sender:
@davxy:matrix.org
content:
* danicuki | Jamixir: , I noticed [here](https://docs.google.com/spreadsheets/d/1JVt_1daKJWslCaP9hfggKQSN4aGuV_le0XkKcsrjXDc/edit?gid=0#gid=0) that Jamixir provides test vectors. Are these vectors the same as the ones listed in this repository: https://github.com/jamixir/jamtestnet? If so, how do they differ from the ones provided by JamDuna?

id:
2711
timestamp:
2025-02-22T16:39:14.174Z
sender:
@dakkk:matrix.org
content:
> <@davxy:matrix.org> danicuki | Jamixir: , I noticed [here](https://docs.google.com/spreadsheets/d/1JVt_1daKJWslCaP9hfggKQSN4aGuV_le0XkKcsrjXDc/edit?gid=0#gid=0) that Jamixir provides test vectors. Are these vectors the same as the ones listed in this repository: https://github.com/jamixir/jamtestnet? If so, how do they differ from the ones provided by JamDuna?

You should use the branch called "jamixir"

id:
2710
timestamp:
2025-02-22T19:16:56.798Z
sender:
@davxy:matrix.org
content:
* danicuki | Jamixir: , I noticed [here](https://docs.google.com/spreadsheets/d/1JVt_1daKJWslCaP9hfggKQSN4aGuV_le0XkKcsrjXDc/edit?gid=0#gid=0) that Jamixir provides test vectors. Are these vectors the same as the ones listed in this repository: https://github.com/jamixir/jamtestnet? If not, how do they differ from the ones provided by JamDuna?

id:
2709
timestamp:
2025-02-22T20:45:45.585Z
sender:
@junger0x:matrix.org
content:
Hi, it's Jünger from Taiko, a based rollup on Ethereum. I want to analyse JAM for Ethereum perspective for potential new designs. what are the best sources other than the whitepaper?

id:
2708
timestamp:
2025-02-22T20:46:38.530Z
sender:
@gav:polkadot.io
content:
There is no white paper - just the graypaper:)

id:
2707
timestamp:
2025-02-22T20:46:53.934Z
sender:
@junger0x:matrix.org
content:
Yeah, you're right 😆

id:
2706
timestamp:
2025-02-22T20:46:57.214Z
sender:
@gav:polkadot.io
content:
But that is really the best and only proper source 

id:
2705
timestamp:
2025-02-22T20:47:55.109Z
sender:
@gav:polkadot.io
content:
I’ve done some talks on it and the one I consider the best (Taipei last week) will be published in the next day or two. For a basic introduction to its premise this would be a good start. 

id:
2704
timestamp:
2025-02-22T20:48:25.704Z
sender:
@gav:polkadot.io
content:
There are some overviews that people has written; you can find many of these linked on graypaper.com

id:
2703
timestamp:
2025-02-22T20:48:32.127Z
sender:
@gav:polkadot.io
content:
* There are some overviews that people have written; you can find many of these linked on graypaper.com

id:
2702
timestamp:
2025-02-22T20:48:55.220Z
sender:
@gav:polkadot.io
content:
But they’re not always up to date, complete or even correct. 

id:
2701
timestamp:
2025-02-22T20:49:15.258Z
sender:
@gav:polkadot.io
content:
The graypaper is the definitive resource for doing any kind of rational analysis. 

id:
2700
timestamp:
2025-02-22T20:52:14.563Z
sender:
@junger0x:matrix.org
content:
Let me get my hands dirty with greypaper then. Thanks for the answers Gavin. 

id:
2699
timestamp:
2025-02-22T20:53:22.314Z
sender:
@gav:polkadot.io
content:
Have fun!:)

id:
1305
timestamp:
2025-02-23T03:52:01.661Z
sender:
@luke_fishman:matrix.org
content:
* good morning everyone
very small specific question about encoding

looking at the encoding in the calculation of `i` in [B.9](https://graypaper.fluffylabs.dev/#/5f542d7/2ef9002ef900)

`E(s, η0′ , Ht )`

for me the symbol e means general encoding [C.6](https://graypaper.fluffylabs.dev/#/5f542d7/365702365702)

But the text under C.6 says
_"Note that at present this is utilized only in encoding the length prefix of variable-length sequences."_

which would imply:

- service index is encoded as 4 bytes
(refs [9.1](https://graypaper.fluffylabs.dev/#/5f542d7/10e40010e400), [C.23](https://graypaper.fluffylabs.dev/#/5f542d7/377202377202))
- timeslot is encoded as 4 bytes as well (refs [I.1.1](https://graypaper.fluffylabs.dev/#/5f542d7/3e36003e3600), [C.16, C.20, C.22](https://graypaper.fluffylabs.dev/#/5f542d7/37e50037e500))

id:
1304
timestamp:
2025-02-23T11:26:24.306Z
sender:
@vinsystems:matrix.org
content:
In eq [12.10](https://graypaper.fluffylabs.dev/#/5f542d7/16c90116d001), Does `m` should be `m'` since `Ht = tau'`?

id:
1303
timestamp:
2025-02-23T12:04:22.426Z
sender:
@gav:polkadot.io
content:
I generally prefer not using a prime unless the plain (non-prime) term is also used. 

id:
2698
timestamp:
2025-02-23T15:01:30.058Z
sender:
@oliver.tale-yazdi:parity.io
content:
https://x.com/jam_brains/status/1893662855334965306

id:
2697
timestamp:
2025-02-23T15:01:46.272Z
sender:
@oliver.tale-yazdi:parity.io
content:
* https://x.com/jam\_brains/status/1893662855334965306  

We went ahead with this idea from sourabhniyogi on adding an RPC for STF validation

id:
2696
timestamp:
2025-02-23T15:02:39.800Z
sender:
@oliver.tale-yazdi:parity.io
content:
Let us know how helpful this is with validating your STF vectors. Currently our validation logic is probably too lenient, so some invalid blocks wil import, but no valid block should be rejected.

id:
2695
timestamp:
2025-02-23T15:02:53.152Z
sender:
@oliver.tale-yazdi:parity.io
content:
* Let us know how helpful this is with validating your STF vectors. Currently our validation logic is probably too lenient, so some invalid blocks will import, but no valid block should be rejected.

id:
1302
timestamp:
2025-02-23T18:32:05.072Z
sender:
@ascriv:matrix.org
content:
I assume that when inspecting memory during the sbrk instruction, this should not case a memory-access exception, right? Also, is it right to interpret the math as saying “find the earliest inaccessible contiguous memory segment starting at or above h of length wa, and set it as mutable”?

id:
1301
timestamp:
2025-02-23T19:05:49.805Z
sender:
@gav:polkadot.io
content:
For sbrk, I’ll leave it in the hands of [@jan:parity.io](https://matrix.to/#/@jan:parity.io) - I’d personally quite like to get rid of it:))

id:
1300
timestamp:
2025-02-23T20:49:47.849Z
sender:
@ascriv:matrix.org
content:
neg_add_imm_64 has a +2^64 but then mods by 2^64 so this addition is the same as +0, so it’s redundant. Unless there’s a typo 

id:
1299
timestamp:
2025-02-23T21:06:57.665Z
sender:
@ascriv:matrix.org
content:
rot_r_64_imm performs a left shift as written (ith bit of w’a = i+vx bit of wb), but the name suggests a right shift. Is this correct?

id:
1298
timestamp:
2025-02-23T21:07:30.352Z
sender:
@ascriv:matrix.org
content:
Similar for the next 3 instructions 

id:
1297
timestamp:
2025-02-23T23:09:44.035Z
sender:
@ascriv:matrix.org
content:
^similar for instructions 220-223

id:
1296
timestamp:
2025-02-23T23:10:32.822Z
sender:
@ascriv:matrix.org
content:
Also, shouldn’t we be doing Z inverse on the result of the max (227) and min (229) instructions? To convert back to unsigned before storing in the register 

id:
1295
timestamp:
2025-02-24T02:58:22.643Z
sender:
@charliewinston14:matrix.org
content:
Morning all. Are the "EC shards" referenced in CE137 the same as the "bundle shards" referenced in CE138? What is the difference between these two APIs? Are they essentially the same except CE137 also returns segment shards?

id:
2694
timestamp:
2025-02-24T09:25:47.660Z
sender:
@danicuki:matrix.org
content:
I just submitted the a new proposal do the events bounty with a $18k budget. This is the budget breakdown spreadsheet. 

https://docs.google.com/spreadsheets/d/1GnWLxHMl9cj7lA3RkZMbqhH8XnVpN4nV81Xao95n_m4/edit?usp=sharing

id:
2693
timestamp:
2025-02-24T09:26:17.822Z
sender:
@danicuki:matrix.org
content:
* I just submitted a new proposal do the events bounty with a $18k budget. This is the budget breakdown spreadsheet. 

https://docs.google.com/spreadsheets/d/1GnWLxHMl9cj7lA3RkZMbqhH8XnVpN4nV81Xao95n\_m4/edit?usp=sharing

id:
2692
timestamp:
2025-02-24T10:45:06.732Z
sender:
@danicuki:matrix.org
content:
We are still working to setup for the JAM Experience May/25 event in Lisbon. I would ask you please to subscribe [here](https://lu.ma/ob0n7pdy) if you plan to come. This is very **important**, so we can plan better with the approximate number of people.

id:
2691
timestamp:
2025-02-24T10:46:29.661Z
sender:
@danicuki:matrix.org
content:
* We are still working to setup for the JAM Experience May/25 event in Lisbon. I would ask you please to subscribe [here](https://lu.ma/ob0n7pdy) if you plan to come. This is very **important**, so we can plan better with the approximate number of people.

As suggested by @gav, we will make the event at the Parity's office in Lisbon, making it less complex and more affordable. 

id:
2690
timestamp:
2025-02-24T10:46:55.983Z
sender:
@danicuki:matrix.org
content:
* We are still working to setup for the JAM Experience May/25 event in Lisbon. I would ask you please to subscribe [here](https://lu.ma/ob0n7pdy) if you plan to come. This is very **important**, so we can plan better with the approximate number of people.

As suggested by gav , we will make the event at the Parity's office in Lisbon, less complex and more affordable.

id:
1294
timestamp:
2025-02-24T11:57:34.266Z
sender:
@shwchg:matrix.org
content:
Hi Dr.Wood
https://github.com/gavofyork/graypaper/pull/248
Can we use beta_dagga as the only reference(instead of beta+beta_dagga) for all processes in Section 11?


id:
1293
timestamp:
2025-02-24T12:03:46.972Z
sender:
@gav:polkadot.io
content:
> <@shwchg:matrix.org> Hi Dr.Wood
> https://github.com/gavofyork/graypaper/pull/248
> Can we use beta_dagga as the only reference(instead of beta+beta_dagga) for all processes in Section 11?
> 

The two should give equal effects in Section 11 as the only difference is the placement of the beefy root. 

id:
1292
timestamp:
2025-02-24T12:04:41.501Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> neg_add_imm_64 has a +2^64 but then mods by 2^64 so this addition is the same as +0, so it’s redundant. Unless there’s a typo 

I don’t define negative modulo; this ensures the modulo is positive. 

id:
1291
timestamp:
2025-02-24T12:07:03.998Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> rot_r_64_imm performs a left shift as written (ith bit of w’a = i+vx bit of wb), but the name suggests a right shift. Is this correct?

It is correct. Check the implementation of caligraphic B. 

id:
1290
timestamp:
2025-02-24T12:07:14.912Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> rot_r_64_imm performs a left shift as written (ith bit of w’a = i+vx bit of wb), but the name suggests a right shift. Is this correct?

* It is correct. Check the definition of caligraphic B. 

id:
1289
timestamp:
2025-02-24T12:08:10.123Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> Also, shouldn’t we be doing Z inverse on the result of the max (227) and min (229) instructions? To convert back to unsigned before storing in the register 

Yes. If you’re in the mood feel free to submit a PR. 

id:
1288
timestamp:
2025-02-24T13:23:47.585Z
sender:
@0xjunha:matrix.org
content:
> <@ascriv:matrix.org> Also, shouldn’t we be doing Z inverse on the result of the max (227) and min (229) instructions? To convert back to unsigned before storing in the register 

Actually this change is merged into main - probably will be included in the next release? https://github.com/gavofyork/graypaper/pull/228

id:
2689
timestamp:
2025-02-25T14:57:06.791Z
sender:
@danicuki:matrix.org
content:
Trying to understand the flow of information from end-user to blockchain state in JAM, I created this "step-by-step" doc. I am still not 100% sure it is accurate, so any feedback is very welcome:

https://hackmd.io/@w_7Gu9yjR4q8dsYR89m4OA/ryIwYLj5kl

id:
2688
timestamp:
2025-02-25T14:59:06.195Z
sender:
@dvdplas:matrix.org
content:
Amazing!

id:
2687
timestamp:
2025-02-25T15:02:50.958Z
sender:
@ascriv:matrix.org
content:
Only going if there are jam themed snacks 

id:
2686
timestamp:
2025-02-25T15:03:34.855Z
sender:
@danicuki:matrix.org
content:
good idea! Let's make it a reality

id:
1287
timestamp:
2025-02-25T15:20:30.530Z
sender:
@jay_ztc:matrix.org
content:
Hi team 👋 Quick question about the Merkle function in D.6-> the branch splitting conditional implies that the key should be left shifted one bit before each recursion. Am I interpreting this correctly? The tests & gh consensus suggests that the keys shouldn't be rotated at each recursion, but rather that the $d'th bit should be used in the splitting conditional at recursion depth $d. I'm happy to open a PR to the GP repo if appropriate.

https://graypaper.fluffylabs.dev/#/5f542d7/391b01391c01

id:
1286
timestamp:
2025-02-25T15:58:58.926Z
sender:
@shwchg:matrix.org
content:
https://graypaper.fluffylabs.dev/#/5f542d7/10b00010b000
If the authorization pool is all the same hash, and there is a guarantee that using that hash as authorization, will it consume the entire pool? and will the queue then refill with eight more?

id:
1285
timestamp:
2025-02-25T16:01:39.453Z
sender:
@dave:parity.io
content:
Only one of the hashes should be consumed, see https://graypaper.fluffylabs.dev/#/5f542d7/07f70007fa00

id:
1284
timestamp:
2025-02-25T16:10:33.366Z
sender:
@shwchg:matrix.org
content:
ok I see!
thanks for reply

id:
1283
timestamp:
2025-02-25T16:23:22.439Z
sender:
@sourabhniyogi:matrix.org
content:
In order to compare large amounts of PVM traces between teams precisely, I would like a formula to hash the registers with the PVM paged memory for teams to know they ended up with same answer at the end, and if they did not, be able to quickly determine which line they differed in results.  

Its not hard to come up with a procedure, but does a ready made answer  exist within the PolkaVM repo or is there some public algorithm to do this kind of operation so we don't reinvent the wheel needlessly?   

id:
2685
timestamp:
2025-02-25T16:26:53.046Z
sender:
@vinsystems:matrix.org
content:
Interesting article Daniel 👌 after reading it I have a (maybe very basic) question: What kind of nodes interact with Jam? My understanding is:

Guarantors: nodes running "Jam".
Validators: nodes running "validator software".
Auditors: nodes running "auditor software".
Data Lake: Nodes running data lake' software

id:
2684
timestamp:
2025-02-25T16:27:04.431Z
sender:
@vinsystems:matrix.org
content:
* Interesting article Daniel 👌 after reading it I have a (maybe very basic) question: What kind of nodes interact with Jam? My understanding is:

Guarantors: nodes running "Jam".
Validators: nodes running "validator software".
Auditors: nodes running "auditor software".
Data Lake: Nodes running "data lake software"

id:
1282
timestamp:
2025-02-25T16:30:56.752Z
sender:
@sourabhniyogi:matrix.org
content:
* In order to compare large amounts of PVM traces between teams precisely, I would like a formula to hash the registers with the PVM paged memory for teams to know they ended up with same answer at the end, and if they did not, be able to quickly determine which line in some PVM trace of PC they differed in results.  

Its not hard to come up with a procedure, but does a ready made answer  exist within the PolkaVM repo or is there some public algorithm to do this kind of operation so we don't reinvent the wheel needlessly?   

id:
2683
timestamp:
2025-02-25T16:32:56.845Z
sender:
@vinsystems:matrix.org
content:
* Interesting article Daniel 👌 after reading it I have a (maybe very basic) question: What kind of nodes running different software interact with Jam? My understanding is:

Guarantors: nodes running "Jam".
Validators: nodes running "validator software".
Auditors: nodes running "auditor software".
Data Lake: Nodes running "data lake software"

id:
2682
timestamp:
2025-02-25T16:34:29.001Z
sender:
@danicuki:matrix.org
content:
I believe JAM is the amalgamation of all these roles. Most JAM implementers are creating nodes capable of running all this node roles in the same box. I don't know if it will be economically interesting for anyone run only partially

id:
1281
timestamp:
2025-02-25T16:53:15.195Z
sender:
@jaymansfield:matrix.org
content:
Hey! Hoping to get a clarification on the justifications for CE-138 (audit shard request). It mentions "The assurer should construct this by appending the corresponding segment shard root to the justification received via CE 137.". 

What is the segment shard root corresponding too exactly? 

The justification in CE 137 calls trace which returns a list of hashed values (original value is lost since it prefixes with 'node' and hashes)

id:
1280
timestamp:
2025-02-25T17:19:37.020Z
sender:
@jaymansfield:matrix.org
content:
* Hey! Hoping to get a clarification on the justifications for CE-138 (audit shard request). It mentions "The assurer should construct this by appending the corresponding segment shard root to the justification received via CE 137.". 

What is the segment shard root corresponding too exactly when it's a request about a work package shard?

id:
1279
timestamp:
2025-02-25T21:26:41.616Z
sender:
@sourabhniyogi:matrix.org
content:
The Parity Service trait definition for `accumulate` [here](https://docs.rs/jam-pvm-common/latest/jam_pvm_common/) returns an `Option<Hash>`

```
fn accumulate(_slot: Slot, _id: ServiceId, items: Vec<AccumulateItem>) -> Option<Hash>
```

but it appears there are TWO ways to provide a `Some`  for `accumulate`:

(1) if $\omega_8=32$, then the [B.12 ${\bf o} \in \mathbb{H}$ condition applies](https://graypaper.fluffylabs.dev/#/5f542d7/2ee2022ee202) 

(2) the [`yield` host function](https://graypaper.fluffylabs.dev/#/5f542d7/337902337902) 

As B.12 is written, (1) takes precedence over (2), but  the new (2) `yield` is a cleaner solution otherwise why was it added?   Now that its been added, I believe we don't need (1).  Having both is not *needed* since whatever 32-byte optional yield could go through output (1) OR (2), so perhaps we can eliminate (1).

Nitpick check: is $w8 = 32$ a sufficient criteria for (1) ?

id:
1278
timestamp:
2025-02-26T01:08:38.016Z
sender:
@sourabhniyogi:matrix.org
content:
* The Parity Service trait definition for `accumulate` [here](https://docs.rs/jam-pvm-common/latest/jam_pvm_common/) returns an `Option<Hash>`

```
fn accumulate(_slot: Slot, _id: ServiceId, items: Vec<AccumulateItem>) -> Option<Hash>
```

but it appears there are TWO ways to provide a `Some`  for `accumulate`:

(1) if $\\omega\_8=32$, then the [B.12 ${\\bf o} \\in \\mathbb{H}$ condition applies](https://graypaper.fluffylabs.dev/#/5f542d7/2ee2022ee202)

(2) the [`yield` host function](https://graypaper.fluffylabs.dev/#/5f542d7/337902337902)

As B.12 is written, (1) takes precedence over (2), but  the new (2) `yield` is a cleaner solution otherwise why was it added?   Now that its been added, I believe we don't need (1).  Having both is not _needed_ since whatever 32-byte optional yield could go through output (1) OR (2), so perhaps we can eliminate (1).

Nitpick check: is $omega8 = 32$ a sufficient criteria for (1) to take precedence over (2) ?

id:
1277
timestamp:
2025-02-26T01:53:47.228Z
sender:
@ascriv:matrix.org
content:
Should (A.43) have x’ instead of x? For clarity that it’s the x after the host call

id:
1276
timestamp:
2025-02-26T01:55:57.785Z
sender:
@ascriv:matrix.org
content:
And should the type of the gas in the return for (A.42) be signed (Zg) to handle e.g. when the host call returns out of gas?

id:
1275
timestamp:
2025-02-26T01:58:21.034Z
sender:
@sourabhniyogi:matrix.org
content:
* The Parity Service trait definition for `accumulate` [here](https://docs.rs/jam-pvm-common/latest/jam_pvm_common/) returns an `Option<Hash>`

```
fn accumulate(_slot: Slot, _id: ServiceId, items: Vec<AccumulateItem>) -> Option<Hash>
```

but it appears there are TWO ways to provide a `Some`  for `accumulate`:

(1) if $\\omega\_8=32$, then the [B.12 ${\\bf o} \\in \\mathbb{H}$ condition applies](https://graypaper.fluffylabs.dev/#/5f542d7/2ee2022ee202)

(2) the [`yield` host function](https://graypaper.fluffylabs.dev/#/5f542d7/337902337902)

As B.12 is written, (1) takes precedence over (2), but  the new (2) `yield` is a cleaner solution otherwise why was it added?   Now that its been added, I believe we don't need (1).  Having both is not _needed_ since whatever 32-byte optional yield could go through output (1) OR (2), so perhaps we can eliminate (1).

Nitpick check: is $omega8 = 32$ a sufficient criteria for (1) to take precedence over (2) ?  What if $omega8 > 32$?  What if $omega8 < 32$?  

id:
1274
timestamp:
2025-02-26T03:47:52.468Z
sender:
@sourabhniyogi:matrix.org
content:
* The Parity Service trait definition for accumulate here returns an Option<Hash>
fn accumulate(_slot: Slot, _id: ServiceId, items: Vec<AccumulateItem>) -> Option<Hash>
but it appears there are TWO ways to provide a Some for accumulate:
(1) if $\omega_8=32$, then the B.12 ${\bf o} \in \mathbb{H}$ condition applies
(2) the yield host function
As B.12 is written, (1) takes precedence over (2), but the new (2) yield is a cleaner solution otherwise why was it added? Now that its been added, I believe we don't need (1). Having both is not needed since whatever 32-byte optional yield could go through output (1) OR (2), so perhaps we can eliminate (1).
Nitpick check: is $omega8 = 32$ a sufficient criteria for (1) to take precedence over (2) ? What if $omega8 > 32$? What if $omega8 < 32$?

Related nitpick check: is there a way to change the C notation in eq 4.7 vs 4.17 to eliminate the appearance of dependency loops.  We are pretty sure  the C in 4.7 is from the previous states  4.17 but seek confirmation? 

id:
2681
timestamp:
2025-02-26T12:44:33.140Z
sender:
@gav:polkadot.io
content:
It certainly shouldn’t be - if it is then it’s a protocol flaw. 

id:
1273
timestamp:
2025-02-26T13:44:09.057Z
sender:
@jaymansfield:matrix.org
content:
Hey! Question about the state transition dependency graph 4.2.1. Should the calculation of β′ be moved further down since it depends on the commitment map C which doesn't exist yet, or does it use the commitment map from the previous block?

id:
1272
timestamp:
2025-02-26T13:54:16.343Z
sender:
@gav:polkadot.io
content:
> <@sourabhniyogi:matrix.org> In order to compare large amounts of PVM traces between teams precisely, I would like a formula to hash the registers with the PVM paged memory for teams to know they ended up with same answer at the end, and if they did not, be able to quickly determine which line in some PVM trace of PC they differed in results.  
> 
> Its not hard to come up with a procedure, but does a ready made answer  exist within the PolkaVM repo or is there some public algorithm to do this kind of operation so we don't reinvent the wheel needlessly?   

No, there’s no canonical PVM state serialization. Registers are trivial, but for memory we would need to have some definition on how to encode the pages and their accessibility. 

id:
1271
timestamp:
2025-02-26T14:02:17.357Z
sender:
@gav:polkadot.io
content:
> <@sourabhniyogi:matrix.org> The Parity Service trait definition for accumulate here returns an Option<Hash>
> fn accumulate(_slot: Slot, _id: ServiceId, items: Vec<AccumulateItem>) -> Option<Hash>
> but it appears there are TWO ways to provide a Some for accumulate:
> (1) if $\omega_8=32$, then the B.12 ${\bf o} \in \mathbb{H}$ condition applies
> (2) the yield host function
> As B.12 is written, (1) takes precedence over (2), but the new (2) yield is a cleaner solution otherwise why was it added? Now that its been added, I believe we don't need (1). Having both is not needed since whatever 32-byte optional yield could go through output (1) OR (2), so perhaps we can eliminate (1).
> Nitpick check: is $omega8 = 32$ a sufficient criteria for (1) to take precedence over (2) ? What if $omega8 > 32$? What if $omega8 < 32$?
> 
> Related nitpick check: is there a way to change the C notation in eq 4.7 vs 4.17 to eliminate the appearance of dependency loops.  We are pretty sure  the C in 4.7 is from the previous states  4.17 but seek confirmation? 

We can consider removing (1) at a later stage. Host calls are not especially cheap and returning data is a more natural pattern than relying on the side-effect of a host call. 

id:
2680
timestamp:
2025-02-26T14:12:35.207Z
sender:
@olanod:virto.community
content:
Thanks for the write up! As a service implementer that might not go through the details of the gray paper it helps to understand the different flows at a higher level. 
So Guarantors are the ones running the whole refine/accumulate pipeline in the code of the service the work package is referencing? or just refine ? it says guarantors run computation off-chain so no accumulate? sorry if it's stupid but I'm still trying to understand different concepts and draw a parallel with what I thought I understood from the refine/accumulate process. Is the off-chain code run by a Guarantor receiving a work package run only once? or does it need to be deterministic to be re-executed by other Guarantors or other actors?

id:
2679
timestamp:
2025-02-26T14:49:56.153Z
sender:
@gav:polkadot.io
content:
> <@olanod:virto.community> Thanks for the write up! As a service implementer that might not go through the details of the gray paper it helps to understand the different flows at a higher level. 
> So Guarantors are the ones running the whole refine/accumulate pipeline in the code of the service the work package is referencing? or just refine ? it says guarantors run computation off-chain so no accumulate? sorry if it's stupid but I'm still trying to understand different concepts and draw a parallel with what I thought I understood from the refine/accumulate process. Is the off-chain code run by a Guarantor receiving a work package run only once? or does it need to be deterministic to be re-executed by other Guarantors or other actors?

These are protocol-level distinctions. They are not service-provider-level distinctions. Validators must have the capability of being a block author, guarantor, assurer and auditor. They must not be separated. 

id:
2678
timestamp:
2025-02-26T15:53:13.532Z
sender:
@danicuki:matrix.org
content:
The off-chain (refine) code is executed by 2 other validators (3 in total: remember off-chain code runs only on cores and each core has 3 validators). 

id:
2677
timestamp:
2025-02-26T15:59:44.434Z
sender:
@gav:polkadot.io
content:
Actually executed by an average of about 32 validators. 

id:
2676
timestamp:
2025-02-26T16:00:00.540Z
sender:
@gav:polkadot.io
content:
Between guaranteeing and auditing. 

id:
2675
timestamp:
2025-02-26T16:16:49.214Z
sender:
@danicuki:matrix.org
content:
Does auditing run full execution? I understood that auditors perform a more selective verification rather than executing the full computation of every work package

id:
2674
timestamp:
2025-02-26T16:20:58.805Z
sender:
@dave:parity.io
content:
Guaranteeing and auditing do almost the same work, in particular they "fully execute" the refine function

id:
2673
timestamp:
2025-02-26T16:22:13.975Z
sender:
@dave:parity.io
content:
One difference between the two is that guarantors fetch imported segments from DA directly, whereas auditors fetch these as part of the work-package "bundle"

id:
2672
timestamp:
2025-02-26T16:23:49.669Z
sender:
@dave:parity.io
content:
And obviously guarantors receive the work-package and extrinsic data from builders or other guarantors on the same assignment, whereas auditors fetch this data from DA (though in the full network protocol there will likely be a "fast path" for fetching this data from the guarantors)

id:
2671
timestamp:
2025-02-26T16:30:23.899Z
sender:
@danicuki:matrix.org
content:
correcting: ~32 validators, including auditors: https://matrix.to/#/!wBOJlzaOULZOALhaRh:polkadot.io/$NZiNFgDQ172zoOlcyvrAZ4zyj1OW4DeDVfkV7UUOvDI?via=polkadot.io&via=matrix.org&via=parity.io

id:
2670
timestamp:
2025-02-26T17:32:13.373Z
sender:
@dave:parity.io
content:
I've made some comments. There seems to be some confusion as to how WP execution is verified. Not sure if there is a good summary of this anywhere.

id:
2669
timestamp:
2025-02-26T18:42:43.223Z
sender:
@jay_ztc:matrix.org
content:
Does it make sense to enforce static branch/jmp target validations during program initialization? Or should we assemble & execute regardless?

id:
2668
timestamp:
2025-02-26T18:44:51.514Z
sender:
@danicuki:matrix.org
content:
thank you so much! Will apply some corrections based on your comments.



id:
2667
timestamp:
2025-02-26T18:45:52.354Z
sender:
@jay_ztc:matrix.org
content:
I'm leaning on the side of stricter program initialization validations, assuming most pvm compilers will protect against generating this sort of flawed bytecode anyways. Curious to hear the groups thoughts though.

id:
2666
timestamp:
2025-02-26T18:46:43.663Z
sender:
@jay_ztc:matrix.org
content:
* I'm leaning on the side of stricter program initialization validations, assuming most pvm compilers will protect against generating this sort of flawed bytecode already. Curious to hear the groups thoughts.

id:
2665
timestamp:
2025-02-26T18:46:51.477Z
sender:
@jay_ztc:matrix.org
content:
* I'm leaning on the side of stricter program initialization validations, assuming most pvm compilers will protect against generating this sort of flawed bytecode. Curious to hear the groups thoughts.

id:
2664
timestamp:
2025-02-26T18:51:03.842Z
sender:
@jay_ztc:matrix.org
content:
better not to "burn" the gas executing parts/all of a program that we know has invalid behavior that might be triggered?

id:
2663
timestamp:
2025-02-26T18:52:01.381Z
sender:
@jay_ztc:matrix.org
content:
* better not to "burn" the gas executing the valid parts of a program that we know has some invalid instructions that might be triggered?

id:
1270
timestamp:
2025-02-26T19:16:21.735Z
sender:
@sourabhniyogi:matrix.org
content:
https://hackmd.io/@sourabhniyogi/pvmhash 
is a first try, hopefully a couple of us will try to converge on something as we get our host function implementations and PVM interpreter implementations correct.

I am wondering why R/W/A page accessibility came to your mind right away (as opposed to X/Y contexts which has most of the immediate debugging problems) -- I must  be missing something since any discrepancy in internal representations of page accessibility would be visible by some load/store instructions effect (or lack thereof) on a particular page -- encoding this page accessibility would be for 2 teams to reason about the contents of the memory after they saw the memory affected/not affected based on this bit.

id:
2662
timestamp:
2025-02-26T20:50:34.451Z
sender:
@xlchen:matrix.org
content:
I think it is unnecessary. people shouldn’t upload bad program at first place and if they did, well, why save cost for them with trade off of more overhead for everyone else?

id:
2661
timestamp:
2025-02-26T21:38:12.857Z
sender:
@jay_ztc:matrix.org
content:
I'm not sure it would be more overhead actually... If anything, I imagine it would be less overhead to validate at assemble-time. Validate once at assemble-time vs every time the instruction is called right?

id:
2660
timestamp:
2025-02-26T21:39:56.042Z
sender:
@dave:parity.io
content:
As long as behaviour is as specified in the GP it doesn't really matter how you implement it. Of course there will be minimum performance requirements, but if you think you can implement optimisations to go faster than this then great

id:
1269
timestamp:
2025-02-26T21:42:31.890Z
sender:
@sourabhniyogi:matrix.org
content:
* https://hackmd.io/@sourabhniyogi/pvmhash 
is a first try, hopefully a couple of us will try to converge on something as we get our host function implementations and PVM interpreter implementations correct.

I am wondering why R/W/0 page accessibility came to your mind right away (as opposed to X/Y contexts which has most of the immediate debugging problems) -- I must  be missing something since any discrepancy in internal representations of page accessibility would be visible by some load/store instructions effect (or lack thereof) on a particular page -- encoding this page accessibility would be for 2 teams to reason about the contents of the memory after they saw the memory affected/not affected based on R/W/0 page accessibility bits.

id:
2659
timestamp:
2025-02-26T21:44:10.017Z
sender:
@jay_ztc:matrix.org
content:
Throwing panic/error at assemble-time would be a different result than running the buggy bytecode right? I'm assuming the consensus would be different between these two approaches. Gas deducted comes to top of mind.

id:
1268
timestamp:
2025-02-26T21:48:28.082Z
sender:
@sourabhniyogi:matrix.org
content:
* https://hackmd.io/@sourabhniyogi/pvmhash 
is a first try, hopefully a couple of us will try to converge on something as we get our host function implementations and PVM interpreter implementations correct.

I am wondering why R/W/0 page accessibility came to your mind right away (as opposed to X/Y contexts which has most of the immediate debugging problems) -- I must  be missing something since any discrepancy in internal representations of page accessibility would be visible by some load/store instructions effect (or lack thereof) on a particular page -- encoding this page accessibility would be for 2 teams to reason about the contents of the memory after they saw the memory affected/not affected based on R/W/0 page accessibility bits -- if you anticipate this to be quite important early, I would like to put it in early in a "v1" (like in a page) -- is it?

Related question maybe?:  Was the W_G=4104 segment size chosen for segments to match a 4096-page size and a specific 8-byte encoding of the page, a page number and some specific metadata, specifically the accessibility bits.  If so, we might as well get the "dump a page" to map into the 4104 encoding imagined for CoreVM service?  Just a guess.  

id:
2658
timestamp:
2025-02-26T21:48:59.740Z
sender:
@dave:parity.io
content:
Sure, that just means you can't throw an error at assembly time. It doesn't mean you can't optimise the generated assembly according to this static analysis, for example omitting a runtime check if you have checked at assembly time.

id:
2657
timestamp:
2025-02-26T21:50:56.735Z
sender:
@dave:parity.io
content:
If you think the specification in the GP should be changed, then feel free to make your case in the Gray Paper channel

id:
2656
timestamp:
2025-02-26T21:51:10.337Z
sender:
@jay_ztc:matrix.org
content:
good point, I think the original question still stands though

id:
2655
timestamp:
2025-02-26T21:52:19.610Z
sender:
@jay_ztc:matrix.org
content:
Yep, good callout. Was thinking I'd get peoples thoughts here first, since this isn't explicitly addressed in the GP.

id:
2654
timestamp:
2025-02-26T21:53:22.006Z
sender:
@dave:parity.io
content:
The GP in general won't say how something should be implemented, it just defines the observable behaviour

id:
2653
timestamp:
2025-02-26T21:53:40.697Z
sender:
@dave:parity.io
content:
Are you saying the observable behaviour is not completely specified in some case?

id:
2652
timestamp:
2025-02-26T21:59:45.657Z
sender:
@jay_ztc:matrix.org
content:
this is what I'm looking to gather thoughts on. GP can be discussed in the GP channel separately

id:
2651
timestamp:
2025-02-26T22:42:27.788Z
sender:
@sourabhniyogi:matrix.org
content:
https://github.com/jam-duna/jamtestnet/actions/runs/13554662679

Super spiffy!  Hope we can get more matrix endpoints soon
https://github.com/jam-duna/jamtestnet/blob/main/.github/workflows/deploy.yaml#L14-L17

id:
1267
timestamp:
2025-02-27T01:03:00.902Z
sender:
@sourabhniyogi:matrix.org
content:
* https://hackmd.io/@sourabhniyogi/pvmhash 
is a first try, hopefully a couple of us will try to converge on something as we get our host function implementations and PVM interpreter implementations correct.

I am wondering why R/W/0 page accessibility came to your mind right away (as opposed to X/Y contexts which has most of the immediate debugging problems) -- I must  be missing something since any discrepancy in internal representations of page accessibility would be visible by some load/store instructions effect (or lack thereof) on a particular page -- encoding this page accessibility would be for 2 teams to reason about the contents of the memory after they saw the memory affected/not affected based on R/W/0 page accessibility bits -- if you anticipate this to be quite important early, I would like to put it in early in a "v1" (like in a page) -- is it?

Related question maybe?:  Was the W\_G=4104 segment size chosen for segments to match a 4096-page size and a specific 8-byte encoding of the page, a page number and some specific metadata, specifically the accessibility bits.  If so, we might as well get the "dump a page" to map into the 4104 encoding imagined for CoreVM service with the desired 8-byte encoding for pages maybe?  

id:
1266
timestamp:
2025-02-27T04:35:32.210Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> Should (A.43) have x’ instead of x? For clarity that it’s the x after the host call

Sure https://github.com/gavofyork/graypaper/pull/254

id:
1265
timestamp:
2025-02-27T04:57:16.580Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> And should the type of the gas in the return for (A.42) be signed (Zg) to handle e.g. when the host call returns out of gas?

Yes indeed: https://github.com/gavofyork/graypaper/pull/255

id:
1264
timestamp:
2025-02-27T04:59:25.049Z
sender:
@gav:polkadot.io
content:
The context is not PVM state.

id:
1263
timestamp:
2025-02-27T04:59:47.851Z
sender:
@gav:polkadot.io
content:
 * The context is not PVM state - the whole point is that it's external.

id:
1262
timestamp:
2025-02-27T05:00:32.657Z
sender:
@gav:polkadot.io
content:
Of course you'll likely still want to test it, but I don't think there's any reason to check it before the context is collapsed into the final result from Phi_R

id:
1261
timestamp:
2025-02-27T05:01:17.053Z
sender:
@gav:polkadot.io
content:
 * Of course you'll likely still want to test it, but I don't think there's any reason to check it before the context is collapsed into the final result from Phi\_A

id:
1260
timestamp:
2025-02-27T05:03:56.413Z
sender:
@gav:polkadot.io
content:
> <@jaymansfield:matrix.org> Hey! Question about the state transition dependency graph 4.2.1. Should the calculation of β′ be moved further down since it depends on the commitment map C which doesn't exist yet, or does it use the commitment map from the previous block?

I'm kept the (different variations of) the state components together rather than try to keep any "execution order". Indeed rather the point of this dependency graph is to demonstrate that there exists no specific order since it's a partially parallel rather than a fully serial system.

id:
1259
timestamp:
2025-02-27T05:09:39.914Z
sender:
@gav:polkadot.io
content:
> Nitpick check: is $omega8 = 32$ a sufficient criteria for (1) to take precedence over (2) ? What if $omega8 > 32$? What if $omega8 < 32$?

Not sure what you mean by $omega8, but assuming you mean selecting between that latter 2 variants of B.12, then it's pretty clear: you use the returned value IFF it is a 32-byte sequence (blackboard H). If it's anything other than this (e.g. 31 byte sequence or 33 byte sequence), then you fallback to the *otherwise* condition of using the (success) context.

id:
1258
timestamp:
2025-02-27T05:09:46.388Z
sender:
@gav:polkadot.io
content:
 * > Nitpick check: is $omega8 = 32$ a sufficient criteria for (1) to take precedence over (2) ? What if $omega8 > 32$? What if $omega8 \< 32$?

Not sure what you mean by $omega8, but assuming you mean selecting between that latter 2 variants of B.12, then it's pretty clear: you use the returned value IFF it is a 32-byte sequence (blackboard H). If it's anything other than this (e.g. 31 byte sequence or 33 byte sequence), then you fallback to the _otherwise_ condition of using the (success) context's yield.

id:
2650
timestamp:
2025-02-27T05:12:49.284Z
sender:
@gav:polkadot.io
content:
I think you might get a better response if you put forward a concrete change to the GP together with an argument for it.

id:
2649
timestamp:
2025-02-27T05:13:03.606Z
sender:
@gav:polkadot.io
content:
 * I think you might get a better response if you put forward a concrete proposal for change to the GP together with an argument for it.

id:
2648
timestamp:
2025-02-27T07:24:23.146Z
sender:
@jan:parity.io
content:
That's not how it works. A proper recompiler (and a well written non-naive interpreter) won't "validate every time the instruction is called"; it recompiles a piece of code once, and that's it.

There are roughly two strategies you can have:

1) Recompile the whole program ahead of time.
2) Recompile parts of the program just-in-time as they're executed.

Whether you reject programs with "invalid" bytecode (whichever way you define "invalid") has absolutely no bearing on whether you can do (1), but it prevents you from being able to do (2).

So while we're planning to essentially assume (1) in our gas cost model so far I've designed it so that the implementation has the flexibility to implement (2) and still be compliant (such an execution mode could be useful in certain cases, e.g. you have a huge program blob and you want to only execute a tiny chunk of it), and this means that you cannot require an O(n) pass to validate the program.

id:
2647
timestamp:
2025-02-27T07:56:11.784Z
sender:
@jay_ztc:matrix.org
content:
Thanks for sharing your thoughts in such a detailed response Jan Bujak . The design background provides great clarity as well. I had in mind something similar to the 2nd approach, but combined with jmp table validations that could be done using the bitmask- lighter weight than parsing out the whole program up front. But I missed that the invalid target resolutions could be done as part of the JIT design as well. I'm leaning towards leaving this flexibility as-is in the spec given your points, especially since this type of program bug should be something caught in the process of compiling into pvm bytecode anyways.

id:
2646
timestamp:
2025-02-27T07:56:59.554Z
sender:
@jay_ztc:matrix.org
content:
* Thanks for sharing your thoughts in such a detailed response Jan Bujak . The design background provides great clarity as well. I had in mind something similar to the 2nd approach, but combined with jmp table validations that could be done using the bitmask- lighter weight than parsing out the whole program up front. But I missed that the invalid target resolutions could be done as part of the JIT design as well. I'm leaning towards the opinion of leaving this flexibility as-is in the spec given your points, especially since this type of program bug should be something caught in the process of compiling into pvm bytecode anyways.

id:
2645
timestamp:
2025-02-27T08:10:03.612Z
sender:
@gav:polkadot.io
content:
> <@danicuki:matrix.org> Trying to understand the flow of information from end-user to blockchain state in JAM, I created this "step-by-step" doc. I am still not 100% sure it is accurate, so any feedback is very welcome:
> 
> https://hackmd.io/@w_7Gu9yjR4q8dsYR89m4OA/ryIwYLj5kl

Good that you're trying to get the word out and whatnot. I left some comments.

id:
1257
timestamp:
2025-02-27T08:20:38.747Z
sender:
@sourabhniyogi:matrix.org
content:
Here is the context of this "C" and "Beta" dependency question: 
https://github.com/jam-duna/jamtestnet/issues/101



id:
1256
timestamp:
2025-02-27T08:28:32.223Z
sender:
@dakkk:matrix.org
content:
> <@gav:polkadot.io> I'm kept the (different variations of) the state components together rather than try to keep any "execution order". Indeed rather the point of this dependency graph is to demonstrate that there exists no specific order since it's a partially parallel rather than a fully serial system.

sourabhniyogi: I think this answered your doubt. C used by beta' is the result from accumulation process 

id:
1255
timestamp:
2025-02-27T08:53:06.206Z
sender:
@sourabhniyogi:matrix.org
content:
I'd prefer we merge this:

https://github.com/gavofyork/graypaper/pull/253 

but we'll consider this case closed =)

id:
1254
timestamp:
2025-02-27T10:00:15.252Z
sender:
@sourabhniyogi:matrix.org
content:
* We'll consider this case closed =)

id:
1253
timestamp:
2025-02-27T10:59:06.252Z
sender:
@sourabhniyogi:matrix.org
content:
Then the v2 "hash" intends to capture the PVM state AND both contexts so as to support debugging of incorrect host function implementations.  

Since a Phi_A may have many host function calls, we do have a reason the check this v2 "hash".  Does that make sense?


id:
1252
timestamp:
2025-02-27T11:24:15.399Z
sender:
@sourabhniyogi:matrix.org
content:
* Then the v2 "hash" intends to capture the PVM state AND both contexts so as to support debugging of incorrect host function implementations.  

Since a Phi\_A may have many host function calls, we do have a reason to check this v2 "hash", to see if an intermediate value of the v2 "hash" from one implementation matches another, one of which is incorrect.  Does that make sense?


id:
1251
timestamp:
2025-02-27T11:28:38.382Z
sender:
@sourabhniyogi:matrix.org
content:
* Then the v2 "hash" intends to capture the PVM state AND both contexts so as to support debugging of incorrect host function implementations.

Since a Phi\_A may have many host function calls, we do have a reason to check this v2 "hash", to see if an intermediate value of the v2 "hash" from one implementation matches another after some of those host function calls [which affect the X (or Y) context] complete, one (or maybe both) of which is incorrect.  Does that make sense?

id:
1250
timestamp:
2025-02-27T11:58:58.949Z
sender:
@sourabhniyogi:matrix.org
content:
* Then the v2 "hash" intends to capture the PVM state AND both contexts so as to support debugging of incorrect host function implementations.  What should this be called?

Since a Phi\_A may have many host function calls, we do have a reason to check this v2 "hash", to see if an intermediate value of the v2 "hash" from one implementation matches another after some of those host function calls \[which affect the X (or Y) context\] complete, one (or maybe both) of which is incorrect.  Does that make sense?

id:
1249
timestamp:
2025-02-27T11:59:47.725Z
sender:
@sourabhniyogi:matrix.org
content:
* Then the v2 "hash" intends to capture the PVM state AND both contexts so as to support debugging of incorrect host function implementations.  What should this be called?

Since a Phi\_A result may have many host function calls to get at its result (with many intermediate X + Y contexts), we do have a reason to check this v2 "hash", to see if an intermediate value of the v2 "hash" from one implementation matches another after some of those host function calls \[which affect the X (or Y) context\] complete, one (or maybe both) of which is incorrect.  Does that make sense?

id:
2644
timestamp:
2025-02-27T19:24:48.498Z
sender:
@jay_ztc:matrix.org
content:
Jan Bujak: are you accepting PRs to your pvm testvec repo? I modified a few of the branch/jmp vectors to test for these edge cases that aren't currently covered by the suite.

id:
1248
timestamp:
2025-02-27T23:51:42.866Z
sender:
@gav:polkadot.io
content:
Sure. Then indeed you’ll want to serialise the context also. 

id:
2643
timestamp:
2025-02-28T00:03:19.034Z
sender:
@jay_ztc:matrix.org
content:
* Jan Bujak: are you accepting PRs to your pvm testvec repo? I modified a few of the existing branch/jmp vectors to create new tests for these edge cases that aren't currently covered by the suite.

id:
1247
timestamp:
2025-02-28T02:43:16.473Z
sender:
@ascriv:matrix.org
content:
I’m guessing in (B.1) p_c should be p_p?

id:
1246
timestamp:
2025-02-28T03:03:36.228Z
sender:
@qiwei:matrix.org
content:
see 14.9, there is a authorization code

id:
1245
timestamp:
2025-02-28T03:17:02.988Z
sender:
@ascriv:matrix.org
content:
Thanks. Should the returned gas value in (A.34) be signed? 

id:
1244
timestamp:
2025-02-28T03:36:14.047Z
sender:
@jay_ztc:matrix.org
content:
Jan Bujak: are you able to confirm if the rv test vectors are compliant with A.17? Based on my testing, I suspect they may be branching to the middle of basic blocks on passing tests. One example I found is the branch_eq opcode at pc 466 in rv64ui_add https://graypaper.fluffylabs.dev/#/5f542d7/24e40224e502

id:
1243
timestamp:
2025-02-28T03:39:06.140Z
sender:
@jay_ztc:matrix.org
content:
* Jan Bujak: are you able to confirm if the rv test vectors are compliant with A.17? Based on my testing, I suspect they may be branching to the middle of basic blocks on passing tests. One example I found is the branch\_eq instruction at pc 466 in rv64ui\_add https://graypaper.fluffylabs.dev/#/5f542d7/24e40224e502

id:
1242
timestamp:
2025-02-28T04:09:35.098Z
sender:
@jan:parity.io
content:
I'm confused. There's nothing wrong with the branch_eq instruction at pc 466 in rv64ui_add test and it certainly doesn't branch into the middle of a basic block?

```
   448: 01                       fallthrough
      :                          @20
   449: 33 00 0d                 r0 = 0xd
   452: 33 01 0b                 r1 = 0xb
   455: c8 10 0b                 r11 = r0 + r1
   458: 64 b3                    r3 = r11
   460: 95 aa 01                 r10 = r10 + 0x1
   463: 33 02 02                 r2 = 0x2
   466: ab 2a ef                 jump 449 if r10 != r2
```

id:
1241
timestamp:
2025-02-28T04:21:57.989Z
sender:
@jay_ztc:matrix.org
content:
You're right, this is my mistake... embarrassingly small bug on my end, should have reviewed more thoroughly before posting 🤦‍♂️ Thanks for your response.

id:
1240
timestamp:
2025-02-28T04:24:50.863Z
sender:
@jay_ztc:matrix.org
content:
* You're right, this is my mistake... embarrassingly small bug on my end, should have reviewed the target more thoroughly before posting as well (different debugging scope) 🤦‍♂️ Thanks for your response.

id:
1239
timestamp:
2025-02-28T08:27:25.912Z
sender:
@jay_ztc:matrix.org
content:
* You're right, this is my mistake... embarrassingly small bug on my end, should have reviewed the target more thoroughly before posting as well (different debugging scope & missed fallthrough is bb terminator) 🤦‍♂️ Thanks for your response.

id:
2642
timestamp:
2025-02-28T10:23:26.652Z
sender:
@clearloop:matrix.org
content:
ima_90c7a16.jpeg

id:
2641
timestamp:
2025-02-28T10:24:10.412Z
sender:
@clearloop:matrix.org
content:
Jam Tour Live in Shanghai 🎶

id:
1238
timestamp:
2025-02-28T10:30:31.770Z
sender:
@faiz_871:matrix.org
content:
I am struggling to understand the meaning of t_t, t_l and t_i in info host call function that is defined here: https://graypaper.fluffylabs.dev/#/5f542d7/306802308802

id:
1237
timestamp:
2025-02-28T10:44:20.622Z
sender:
@tomusdrw:matrix.org
content:
Faiz Ahmad: This should help: https://graypaper.fluffylabs.dev/#/5f542d7/115c01115c01 (`t_t & t_i`), https://graypaper.fluffylabs.dev/#/5f542d7/115400115700 (`t_l`)

id:
2640
timestamp:
2025-02-28T12:24:06.681Z
sender:
@sourabhniyogi:matrix.org
content:
https://github.com/jam-duna/jamtestnet/pull/109

is a first attempt at dumping out at the shape of some test vectors.  We need to get alignment on a erasure coding scheme that works for "tiny" V=6 network (and then "small" V=24) outside of Appendix H full V=1023 -- it would be awesome to get everyone's ideas on what is actually truly necessary to have multi-team tiny vs small testnet this Spring.

id:
2639
timestamp:
2025-02-28T13:27:48.239Z
sender:
@danicuki:matrix.org
content:
Hey Jammers, we just published a PVM workshop we recorded yesterday:

https://x.com/luke_fishman/status/1895409726898467236

PS: Everyone is welcome to publish videos in the JAM Community Youtube Channel: 
https://www.youtube.com/@PolkadotJAM

id:
2638
timestamp:
2025-02-28T15:08:57.257Z
sender:
@jay_ztc:matrix.org
content:
Is there a scenario where a validator might accept tickets as part of stage1 jamnp distribution, but selectively drop peer tickets in favor of their own? Would a rewards/cooling mechanic based on the "tickets submitted" activity stat come into play here? 

id:
2637
timestamp:
2025-02-28T15:11:13.811Z
sender:
@dave:parity.io
content:
Do you mean Safrole tickets? There is possibly scope for this happening, but each validator can only produce 2 tickets per epoch, and the VRF output must be "good enough" to be included in the accumulator, so not sure this scope is very large

id:
2636
timestamp:
2025-02-28T15:14:23.092Z
sender:
@jay_ztc:matrix.org
content:
yep, was wondering if it might be a possible scenario for validator to drop any peer tickets that might have higher priority than their own

id:
2635
timestamp:
2025-02-28T15:14:48.966Z
sender:
@jay_ztc:matrix.org
content:
* yep, was wondering if it might be a possible scenario for validator to drop any (or some percent of) peer tickets that might have higher priority than their own

id:
2634
timestamp:
2025-02-28T15:16:01.861Z
sender:
@jay_ztc:matrix.org
content:
* yep, was wondering if it might be a possible scenario for validator to drop any (or some percent of) peer tickets that have higher priority than their own

id:
2633
timestamp:
2025-02-28T15:16:44.948Z
sender:
@dave:parity.io
content:
They could plausibly include the lowest value tickets possible, but at some point some other validator will probably include the higher value tickets so not sure it ultimately makes much difference

id:
2632
timestamp:
2025-02-28T15:28:07.064Z
sender:
@dave:parity.io
content:
In the case where not enough tickets are accumulated, author selection falls back to a round-robin thing. This limits how much you can gain by censoring tickets

id:
2631
timestamp:
2025-02-28T15:30:11.016Z
sender:
@jay_ztc:matrix.org
content:
good callout on the fallback, yep. Probably some game theory equilibrium formula that's of relevance here... Doesn't sound like a big concern at present though. Thanks for sharing your thoughts.

id:
2630
timestamp:
2025-02-28T15:37:06.828Z
sender:
@dave:parity.io
content:
With 600-slot epochs, there could be an equilibrium where only 300 validators author blocks, and they refuse to include tickets for any other validators. Don't know if this is a legitimate concern or not

id:
2629
timestamp:
2025-02-28T15:38:43.568Z
sender:
@dave:parity.io
content:
300 is obviously a lot, but it's less than 1/3, which is the maximum number of byzantine nodes for many other bits of JAM

id:
2628
timestamp:
2025-02-28T15:38:55.712Z
sender:
@dave:parity.io
content:
* 300 is obviously a lot, but it's less than 1/3, which is the maximum number of byzantine validators for many other bits of JAM

id:
2627
timestamp:
2025-02-28T15:41:35.976Z
sender:
@dave:parity.io
content:
There also wouldn't be any real way of punishing this behaviour, outside of some governance thing

id:
2626
timestamp:
2025-02-28T15:47:55.303Z
sender:
@dave:parity.io
content:
Re the erasure coding scheme for "tiny": In PolkaJam segments are the same size with V=6 as with V=1023 (4104 bytes). With V=6 each segment is split into _2_ chunks of 2052 bytes (vs 342 chunks of 12 bytes with V=1023). Erasure coding produces an additional 4 chunks, giving 6 total (one per validator).

id:
2625
timestamp:
2025-02-28T15:48:22.305Z
sender:
@jay_ztc:matrix.org
content:
Sounds like a good use-case for a "flexible-simulated-local-test-net"; accelerate the protocol and spin up a mock test net to race through epochs as quick as possible and see if particular equilibriums are reached. Maybe a number of test nets in parallel and the results are aggregated/joined/interpreted in some way.Just thinking off top of head here... 

id:
2624
timestamp:
2025-02-28T15:48:43.259Z
sender:
@jay_ztc:matrix.org
content:
Probably some others here who have thought a bit about this already, in the context of the toaster 

id:
2623
timestamp:
2025-02-28T15:49:29.878Z
sender:
@prasad-kumkar:matrix.org
content:
maybe we could multiply the tickets by eta1' before scoring? to bring even more randomness

id:
2622
timestamp:
2025-02-28T15:49:39.500Z
sender:
@dave:parity.io
content:
Well this equilibrium will only happen with byzantine validators, which aren't including all the tickets they receive

id:
2621
timestamp:
2025-02-28T15:51:21.580Z
sender:
@dave:parity.io
content:
In general only including your own tickets would be a bad strategy as this would reduce your reward

id:
2620
timestamp:
2025-02-28T15:52:17.223Z
sender:
@dave:parity.io
content:
It's only a "good" strategy if you are colluding with many other validators to ensure you always get to author 2 blocks per epoch

id:
2619
timestamp:
2025-02-28T15:58:29.232Z
sender:
@dave:parity.io
content:
My point being this kind of equilibrium will _never_ occur in a testnet where we're only running honest validators

id:
2618
timestamp:
2025-02-28T16:00:18.048Z
sender:
@jay_ztc:matrix.org
content:
Yes. Worth calling out that is a pretty big assumption in that statement though-> as far as parity between testnet and "real world"

id:
2617
timestamp:
2025-02-28T16:02:30.388Z
sender:
@jay_ztc:matrix.org
content:
hence, my thoughts went to not a "true testnet with many parties", but rather a the "flexible-simulated-local-test-net" where behavior/equilibriums can be tested and reached quicker- given the input is "node configurations across validators" type of thing. Probably not putting it into text as cleanly as possible here...

id:
2616
timestamp:
2025-02-28T16:02:56.516Z
sender:
@gav:polkadot.io
content:
Even if 341 validators colluded and censored and didn’t include any tickets at all, that would only leave an expected 200 blocks per epoch without tickets. Even if those 200 tickets all fell in the first 500 blocks where tickets can be submitted, it leaves 300 normal blocks. With 8 tickets able to be introduced per block, there’s plenty enough room (8*300) for all the other legit validators to include all 600 winning tickets. 

id:
2615
timestamp:
2025-02-28T16:03:17.842Z
sender:
@gav:polkadot.io
content:
There’s plenty of slack here. 

id:
2614
timestamp:
2025-02-28T16:05:21.514Z
sender:
@dave:parity.io
content:
The point was if you are in a situation where there are 300 validators each producing 2 blocks in an epoch, they can just collude to include only their tickets for the next epoch, so no other validator ever gets a chance to author a block. Of course it might be practically impossible to get into this situation to start with

id:
2613
timestamp:
2025-02-28T16:06:27.672Z
sender:
@gav:polkadot.io
content:
Statistically that wouldn’t happen. 

id:
2612
timestamp:
2025-02-28T16:06:30.910Z
sender:
@gav:polkadot.io
content:
Ever. 

id:
2611
timestamp:
2025-02-28T16:06:41.964Z
sender:
@gav:polkadot.io
content:
* Statistically speaking that wouldn’t happen. 

id:
2610
timestamp:
2025-02-28T16:08:26.494Z
sender:
@gav:polkadot.io
content:
The chance of there being only 300 block producing validators in an epoch is small enough. The chance of them happening to be the same 300 colliding of the 1023 total is (300/1023)^300

id:
2609
timestamp:
2025-02-28T16:08:46.564Z
sender:
@gav:polkadot.io
content:
* The chance of there being only 300 block producing validators in an epoch is small enough. The chance of them happening to be the same 300 colliding of the 1023 total is (300/1023)^300 =~ 0

id:
2608
timestamp:
2025-02-28T16:09:28.285Z
sender:
@gav:polkadot.io
content:
* The chance of there being only 300 block producing validators in an epoch is small enough. The chance of them happening to be the same 300 colluding out of the 1023 total is something magnificently small like(300/1023)^300

id:
2607
timestamp:
2025-02-28T16:09:37.875Z
sender:
@gav:polkadot.io
content:
* The chance of there being only 300 block producing validators in an epoch is small enough. The chance of them happening to be the same 300 colluding out of the 1023 total is something magnificently small like (300/1023)^300

id:
2606
timestamp:
2025-02-28T16:10:29.615Z
sender:
@gav:polkadot.io
content:
It’s probably not quite that small (combinatorics being what it is) but I expect it’s vanishingly small regardless 

id:
2605
timestamp:
2025-02-28T16:11:45.580Z
sender:
@gav:polkadot.io
content:
And if you control literally every block in an epoch you could also block audits too. 

id:
2604
timestamp:
2025-02-28T16:11:52.623Z
sender:
@gav:polkadot.io
content:
So you’d have a lot more to worry about. 

id:
2603
timestamp:
2025-02-28T16:11:53.230Z
sender:
@dave:parity.io
content:
You probably don't need to start with only 300 dishonest validators authoring, the dishonest validators could eg just fork to exclude blocks authored by other validators. Maybe even then it's unlikely you could get a foothold, dunno

id:
2602
timestamp:
2025-02-28T16:12:23.331Z
sender:
@gav:polkadot.io
content:
You’d need to control over half of the slots. 

id:
2601
timestamp:
2025-02-28T16:12:28.261Z
sender:
@gav:polkadot.io
content:
Again, vanishingly small. 

id:
2600
timestamp:
2025-02-28T16:13:40.596Z
sender:
@gav:polkadot.io
content:
I expect the paper on Sassafras has this better modelled though. 

id:
2599
timestamp:
2025-02-28T16:14:12.319Z
sender:
@gav:polkadot.io
content:
Davide at the foundation can probably give a link for anyone interested 

id:
2598
timestamp:
2025-02-28T16:55:10.098Z
sender:
@sourabhniyogi:matrix.org
content:
thank you — is it reasonable to post a sample 4104 original and the 6 total so we can put it back together with the same parity ffi ?  If not we will do it based on your hint, we do enjoy the exercise.   

id:
2597
timestamp:
2025-02-28T17:07:11.580Z
sender:
@danicuki:matrix.org
content:
Updates about the JAM Experience event in Portugal 🇵🇹:

We’ve got approved budget for a modest event 💥🎉. It will happen at Parity’s office, with a closing party open to broad crypto community. 

The budget we have is for 40 pax, so if you plan to come, register asap here: https://lu.ma/ob0n7pdy

Dates: 6th and 7th May 

ETHLisbon is right after, 9-10th. Would be a plus to have you all there too, jamming along the hackathon. Subscriptions here: https://lu.ma/ETH-Lisbon-2025?tk=xlyRJ9


id:
2596
timestamp:
2025-02-28T17:45:15.869Z
sender:
@dave:parity.io
content:
* Re the erasure coding scheme for "tiny": In PolkaJam segments are the same size with V=6 as with V=1023 (4104 bytes). With V=6 each segment is split into _2_ shards of 2052 bytes (vs 342 shards of 12 bytes with V=1023). Erasure coding produces an additional 4 shards, giving 6 total (one per validator).

id:
1236
timestamp:
2025-02-28T19:09:46.343Z
sender:
@jay_ztc:matrix.org
content:
* Are alterations to the PC as a result of a host call required to be the start of a basic block? Not sure if I'm interpreting "call" appropriately in this sentence in the GP.
https://graypaper.fluffylabs.dev/#/5f542d7/24e40224e402 

If so, does reinvoking with the same pc of the host call (in the case of a host call page fault) imply a strict requirement that the instruction prior to the host call is a block terminator?
https://graypaper.fluffylabs.dev/#/5f542d7/2b23022b2302

id:
2595
timestamp:
2025-02-28T21:27:27.089Z
sender:
@mkchung:matrix.org
content:
https://graypaper.fluffylabs.dev/#/5f542d7/1b4c011b5701 

So in tiny setting matching shards of 2052 bytes, the C_6 here should be C_1026, correct? To generalize this, I think C_6 should perhaps be replaced with C_W_P? 

id:
2594
timestamp:
2025-02-28T21:43:22.537Z
sender:
@dave:parity.io
content:
Yes, I think I agree with both

id:
2593
timestamp:
2025-02-28T21:48:03.368Z
sender:
@dave:parity.io
content:
Though the definitions of C and C_k in the GP also assume V=1023 and need adjusting appropriately for the tiny config

id:
2592
timestamp:
2025-02-28T23:13:01.070Z
sender:
@sourabhniyogi:matrix.org
content:
image.png

id:
2591
timestamp:
2025-02-28T23:15:08.384Z
sender:
@sourabhniyogi:matrix.org
content:
Are all these 8 configurations reasonable looking ?

id:
2590
timestamp:
2025-02-28T23:19:51.294Z
sender:
@dave:parity.io
content:
Looks plausible to me. I think the main thing we need is that the size of a segment is the same across all configs, and obviously that total # shards = # validators

id:
2589
timestamp:
2025-02-28T23:36:28.485Z
sender:
@sourabhniyogi:matrix.org
content:
https://github.com/jam-duna/jamtestnet/issues/112

id:
2588
timestamp:
2025-03-01T02:27:49.205Z
sender:
@sourabhniyogi:matrix.org
content:
Adjusted https://docs.google.com/spreadsheets/d/1ueAisCMOx7B-m_fXMLT0FXBxfVzydJyr-udE8jKwDN8/edit?gid=723798716#gid=723798716 based on the idea that all 8 configurations from tiny to full should have the same W_G=4104 and be basically "corevm" friendly

id:
2587
timestamp:
2025-03-01T02:40:02.573Z
sender:
@sourabhniyogi:matrix.org
content:
* Adjusted https://docs.google.com/spreadsheets/d/1ueAisCMOx7B-m\_fXMLT0FXBxfVzydJyr-udE8jKwDN8/edit?gid=723798716#gid=723798716 based on the idea that all 8 configurations from tiny to full should have the same W\_G=4104 and be basically "corevm" friendly -- if anyone has wishes on what they should instead please share your thoughts!

id:
1235
timestamp:
2025-03-01T13:47:33.009Z
sender:
@sourabhniyogi:matrix.org
content:
Can a parent VM determine which PVM memory pages have been modified by an "invoked" child VM?

CoreVM-type services aim to extend JAM computation across multiple work packages by exporting PVM memory pages as segments at the end of a task and retrieving them at the start of the next.

While the [memcpy/memset host call](https://github.com/gavofyork/graypaper/issues/145) can support data transfer between parent and child VMs (an imported segment copied into some child VM page index), the parent needs a way to identify all mutated memory pages—not just those explicitly copied via memcpy—to ensure they are correctly included in the exported segments.

One approach could be extending [M](https://graypaper.fluffylabs.dev/#/5f542d7/2d22002d2200) to track modified page indexes and providing an explicit mechanism for the parent VM to access this set, supporting efficient page/segment export.

Is this a good idea or is there a better approach?

id:
2586
timestamp:
2025-03-01T15:53:02.030Z
sender:
@tushar_kumar:matrix.org
content:
can we know the approx date for milestone submissions?? 

id:
2585
timestamp:
2025-03-01T15:59:21.511Z
sender:
@ascriv:matrix.org
content:
Latest info as of a week ago ish is that v1.0 of the gp is estimate at end of Q3 of this year, but with a decent error margin

id:
2584
timestamp:
2025-03-01T15:59:52.291Z
sender:
@ascriv:matrix.org
content:
Milestone 1 submissions won’t be accepted until 1.0

id:
2583
timestamp:
2025-03-01T16:07:07.335Z
sender:
@tushar_kumar:matrix.org
content:
thanks for reply, do know from where can i be aware of latest updates on jam

id:
2582
timestamp:
2025-03-01T16:07:15.109Z
sender:
@tushar_kumar:matrix.org
content:
* thanks for reply, do you know from where can i be aware of latest updates on jam

id:
2581
timestamp:
2025-03-01T16:08:21.994Z
sender:
@tushar_kumar:matrix.org
content:
as you said Q3, its much time for v1.0

id:
2580
timestamp:
2025-03-01T16:08:53.034Z
sender:
@ascriv:matrix.org
content:
AFAIK the single source of truth is the gray paper and secondarily gav/ the gray paper chat

id:
2579
timestamp:
2025-03-01T16:09:56.337Z
sender:
@tushar_kumar:matrix.org
content:
thanks!

id:
2578
timestamp:
2025-03-01T16:19:06.078Z
sender:
@tushar_kumar:matrix.org
content:
wait, in the jam experience event description, its says the launch of GP v.1.0.https://lu.ma/ob0n7pdy. 

check it

id:
2577
timestamp:
2025-03-01T16:25:17.461Z
sender:
@ascriv:matrix.org
content:
Latest estimate is by end of Q3. But will depend a lot on the outcome of Toaster and initial service development. 

id:
2576
timestamp:
2025-03-01T16:26:19.883Z
sender:
@ascriv:matrix.org
content:
^quote from gav in the gray paper channel. Either they moved it up a lot since that message, or the jam experience event description is wrong, or I’m otherwise mistaken 

id:
2575
timestamp:
2025-03-01T16:27:07.954Z
sender:
@tushar_kumar:matrix.org
content:
can i get the link for gray paper channel

id:
2574
timestamp:
2025-03-01T16:30:09.347Z
sender:
@dakkk:matrix.org
content:
https://matrix.to/#/!ddsEwXlCWnreEGuqXZ:polkadot.io?via=matrix.org&via=parity.io&via=web3.foundation

id:
2573
timestamp:
2025-03-01T16:55:18.169Z
sender:
@tushar_kumar:matrix.org
content:
gav: will be great if you can clarify.

id:
2572
timestamp:
2025-03-01T19:06:56.296Z
sender:
@danicuki:matrix.org
content:
To avoid speculation, we removed GP1.0 from the JAM Experience Event description. Off-course we would love to have 1.0 ready by then, but this is just a wish. With 1.0 or not, the event will be awesome. Everybody is working hard, and milestone estimations don't help to accelerate things.

id:
1234
timestamp:
2025-03-02T04:27:19.712Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> Thanks. Should the returned gas value in (A.34) be signed?

Yes. Already merged.

id:
2571
timestamp:
2025-03-02T04:27:44.859Z
sender:
@gav:polkadot.io
content:
> <@danicuki:matrix.org> To avoid speculation, we removed GP1.0 from the JAM Experience Event description. Off-course we would love to have 1.0 ready by then, but this is just a wish. With 1.0 or not, the event will be awesome. Everybody is working hard, and milestone estimations don't help to accelerate things.

Indeed they do not.

id:
1233
timestamp:
2025-03-02T04:49:28.388Z
sender:
@ascriv:matrix.org
content:
A lot of the host functions (eg solicit, forget, yield, etc) use Z_o…+32 (for example). I think these can never be negative since they’re register values? So N is a bit more clear

id:
2570
timestamp:
2025-03-02T05:03:39.429Z
sender:
@gav:polkadot.io
content:
> <@tushar_kumar:matrix.org> gav: will be great if you can clarify.

I was going to prepare an implementors' update for JAMXP, but I may as well give the gist here.

The conditions for 0.9 are that we credibly expect to be able to run the Polkadot-compatible parachains service *and* CorePlay at a speed which our best modelling implies. This is an art and not a science - don't forget JAM is bleeding edge tech - so it will depend on the outcome of empirical experiments/analysis with the Toaster. I'd say this will probably (P>0.5) happen by EOQ3. The GP will need to be audited; this I expect (P>0.5) to take around 3 months and not to throw up too many issues. We may be able to audit implementations at the same time to shorten the TTL.

Regarding M1 submissions, my feeling is to get to 0.7 or 0.8, probably calling it in mid-April, and then open submissions for M1. Any milestone submission should be made for the most recent GP release at the time, this obviously includes submissions by teams who have already submitted for milestones on older GP versions.

Finally, I expect to introduce two additional paths for implementors to go down beyond the basic "validating node":

- Non-PVM-recompiler validating node: This implementation allows teams who do not wish to build a recompiler to shortcut to M5 and thus become viable network validator nodes. Basically such teams would be able to integrate with any PVM recompiler of their choice through FFI or native linking. This would come at the cost of only being able to claim **HALF** of each milestone prize as well as accordingly less of some other benefits. Such teams would have the possibility of eventually resubmitting M4 and M5 with their own recompiling PVM impl to claim the reset of the prizes/benefits.
- Light-client implementations; these would retain M1 (dumb full node) but their own M2-M5 milestones each worth half of the regular prize, giving overall rewards at 60% of the regular recompiling, validating full-node. Milestones 2, 3 and 4 would be based on resource usage (CPU, storage, memory and networking) as well as synchronisation time. No recompiler would be needed and resource usage expectations would likely be in line with Smoldot.

id:
2569
timestamp:
2025-03-02T05:04:33.637Z
sender:
@gav:polkadot.io
content:
> <@tushar_kumar:matrix.org> gav: will be great if you can clarify.

 * I was going to prepare an implementors' update for JAMXP, but I may as well give the gist here.

The conditions for 0.9 are that we credibly expect to be able to run the Polkadot-compatible parachains service _and_ CorePlay at a speed which our best modelling implies. This is an art and not a science - don't forget JAM is bleeding edge tech - so it will depend on the outcome of empirical experiments/analysis with the Toaster. I'd say this will probably (P>0.5) happen by EOQ3. The GP will need to be audited; this I expect (P>0.5) to take around 3 months and not to throw up too many issues. We may be able to audit implementations at the same time to shorten the TTL.

Regarding M1 submissions, my feeling is to get to 0.7 or 0.8, probably calling it in mid-April, and then open submissions for M1. Any milestone submission should be made for the most recent GP release at the time, this obviously includes submissions by teams who have already submitted for milestones on older GP versions.

Finally, I expect to introduce two additional paths for implementors to go down beyond the initial 5 milestones of the "validating node":

- Non-PVM-recompiler validating node: This implementation allows teams who do not wish to build a recompiler to shortcut to M5 and thus become viable network validator nodes. Basically such teams would be able to integrate with any PVM recompiler of their choice through FFI or native linking. This would come at the cost of only being able to claim **HALF** of each milestone prize as well as accordingly less of some other benefits. Such teams would have the possibility of eventually resubmitting M4 and M5 with their own recompiling PVM impl to claim the reset of the prizes/benefits.
- Light-client implementations; these would retain M1 (dumb full node) but their own M2-M5 milestones each worth half of the regular prize, giving overall rewards at 60% of the regular recompiling, validating full-node. Milestones 2, 3 and 4 would be based on resource usage (CPU, storage, memory and networking) as well as synchronisation time. No recompiler would be needed and resource usage expectations would likely be in line with Smoldot.

id:
2568
timestamp:
2025-03-02T05:06:20.551Z
sender:
@gav:polkadot.io
content:
None of this is decided yet and in particular we may need an incentive structure to avoid too many people switching to light-clients or getting too many low-power light-client implementations, but I do believe JAM will need a solid light-client community to be successful.

id:
2567
timestamp:
2025-03-02T05:07:15.268Z
sender:
@gav:polkadot.io
content:
 * I was going to prepare an implementors' update for JAMXP, but I may as well give the gist here.

The conditions for 0.9 are that we credibly expect to be able to run the Polkadot-compatible parachains service _and_ CorePlay at a speed which our best modelling implies. This is an art and not a science - don't forget JAM is bleeding edge tech - so it will depend on the outcome of empirical experiments/analysis with the Toaster. I'd say this will probably (P>0.5) happen by EOQ3. The GP will need to be audited; this I expect (P>0.5) to take around 3 months and not to throw up too many issues. We may be able to audit implementations at the same time to shorten the TTL.

Regarding M1 submissions, my feeling is to get to 0.7 or 0.8, probably calling it in mid-April, and then open submissions for M1. Any milestone submission should be made for the most recent GP release at the time, this obviously includes submissions by teams who have already submitted for milestones on older GP versions.

Finally, I expect to introduce two additional paths for implementors to go down beyond the initial 5 milestones of the "validating node":

- Non-PVM-recompiler validating node: This implementation allows teams who do not wish to build a recompiler to shortcut to M5 and thus become viable network validator nodes. Basically such teams would be able to integrate with any PVM recompiler of their choice through FFI or native linking. This would come at the cost of only being able to claim **HALF** of each milestone prize as well as accordingly less of some other benefits. Such teams would have the possibility of eventually resubmitting M4 and M5 with their own recompiling PVM impl to claim the reset of the prizes/benefits.
- Light-client implementations; these would retain M1 (dumb full node) but their own M2-M5 milestones each worth half of the regular prize, giving overall rewards at 60% of the regular recompiling, validating full-node. Milestones 2, 3 and 4 would be based on resource usage over time (CPU, storage, memory and networking) as well as synchronisation time from zero, partial synchronisation time and time to service a random basket of requests. No recompiler would be needed and resource usage expectations would likely be in line with Smoldot.

id:
1232
timestamp:
2025-03-02T05:08:00.676Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> A lot of the host functions (eg solicit, forget, yield, etc) use Z_o…+32 (for example). I think these can never be negative since they’re register values? So N is a bit more clear

Yes, feel free to make a PR.

id:
2566
timestamp:
2025-03-02T05:24:30.687Z
sender:
@gav:polkadot.io
content:
 * I was going to prepare an implementors' update for JAMXP, but I may as well give the gist here.

The conditions for 0.9 are that we credibly expect to be able to run the Polkadot-compatible parachains service _and_ CorePlay at a speed which our best modelling implies. This is an art and not a science - don't forget JAM is bleeding edge tech - so it will depend on the outcome of empirical experiments/analysis with the Toaster. I'd say this will probably (P>0.5) happen by EOQ3. The GP will need to be audited; this I expect (P>0.5) to take around 3 months and not to throw up too many issues. We may be able to audit implementations at the same time to shorten the TTL.

Regarding M1 submissions, my feeling is to get to 0.7 or 0.8, probably calling it in mid-April, and then open submissions for M1. Any milestone submission should be made for the most recent GP release at the time, this obviously includes submissions by teams who have already submitted for milestones on older GP versions.

Finally, I expect to introduce two additional paths for implementors to go down beyond the initial 5 milestones of the "validating node":

- Non-PVM-recompiler validating node: This implementation allows teams who do not wish to build a recompiler to shortcut to M5 and thus become viable network validator nodes. Basically such teams would be able to integrate with any PVM recompiler of their choice through FFI or native linking. This would come at the cost of only being able to claim **HALF** of each milestone prize as well as accordingly less of some other benefits. Such teams would have the possibility of eventually resubmitting M4 and M5 with their own recompiling PVM impl to claim the rest of the prizes/benefits.
- Light-client implementations; these would retain M1 (dumb full node) but their own M2-M5 milestones each worth half of the regular prize, giving overall rewards at 60% of the regular recompiling, validating full-node. Milestones 2, 3 and 4 would be based on resource usage over time (CPU, storage, memory and networking) as well as synchronisation time from zero, partial synchronisation time and time to service a random basket of requests. No recompiler would be needed and resource usage expectations would likely be in line with Smoldot.

id:
1231
timestamp:
2025-03-02T14:06:22.474Z
sender:
@ascriv:matrix.org
content:
https://github.com/gavofyork/graypaper/pull/262

id:
1230
timestamp:
2025-03-02T14:23:40.712Z
sender:
@ascriv:matrix.org
content:
Is there always an implicit mod 2^32 when inspecting or mutating the ram given a register (64 bit unsigned)? we do this a lot and wondering how it should be handled 

id:
1229
timestamp:
2025-03-02T16:57:09.774Z
sender:
@danicuki:matrix.org
content:
I have a doubt about the work package execution formula: https://graypaper.fluffylabs.dev/#/5f542d7/1a48021a5d02

it says that `I(p,j) = (r, e)` when `|e| = we`, but what if r is an error and the |e| = we? Shouldn't be

- `(r, [G0, G0, ...) if r not binary` first 
- `(r, e) if |e| = we` second 

?

id:
1228
timestamp:
2025-03-02T22:01:47.628Z
sender:
@ascriv:matrix.org
content:
According to the gp, we only handle memory access/modification exceptions in the single step function, but presumably we’d want to catch such exceptions in , e.g host calls as well, no?

id:
1227
timestamp:
2025-03-02T22:17:45.709Z
sender:
@jay_ztc:matrix.org
content:
believe mem fault in nested pvm is returned to parent pvm instance for the pvm program to handle, someone correct me if I'm wrong here

id:
1226
timestamp:
2025-03-02T22:21:09.533Z
sender:
@jay_ztc:matrix.org
content:
https://graypaper.fluffylabs.dev/#/5f542d7/364401364d01

id:
1225
timestamp:
2025-03-02T22:24:41.929Z
sender:
@ascriv:matrix.org
content:
Yes, but if for example the 8th register is not a valid memory location, then when deserializing to construct g, we should fault, right? Before the pvm function is called 

id:
1224
timestamp:
2025-03-02T22:25:08.361Z
sender:
@ascriv:matrix.org
content:
Similar for other host functions 

id:
1223
timestamp:
2025-03-02T22:31:40.499Z
sender:
@jay_ztc:matrix.org
content:
* Good question. I would assume write to that location from an 'outer-shell' implementation perspective, and if the pvm program attempts to access it, then it would trigger a page fault. Interested to hear other folks thoughts here.

id:
1222
timestamp:
2025-03-02T22:31:50.066Z
sender:
@jay_ztc:matrix.org
content:
* Good question. I would assume write to that location from an 'outer-shell' implementation perspective, and if the pvm program attempts to access it, then it would trigger a page fault. Interested to hear other folks thoughts on this though.

id:
2565
timestamp:
2025-03-03T01:04:21.124Z
sender:
@ascriv:matrix.org
content:
Is there a recording somewhere of gav’s taipei jam tour talk?

id:
2564
timestamp:
2025-03-03T01:29:24.412Z
sender:
@obnty:matrix.org
content:
It will be published today if there's no network issue. There has been a delay as the video crew has been in China after Taipei (we had to use mobile network with a vpn).

id:
1221
timestamp:
2025-03-03T02:45:45.006Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> Is there always an implicit mod 2^32 when inspecting or mutating the ram given a register (64 bit unsigned)? we do this a lot and wondering how it should be handled 

No. Everything is explicit.

id:
1220
timestamp:
2025-03-03T02:46:30.258Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> According to the gp, we only handle memory access/modification exceptions in the single step function, but presumably we’d want to catch such exceptions in , e.g host calls as well, no?

They are handled explicitly. 

id:
1219
timestamp:
2025-03-03T02:47:10.276Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> According to the gp, we only handle memory access/modification exceptions in the single step function, but presumably we’d want to catch such exceptions in , e.g host calls as well, no?

* They are handled explicitly. If you believe there is an instance where unarmed memory may be addressed, please report. 

id:
1218
timestamp:
2025-03-03T02:49:08.871Z
sender:
@gav:polkadot.io
content:
> <@danicuki:matrix.org> I have a doubt about the work package execution formula: https://graypaper.fluffylabs.dev/#/5f542d7/1a48021a5d02
> 
> it says that `I(p,j) = (r, e)` when `|e| = we`, but what if r is an error and the |e| = we? Shouldn't be
> 
> - `(r, [G0, G0, ...) if r not binary` first 
> - `(r, e) if |e| = we` second 
> 
> ?

No. GP is correct. 

id:
1217
timestamp:
2025-03-03T02:50:45.033Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> Yes, but if for example the 8th register is not a valid memory location, then when deserializing to construct g, we should fault, right? Before the pvm function is called 

See the second line there. The range beginning with o is ensured to be in the set of valid memory addresses. 

id:
1216
timestamp:
2025-03-03T04:53:15.953Z
sender:
@ascriv:matrix.org
content:
In export, since we’re reading indices p…+z wrapped, should we also be checking if Np…+z (mod ram size) is in Vu ?

id:
1215
timestamp:
2025-03-03T04:54:47.543Z
sender:
@ascriv:matrix.org
content:
Similar for poke 

id:
1214
timestamp:
2025-03-03T08:11:08.542Z
sender:
@0xjunha:matrix.org
content:
I have several questions/comments regarding the historical lookup and related constants:

1. The constant `D` has been updated from 28,800 slots (48 hrs) to 4,800 slots (8 hrs). I opened a PR to update it in appendix I too: https://github.com/gavofyork/graypaper/pull/260

2. Probably the constant value `L` should be reduced too? `D` seems to be introduced to prevent a preimage data from being removed while it still could be referenced during auditing. So `D` should be larger than `L`. However, current value of `L` is 14,400 (24 hrs): (https://graypaper.fluffylabs.dev/#/5f542d7/417000417000 and https://graypaper.fluffylabs.dev/#/5f542d7/0c9f000c9f00) which hasn't been updated since the initial commit, while `D` was updated as mentioned above.

3. https://graypaper.fluffylabs.dev/#/5f542d7/113b00113b00 Regarding the brief definition of the historical lookup function, should the constant `C_D` be `D` instead? I wonder if this is a typo or I'm missing something. Also, while the function is designed to be called off-chain, should we interpret the `H_t` here as "The timeslot index of the last finalized block header that an auditor sees at the point of auditing"?

id:
1213
timestamp:
2025-03-03T09:03:03.790Z
sender:
@gav:polkadot.io
content:
> <@0xjunha:matrix.org> I have several questions/comments regarding the historical lookup and related constants:
> 
> 1. The constant `D` has been updated from 28,800 slots (48 hrs) to 4,800 slots (8 hrs). I opened a PR to update it in appendix I too: https://github.com/gavofyork/graypaper/pull/260
> 
> 2. Probably the constant value `L` should be reduced too? `D` seems to be introduced to prevent a preimage data from being removed while it still could be referenced during auditing. So `D` should be larger than `L`. However, current value of `L` is 14,400 (24 hrs): (https://graypaper.fluffylabs.dev/#/5f542d7/417000417000 and https://graypaper.fluffylabs.dev/#/5f542d7/0c9f000c9f00) which hasn't been updated since the initial commit, while `D` was updated as mentioned above.
> 
> 3. https://graypaper.fluffylabs.dev/#/5f542d7/113b00113b00 Regarding the brief definition of the historical lookup function, should the constant `C_D` be `D` instead? I wonder if this is a typo or I'm missing something. Also, while the function is designed to be called off-chain, should we interpret the `H_t` here as "The timeslot index of the last finalized block header that an auditor sees at the point of auditing"?

1. Yes there's an issue for this now; the provided PR may not be quite right.

id:
1212
timestamp:
2025-03-03T09:04:26.919Z
sender:
@gav:polkadot.io
content:
 * 1./2. Yes there's an issue for this now; the provided PR may not be quite right.

id:
1211
timestamp:
2025-03-03T09:04:45.949Z
sender:
@gav:polkadot.io
content:
3. Ues indeed `C_D` should be `D`; will be fixed in 0.6.4.

id:
1210
timestamp:
2025-03-03T09:04:49.017Z
sender:
@gav:polkadot.io
content:
 * 3. Yes indeed `C_D` should be `D`; will be fixed in 0.6.4.

id:
1209
timestamp:
2025-03-03T09:06:09.681Z
sender:
@gav:polkadot.io
content:
One may consider 9.5 as the "on-chain" function-contract.

id:
1208
timestamp:
2025-03-03T09:06:43.136Z
sender:
@gav:polkadot.io
content:
Big-Lambda is defined fully at 9.7 and this proper definition does not use `H_t`.

id:
1207
timestamp:
2025-03-03T09:07:04.579Z
sender:
@gav:polkadot.io
content:
9.5 is only provided to help the read understand what problem the function is attempting to solve.

id:
1206
timestamp:
2025-03-03T09:07:28.951Z
sender:
@gav:polkadot.io
content:
@room Grap Paper version 0.6.3 is released: https://github.com/gavofyork/graypaper/releases/tag/v0.6.3

id:
1205
timestamp:
2025-03-03T09:07:32.898Z
sender:
@gav:polkadot.io
content:
 * @room Gray Paper version 0.6.3 is released: https://github.com/gavofyork/graypaper/releases/tag/v0.6.3

id:
1204
timestamp:
2025-03-03T09:10:12.786Z
sender:
@gav:polkadot.io
content:
In addition to many corrections and clarifications, there are several important functional alterations; pay attention to the first 6 items in the changelog.

id:
1203
timestamp:
2025-03-03T09:10:27.664Z
sender:
@gav:polkadot.io
content:
 * In addition to many corrections and clarifications, there are several small but important functional alterations; pay attention to the first 6 items in the changelog.

id:
1202
timestamp:
2025-03-03T09:10:37.568Z
sender:
@gav:polkadot.io
content:
 * 9.5 is only provided to help the reader understand what problem the function is attempting to solve.

id:
1201
timestamp:
2025-03-03T11:17:21.177Z
sender:
@yu2c:matrix.org
content:
Does anyone know why the PDF file for release v0.6.3 is 111 MB, while the previous version v0.6.2 was only 4.22 MB? 🧐

id:
1200
timestamp:
2025-03-03T11:43:41.306Z
sender:
@oliver.tale-yazdi:parity.io
content:
The alternative renders are small though https://github.com/JamBrains/graypaper/actions/runs/13629940636

id:
1199
timestamp:
2025-03-03T11:45:22.400Z
sender:
@oliver.tale-yazdi:parity.io
content:
* The alternative renders are small though https://github.com/JamBrains/graypaper/actions/runs/13629940636 (and if you are not logged into GH, [here](https://jamcha.in/spec))

id:
1198
timestamp:
2025-03-03T13:08:13.006Z
sender:
@yu2c:matrix.org
content:
Is the missing update of $l$ to $y$ in the serialization of $\mathbb{L}$?https://github.com/gavofyork/graypaper/blob/85129dacf739e76ead2065bb5b84a999e8ac71e7/text/serialization.tex#L143

id:
1197
timestamp:
2025-03-03T14:19:36.219Z
sender:
@gav:polkadot.io
content:
> <@yu2c:matrix.org> Does anyone know why the PDF file for release v0.6.3 is 111 MB, while the previous version v0.6.2 was only 4.22 MB? 🧐

Good point!:) I’ll look into getting a smaller rendering. 

id:
1196
timestamp:
2025-03-03T14:20:01.202Z
sender:
@gav:polkadot.io
content:
> <@yu2c:matrix.org> Is the missing update of $l$ to $y$ in the serialization of $\mathbb{L}$?https://github.com/gavofyork/graypaper/blob/85129dacf739e76ead2065bb5b84a999e8ac71e7/text/serialization.tex#L143

PR accepted!:)

id:
2563
timestamp:
2025-03-03T16:20:34.479Z
sender:
@decentration:matrix.org
content:
regarding this situation, where it is not an offender but the key cannot be decompressed. Do we just make it a padding point, but still continue to include it in the set?

in this specfic situation, in [safrole/tiny/enact-epoch-change-with-padding-1.json](https://github.com/davxy/jam-test-vectors/blob/469018009df0e8fa7e34b105f448e6ab2db60fe3/safrole/tiny/enact-epoch-change-with-padding-1.json)

the key:

```
                "bandersnatch": "0x1ecc3686b60ee3b84b6c7d321d70d5c06e9dac63a4d0a79d731b17c0d04d030d",

```

does not seem able to be decompressed, using `ark-ec-vrfs`. 



id:
1195
timestamp:
2025-03-03T16:58:42.458Z
sender:
@yu2c:matrix.org
content:
And small suggestion: Add $\mathbb{N}_{R}$ defined in [(4.23)](https://graypaper.fluffylabs.dev/#/85129da/0a13010a2001?v=0.6.3) in the Appendix I / Sets / Regular Notions

id:
1194
timestamp:
2025-03-03T17:35:05.943Z
sender:
@ascriv:matrix.org
content:
I believe we are not checking the right indices for memory validity in host functions that have wrapping in their memory inspecting/mutation:
http://github.com/gavofyork/graypaper/pull/272

id:
1193
timestamp:
2025-03-03T18:50:42.936Z
sender:
@danicuki:matrix.org
content:
I saw that formula 14.11 (https://graypaper.fluffylabs.dev/#/85129da/1a9b021aad02?v=0.6.3) on 0.6.3 now passes work item components to ΨR, but ΨR function signature didn't change (B.4). It still receives package (p) and item position (i). Where do args S(w,l), X(w) go now? (replaced old î) 

id:
1192
timestamp:
2025-03-03T19:34:41.267Z
sender:
@gav:polkadot.io
content:
> <@yu2c:matrix.org> And small suggestion: Add $\mathbb{N}_{R}$ defined in [(4.23)](https://graypaper.fluffylabs.dev/#/85129da/0a13010a2001?v=0.6.3) in the Appendix I / Sets / Regular Notions

PR?:)

id:
1191
timestamp:
2025-03-03T19:36:08.571Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> I believe we are not checking the right indices for memory validity in host functions that have wrapping in their memory inspecting/mutation:
> http://github.com/gavofyork/graypaper/pull/272

Is there any particular reason you think we should be more lenient?

id:
1190
timestamp:
2025-03-03T19:38:33.343Z
sender:
@gav:polkadot.io
content:
> <@danicuki:matrix.org> I saw that formula 14.11 (https://graypaper.fluffylabs.dev/#/85129da/1a9b021aad02?v=0.6.3) on 0.6.3 now passes work item components to ΨR, but ΨR function signature didn't change (B.4). It still receives package (p) and item position (i). Where do args S(w,l), X(w) go now? (replaced old î) 

Please explain?

id:
1189
timestamp:
2025-03-03T20:50:43.561Z
sender:
@danicuki:matrix.org
content:
ΨR call here passes 11 parameters: (wc,wg ,ws,h,wy ,px, pa,o,S(w,l),X(w),ℓ) (https://graypaper.fluffylabs.dev/#/85129da/1a9b021aad02?v=0.6.3)

But ΨR defined still has only 5 (i,p,o,i,ς): https://graypaper.fluffylabs.dev/#/85129da/2d65002d9300?v=0.6.3 

id:
1188
timestamp:
2025-03-03T21:12:04.559Z
sender:
@ascriv:matrix.org
content:
> <@gav:polkadot.io> Is there any particular reason you think we should be more lenient?

In poke for example, we read from the ram with wrapping , which means we want to allow for the case where z > ram size, in which case we wrap back to 0. But in that case, Ns…z will not be in Vu, so we will panic

id:
1187
timestamp:
2025-03-03T21:12:32.835Z
sender:
@ascriv:matrix.org
content:
If we don’t want to handle the case where z > ram size, then we are wrapping unnecessarily

id:
1186
timestamp:
2025-03-03T21:15:32.236Z
sender:
@ascriv:matrix.org
content:
*s+z

id:
2562
timestamp:
2025-03-04T01:13:21.152Z
sender:
@xlchen:matrix.org
content:
where can I find the jam tour recordings? if they are not yet uploaded, where will it be uploaded to?

id:
2561
timestamp:
2025-03-04T02:12:23.631Z
sender:
@wabkebab:matrix.org
content:
Jam implementors, technologist and Web3 enthusiast, here you can find the video of the JAM Tour, filmed during the lecture at Taipei University 

id:
2560
timestamp:
2025-03-04T02:12:38.704Z
sender:
@wabkebab:matrix.org
content:
https://www.youtube.com/watch?v=aTS4yjFsJd0

id:
1185
timestamp:
2025-03-04T02:13:40.037Z
sender:
@wabkebab:matrix.org
content:
Jam implementors, technologist and Web3 enthusiast, here you can find the video of the JAM Tour, filmed during the lecture at Taipei University

https://www.youtube.com/watch?v=aTS4yjFsJd0

id:
2559
timestamp:
2025-03-04T02:23:10.933Z
sender:
@wabkebab:matrix.org
content:
* Jam implementors, technologists and Web3 enthusiasts, here you can find the video of the JAM Tour, filmed during the lecture at Taipei University

id:
1184
timestamp:
2025-03-04T02:23:36.582Z
sender:
@wabkebab:matrix.org
content:
* Jam implementors, technologists and Web3 enthusiasts, here you can find the video of the JAM Tour, filmed during the lecture at Taipei University
https://www.youtube.com/watch?v=aTS4yjFsJd0

id:
1183
timestamp:
2025-03-04T02:37:22.558Z
sender:
@wabkebab:matrix.org
content:
* Jam implementors, technologists and Web3 enthusiasts, here you can find one of the videos of the JAM Tour, filmed during the lecture at Taipei University. More content coming! https://www.youtube.com/watch?v=aTS4yjFsJd0

id:
2558
timestamp:
2025-03-04T02:37:48.806Z
sender:
@wabkebab:matrix.org
content:
* Jam implementors, technologists and Web3 enthusiasts, here you can find one of the videos of the JAM Tour, filmed during the lecture at Taipei University. More content coming! https://www.youtube.com/watch?v=aTS4yjFsJd0 (editado)

id:
2557
timestamp:
2025-03-04T03:07:31.727Z
sender:
@wabkebab:matrix.org
content:
* Jam implementors, technologists and Web3 enthusiasts, here you can find one of the videos of the JAM Tour, filmed during the lecture at Taipei University. More content coming! (editado)

id:
2556
timestamp:
2025-03-04T03:08:14.384Z
sender:
@wabkebab:matrix.org
content:
* Jam implementors, technologists and Web3 enthusiasts, here you can find one of the videos of the JAM Tour, filmed during the lecture at Taipei University. More content coming!

id:
1182
timestamp:
2025-03-04T03:44:59.004Z
sender:
@sourabhniyogi:matrix.org
content:
In 0.5.4 there was extrinsic data blobs in $a$ of Refine invocation:
https://graypaper.fluffylabs.dev/#/579bd12/2d13012d1301?v=0.5.4
but in 0.6.x these extrinsic data blobs seem to have disappeared:
https://graypaper.fluffylabs.dev/#/78ca0a8/2df3002df300?v=0.6.0
There are no release notes for this change
https://github.com/gavofyork/graypaper/releases/tag/v0.6.0
While `fetch` enables access to the extrinsic blob hashes and their lengths of a work item, the extrinsic blobs are not in the work item:
https://graypaper.fluffylabs.dev/#/85129da/199f00199f00?v=0.6.3
So how does Refine fetch the extrinsic data blobs as of 0.6.x?

id:
1181
timestamp:
2025-03-04T05:26:41.413Z
sender:
@gav:polkadot.io
content:
> <@danicuki:matrix.org> ΨR call here passes 11 parameters: (wc,wg ,ws,h,wy ,px, pa,o,S(w,l),X(w),ℓ) (https://graypaper.fluffylabs.dev/#/85129da/1a9b021aad02?v=0.6.3)
> 
> But ΨR defined still has only 5 (i,p,o,i,ς): https://graypaper.fluffylabs.dev/#/85129da/2d65002d9300?v=0.6.3

Yes indeed: https://github.com/gavofyork/graypaper/pull/273

id:
1180
timestamp:
2025-03-04T05:29:16.961Z
sender:
@gav:polkadot.io
content:
> <@sourabhniyogi:matrix.org> In 0.5.4 there was extrinsic data blobs in $a$ of Refine invocation:
> https://graypaper.fluffylabs.dev/#/579bd12/2d13012d1301?v=0.5.4
> but in 0.6.x these extrinsic data blobs seem to have disappeared:
> https://graypaper.fluffylabs.dev/#/78ca0a8/2df3002df300?v=0.6.0
> There are no release notes for this change
> https://github.com/gavofyork/graypaper/releases/tag/v0.6.0
> While `fetch` enables access to the extrinsic blob hashes and their lengths of a work item, the extrinsic blobs are not in the work item:
> https://graypaper.fluffylabs.dev/#/85129da/199f00199f00?v=0.6.3
> So how does Refine fetch the extrinsic data blobs as of 0.6.x?

As I've said countless times now, the GP does not dictate the specifics of data logistics, only observable behaviour. `fetch` requires implementations to return the correct extrinsic data by virtue of the constraints placed on the return value from `fetch`. How it gets the extrinsic data is entirely implementation-specific and left as an exercise for the reader. Probably it will be supplied along with the rest of the WP by the builder, but the GP remains ambivalent about this.

id:
1179
timestamp:
2025-03-04T05:29:43.935Z
sender:
@gav:polkadot.io
content:
 * As I've said countless times now, the GP does not dictate the specifics of data logistics, only observable behaviour. `fetch` requires implementations to return the correct extrinsic data by virtue of the constraints placed on the return value from `fetch`. How it gets the extrinsic data is entirely implementation-specific and left as an exercise for the reader. Probably it will be supplied along with the rest of the WP by the builder, but the GP does not define this since it is not *observable behaviour*.

id:
1178
timestamp:
2025-03-04T05:32:23.502Z
sender:
@gav:polkadot.io
content:
Your question betrays an presupposition that the formalisms in the GP are 1:1 mappable to some implementation code. That may sometimes be the case but certainly not always.

id:
1177
timestamp:
2025-03-04T05:32:30.626Z
sender:
@gav:polkadot.io
content:
 * Your question betrays a presupposition that the formalisms in the GP are 1:1 mappable to some implementation code. That may sometimes be the case but certainly not always.

id:
1176
timestamp:
2025-03-04T05:33:36.804Z
sender:
@gav:polkadot.io
content:
We are able to use formalisms in mathematics such as `let H(return_value) = input_value`

id:
1175
timestamp:
2025-03-04T05:34:02.118Z
sender:
@gav:polkadot.io
content:
 * e.g. We are able to use formalisms in mathematics such as `let H(return_value) = input_value`. This does not map 1:1 with (procedural) code.

id:
1174
timestamp:
2025-03-04T05:36:18.193Z
sender:
@gav:polkadot.io
content:
 * e.g. We are able to use formalisms in mathematics such as `let H(return_value) = input_value`. This does not map 1:1 with (procedural) code, because it would imply the ability to make a reverse hash, for which no general solution is known.

id:
1173
timestamp:
2025-03-04T05:36:18.616Z
sender:
@gav:polkadot.io
content:
It works in the GP because it is describing *what* we wish to see, not *how* it must be delivered.

id:
1172
timestamp:
2025-03-04T05:36:44.786Z
sender:
@gav:polkadot.io
content:
In the case of implementing the "impossible" reverse hash function, it is fine because implementations are allowed to "cheat" and use external knowledge (such as DA contents or data arriving over the network) in order to arrive at the answer.

id:
1171
timestamp:
2025-03-04T05:40:08.316Z
sender:
@gav:polkadot.io
content:
Implementing the GP is a puzzle and intentionally so. There may be different ways of solving the puzzle. This diversity can help deliver a resilient, even anti-fragile, network. Don't expect a perfectly described path to implementation. You'll need to use your brain.

id:
1170
timestamp:
2025-03-04T05:41:44.907Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> In poke for example, we read from the ram with wrapping , which means we want to allow for the case where z > ram size, in which case we wrap back to 0. But in that case, Ns…z will not be in Vu, so we will panic

Ahh, in fact the wrapping is superfluous.

id:
1169
timestamp:
2025-03-04T05:41:52.002Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> In poke for example, we read from the ram with wrapping , which means we want to allow for the case where z > ram size, in which case we wrap back to 0. But in that case, Ns…z will not be in Vu, so we will panic

 * Ahh, in fact the wrapping is superfluous there.

id:
1168
timestamp:
2025-03-04T05:42:36.822Z
sender:
@gav:polkadot.io
content:
 * Ahh, in fact the wrapping is superfluous there; I'll accept a PR which removes it from the host functions.

id:
1167
timestamp:
2025-03-04T05:43:32.976Z
sender:
@gav:polkadot.io
content:
> <@yu2c:matrix.org> And small suggestion: Add $\mathbb{N}_{R}$ defined in [(4.23)](https://graypaper.fluffylabs.dev/#/85129da/0a13010a2001?v=0.6.3) in the Appendix I / Sets / Regular Notions

PR welcome:)

id:
1166
timestamp:
2025-03-04T06:06:20.775Z
sender:
@yu2c:matrix.org
content:
https://github.com/gavofyork/graypaper/pull/274 This PR adds the definition of $\N_R$ in Appendix I / Sets / Regular Notion.

id:
1165
timestamp:
2025-03-04T09:19:24.413Z
sender:
@clearloop:matrix.org
content:
hi there, I'm currently get confused about the fallback keys (sealing) in grandpa:

while blocks sealed with fallback keys are for the placeholder of contingency, https://graypaper.fluffylabs.dev/#/85129da/1fc0001fc400?v=0.6.3 seems mean that headers with fallback keys are not eligible to be selected as best header, if I'm not mistaken, the blocks with fallback keys should be selected as the best headers if there are no ticket sealed blocks at the end of slots, plz correct me if I'm wrong 🙏

id:
1164
timestamp:
2025-03-04T09:19:51.609Z
sender:
@clearloop:matrix.org
content:
* hi there, I'm currently get confused about the fallback keys (sealing) in grandpa:

while blocks sealed with fallback keys are for the placeholder of contingency, https://graypaper.fluffylabs.dev/#/85129da/1fc0001fc400?v=0.6.3 seems mean that headers with fallback keys are not eligible to be selected as best headers, if I'm not mistaken, the blocks with fallback keys should be selected as the best headers if there are no ticket sealed blocks at the end of slots, plz correct me if I'm wrong 🙏

id:
1163
timestamp:
2025-03-04T09:25:50.034Z
sender:
@clearloop:matrix.org
content:
* hi there, I'm currently confused about the fallback keys (sealing) in grandpa:

while blocks sealed with fallback keys are for the placeholder of contingency, https://graypaper.fluffylabs.dev/#/85129da/1fc0001fc400?v=0.6.3 seems mean that headers with fallback keys are not eligible to be selected as best headers, if I'm not mistaken, the blocks with fallback keys should be selected as the best headers if there are no ticket sealed blocks at the end of slots, plz correct me if I'm wrong 🙏

id:
1162
timestamp:
2025-03-04T09:26:57.550Z
sender:
@clearloop:matrix.org
content:
* hi there, I'm currently confused about the fallback keys (sealing) in grandpa:

while blocks sealed with fallback keys are for the placeholder of contingency, https://graypaper.fluffylabs.dev/#/85129da/1fc0001fc400?v=0.6.3 seems mean that headers with fallback keys are not eligible to be selected as best headers which will make the blocks with fallback keys meaningless, if I'm not mistaken, the blocks with fallback keys should be selected as the best headers if there are no ticket sealed blocks at the end of slots, plz correct me if I'm wrong 🙏

id:
2555
timestamp:
2025-03-04T12:11:28.474Z
sender:
@ultracoconut:matrix.org
content:
Hello guys what do you think about the idea of ​​a Jam OS similar to Eliza OS but for smart contracts and services?🤔

id:
2554
timestamp:
2025-03-04T13:18:54.177Z
sender:
@gav:polkadot.io
content:
Maybe it could use JAM/CorePlay directly. 

id:
1161
timestamp:
2025-03-04T13:20:31.262Z
sender:
@gav:polkadot.io
content:
Such blocks may be the only way of extending the chain and getting to the point of having blocks which increase the best chain score. 

id:
1160
timestamp:
2025-03-04T13:21:58.030Z
sender:
@gav:polkadot.io
content:
* Such blocks may be the only way of extending the chain and getting to the point of having regular ticketed blocks which increase the best chain score. 

id:
2553
timestamp:
2025-03-04T16:48:16.760Z
sender:
@p1sar:matrix.org
content:
On luma, it says the JAM event is 7-8 

id:
2552
timestamp:
2025-03-04T17:39:40.300Z
sender:
@jaymansfield:matrix.org
content:
If any other JAM teams are interested in erasure coding vectors I've published a few. There are just 4 right now which cover JAM's basic requirements. It includes encoding a work package at both the tiny and full chain specs, and encoding a segment at both tiny and full chain specs. Once at least one other team can confirm they are working correctly I will generate a bunch more to have a larger suite. https://github.com/javajamio/javajam-trace/tree/main/erasure_coding

id:
1159
timestamp:
2025-03-04T21:43:55.841Z
sender:
@ascriv:matrix.org
content:
> <@gav:polkadot.io> Ahh, in fact the wrapping is superfluous.

http://github.com/gavofyork/graypaper/pull/272

id:
1158
timestamp:
2025-03-05T01:34:38.725Z
sender:
@ascriv:matrix.org
content:
In New, should we panic if the 8th register is not in N-2^32? Because then (c,l) will not be a valid key for the l component of the new service account 

id:
1157
timestamp:
2025-03-05T02:40:26.315Z
sender:
@clearloop:matrix.org
content:
so the case of finalizing headers with fallback keys is: we just finalized a ticketed blocks, and we are requesting the ancestors of the newly finalized ticketed blocks ( blocks with the fallback keys in the header could be here ) ?

id:
1156
timestamp:
2025-03-05T02:40:37.497Z
sender:
@clearloop:matrix.org
content:
* so the case of finalizing headers with fallback keys is: we just finalized a ticketed block, and we are requesting the ancestors of the newly finalized ticketed blocks ( blocks with the fallback keys in the header could be here ) ?

id:
1155
timestamp:
2025-03-05T02:40:52.157Z
sender:
@clearloop:matrix.org
content:
* so the case of finalizing headers with fallback keys is: we just finalized a ticketed block, and we are requesting the ancestors of the newly finalized ticketed blocks ( blocks with the fallback keys in the headers could be here ) ?

id:
1154
timestamp:
2025-03-05T02:42:46.874Z
sender:
@clearloop:matrix.org
content:
* so the case of finalizing headers with fallback keys is: we just finalized a ticketed block, and we are requesting the ancestors of the newly finalized tickete blocks ( blocks with the fallback keys in the headers could be here ) ?

id:
1153
timestamp:
2025-03-05T02:43:28.355Z
sender:
@clearloop:matrix.org
content:
* so the case of finalizing headers with fallback keys is: we just finalized a ticketed block, and we are requesting the ancestors of the newly finalized ticketed blocks ( blocks with the fallback keys in the headers could be here ) ?

id:
1152
timestamp:
2025-03-05T02:44:57.190Z
sender:
@ascriv:matrix.org
content:
> <@ascriv:matrix.org> In New, should we panic if the 8th register is not in N-2^32? Because then (c,l) will not be a valid key for the l component of the new service account 

Also in new, a is missing the p component which should presumably be {}

id:
1151
timestamp:
2025-03-05T02:53:34.882Z
sender:
@clearloop:matrix.org
content:
* so the case of finalizing headers with fallback keys is: we just confirmed a ticketed block is on the best chain, and we are requesting the ancestors of the newly finalized ticketed blocks ( blocks with the fallback keys in the headers could be here ) ?

id:
1150
timestamp:
2025-03-05T02:53:48.848Z
sender:
@clearloop:matrix.org
content:
* so the case of finalizing headers with fallback keys is: we just confirmed a ticketed block is on the best chain, and we are requesting the ancestors of the best head ( blocks with the fallback keys in the headers could be here ) ?

id:
1149
timestamp:
2025-03-05T02:58:32.995Z
sender:
@ascriv:matrix.org
content:
http://github.com/gavofyork/graypaper/pull/275

id:
2551
timestamp:
2025-03-05T10:41:57.474Z
sender:
@decentration:matrix.org
content:
* davxy: regarding this situation, where it is not an offender but the key cannot be decompressed. Do we just make it a padding point, but still continue to include it in the set?

in this specfic situation, in [safrole/tiny/enact-epoch-change-with-padding-1.json](https://github.com/davxy/jam-test-vectors/blob/469018009df0e8fa7e34b105f448e6ab2db60fe3/safrole/tiny/enact-epoch-change-with-padding-1.json)

the key:

```
                "bandersnatch": "0x1ecc3686b60ee3b84b6c7d321d70d5c06e9dac63a4d0a79d731b17c0d04d030d",

```

does not seem able to be decompressed, using `ark-ec-vrfs`.

id:
2550
timestamp:
2025-03-05T11:11:56.939Z
sender:
@decentration:matrix.org
content:
ah yes i see here are the padding points:

https://github.com/davxy/bandersnatch-vrfs-spec/blob/8744fd26ba7415fcc447e178c715a289180c209e/specification.tex#L593


id:
2549
timestamp:
2025-03-05T11:28:45.220Z
sender:
@decentration:matrix.org
content:
* ah yes the updated padding points in this thread. 



id:
2548
timestamp:
2025-03-05T11:28:51.071Z
sender:
@decentration:matrix.org
content:
* regarding this situation, where it is not an offender but the key cannot be decompressed. Do we just make it a padding point, but still continue to include it in the set?

in this specfic situation, in [safrole/tiny/enact-epoch-change-with-padding-1.json](https://github.com/davxy/jam-test-vectors/blob/469018009df0e8fa7e34b105f448e6ab2db60fe3/safrole/tiny/enact-epoch-change-with-padding-1.json)

the key:

```
                "bandersnatch": "0x1ecc3686b60ee3b84b6c7d321d70d5c06e9dac63a4d0a79d731b17c0d04d030d",

```

does not seem able to be decompressed, using `ark-ec-vrfs`.

id:
2547
timestamp:
2025-03-05T14:57:29.999Z
sender:
@davxy:matrix.org
content:
STF test vectors updated to GP 0.6.3 
https://github.com/davxy/jam-test-vectors

id:
1148
timestamp:
2025-03-05T16:11:53.516Z
sender:
@clearloop:matrix.org
content:
* ~~so the case of finalizing headers with fallback keys is: we just confirmed a ticketed block is on the best chain, and we are requesting the ancestors of the best head ( blocks with the fallback keys in the headers could be here ) ?~~



id:
1147
timestamp:
2025-03-05T16:12:47.126Z
sender:
@clearloop:matrix.org
content:
* ~~so the case of finalizing headers with fallback keys is: we just confirmed a ticketed block is on the best chain, and we are requesting the ancestors of the best head ( blocks with the fallback keys in the headers could be here ) ?~~

clear about it now, best chain voted by blocks with most ancestor blocks, headers with fallback keys could be part of them

id:
2546
timestamp:
2025-03-05T18:10:01.783Z
sender:
@davxy:matrix.org
content:
A couple of test vectors for single segment erasure coding.

https://github.com/davxy/jam-test-vectors/pull/28

id:
1146
timestamp:
2025-03-06T05:05:53.208Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> In New, should we panic if the 8th register is not in N-2^32? Because then (c,l) will not be a valid key for the l component of the new service account 

Well, it’s not that it’s not “valid” (don’t forget we’re not using typed logic here, just basic set formalisms). Rather that there can be early certainty that if l is not in N_2^32, then there can certainly be no keys (c, l) in the set (H, N_2^32).

id:
1145
timestamp:
2025-03-06T05:06:00.885Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> In New, should we panic if the 8th register is not in N-2^32? Because then (c,l) will not be a valid key for the l component of the new service account 

* Well, it’s not that it’s not “valid” (don’t forget we’re not using typed logic here, just basic set formalisms). Rather that there can be early certainty that if l is not in N_2^32, then there can certainly be no item (c, l) in the set (H, N_2^32).

id:
1144
timestamp:
2025-03-06T05:06:14.237Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> In New, should we panic if the 8th register is not in N-2^32? Because then (c,l) will not be a valid key for the l component of the new service account 

* Well, it’s not that it’s not “valid” (don’t forget we’re not using typed logic here, just basic set formalisms). Rather that there can be early certainty that if l is not in N_2^32, then there can be no item (c, l) in the set (H, N_2^32).

id:
1143
timestamp:
2025-03-06T05:06:49.916Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> In New, should we panic if the 8th register is not in N-2^32? Because then (c,l) will not be a valid key for the l component of the new service account 

* Well, it’s not that it’s not “valid” (don’t forget we’re not using typed logic here, just basic set formalisms). Rather that there can be early certainty that if l is not in N_2^32, then there can be no item (c, l) in the set (H, N_2^32). This implies no key can exist. 

id:
2545
timestamp:
2025-03-06T07:37:37.738Z
sender:
@gav:polkadot.io
content:
Phase 1 is about 70% complete..

id:
2544
timestamp:
2025-03-06T07:37:38.217Z
sender:
@gav:polkadot.io
content:
signal-2025-03-05-042811_002.mp4

id:
2543
timestamp:
2025-03-06T07:38:37.319Z
sender:
@gav:polkadot.io
content:
image.png

id:
1142
timestamp:
2025-03-06T15:24:02.639Z
sender:
@dakkk:matrix.org
content:
gav: in your latest lecture in Taipei about JAM, you said that your implementation achieved ~**% speed of native code; would it possible to have the program you used for this benchmark? I'd love to test jampy PVM around different improvements I have in my sleeve

id:
1141
timestamp:
2025-03-06T15:25:13.558Z
sender:
@jan:parity.io
content:
https://github.com/paritytech/polkavm/blob/master/BENCHMARKS.md

id:
1140
timestamp:
2025-03-06T15:25:17.526Z
sender:
@jan:parity.io
content:
https://github.com/paritytech/polkavm/tree/master/guest-programs

id:
1139
timestamp:
2025-03-06T15:27:48.263Z
sender:
@dakkk:matrix.org
content:
thx Jan

id:
1138
timestamp:
2025-03-06T16:00:54.517Z
sender:
@dakkk:matrix.org
content:
Jan Bujak: I'm calling `bash guest-programs/build-benchmarks.sh` in order to create pvm and native binaries: it correctly creates PVM binaries, but the x86_64 contains only shared objects. Am I missing a step? 

id:
1137
timestamp:
2025-03-06T16:01:20.930Z
sender:
@jan:parity.io
content:
tools/benchtool is used to run those

id:
2542
timestamp:
2025-03-06T17:18:06.471Z
sender:
@sourabhniyogi:matrix.org
content:
Is it reasonable to have the package versioning details eg https://docs.rs/reed-solomon-simd/3.0.1/reed_solomon_simd/ and a snippet of the test vector process generation at least for this 4104 tiny case -- with Rule 1, similar to bls + bandersnatch FFIing into w3f libraries, most implementers will likely do FFI into the exact package version.  This erasure coding test vector would then stand as a check of correct FFI behavior rather than some kind of interpretation of GP Appendix H problem to solve.

id:
1136
timestamp:
2025-03-06T17:28:50.975Z
sender:
@dakkk:matrix.org
content:
I'm giving up since generated files are elf or .polkavm format, and I do not want to read other implementation's code. I'll write my own benchmark program

id:
1135
timestamp:
2025-03-06T17:45:33.052Z
sender:
@jay_ztc:matrix.org
content:
for the 'info' host call-> where is the serialization of the service accounts preimage availability dictionary (l) defined? 

https://graypaper.fluffylabs.dev/#/5f542d7/307f02307f02?v=0.6.2

id:
1134
timestamp:
2025-03-06T22:13:14.176Z
sender:
@jay_ztc:matrix.org
content:
Pretty sure this is the case, but want to verify here-> this branch conditional in the read host call is TYPE equivalence rather than literal equivalence? So this branch wouldn't be taken if w7 == s, only in the case where w7 == 2^64-1

https://graypaper.fluffylabs.dev/#/5f542d7/308500308500?v=0.6.2

id:
2541
timestamp:
2025-03-06T23:59:17.486Z
sender:
@dave:parity.io
content:
We're working on EC test vectors, they'll be posted here when they're ready

id:
2540
timestamp:
2025-03-07T13:47:59.863Z
sender:
@danicuki:matrix.org
content:
Here are some updates for the JAM Experience event
- The dates will be **6th-7th** May (it was wrongly configured in lu.ma 
- The JAM Toaster visit will be in one of these two days
- The event will be at Parity's office - R. Serpa Pinto 12, 1200-443 Lisboa, Portugal
- Full schedule yet to be defined, probably in the next couple of weeks.
- We already have 35 confirmed jammers. If you didn't register yourself, hurry up, there are only 10 places left: https://lu.ma/ob0n7pdy
- If you registered and won't be able to come, please release your place for someone else

- We will create a separate luma event for the closing Party - if you are registered in the main 2 days event, you **don't need** to registry for the party. 
- The party is open for all crypto community, so you can share the party link with anyone might be interested in JAM.

id:
2539
timestamp:
2025-03-07T13:50:05.704Z
sender:
@danicuki:matrix.org
content:
* Here are some updates for the JAM Experience event

- The dates will be **6th-7th** May (it was wrongly configured in lu.ma
- The JAM Toaster visit will be in one of these two days
- The event will be at Parity's office - R. Serpa Pinto 12, 1200-443 Lisboa, Portugal
- Full schedule yet to be defined, probably in the next couple of weeks.
- We already have 35 confirmed jammers. If you didn't register yourself, hurry up, there are only 10 places left: https://lu.ma/ob0n7pdy
- If you registered and won't be able to come, please release your place for someone else
- We will create a separate luma event for the closing Party - if you are registered in the main 2 days event, you **don't need** to registry for the party.
- The party is open for all crypto community, so you can share the party link with anyone might be interested in JAM.
- ETH Lisbon is 9th-11th - Registry here: https://ethlisbon.org/ for an extra couple of days together 

id:
2538
timestamp:
2025-03-07T13:50:47.146Z
sender:
@danicuki:matrix.org
content:
* Here are some updates for the JAM Experience event

- The dates will be **6th-7th** May (it was wrongly configured in lu.ma
- The JAM Toaster visit will be in one of these two days
- The event will be at Parity's office - R. Serpa Pinto 12, 1200-443 Lisboa, Portugal
- Full schedule yet to be defined, probably in the next couple of weeks.
- We already have 35 confirmed jammers. If you didn't register yourself, hurry up, there are only 10 places left: https://lu.ma/ob0n7pdy
- If you registered and won't be able to come, please release your place for someone else
- We will create a separate luma event for the closing Party - if you are registered in the main 2 days event, you **don't need** to registry for the party.
- The party is open for all crypto community, so you can share the [party link](https://lu.ma/event/manage/evt-esQH7AOFAIXxEp5) with anyone might be interested in JAM.
- ETH Lisbon is 9th-11th - Registry here: https://ethlisbon.org/ for an extra couple of days together

id:
2537
timestamp:
2025-03-07T13:51:46.956Z
sender:
@danicuki:matrix.org
content:
* Here are some updates for the JAM Experience event

- The dates will be **6th-7th** May (it was wrongly configured in lu.ma
- The JAM Toaster visit will be in one of these two days
- The event will be at Parity's office - R. Serpa Pinto 12, 1200-443 Lisboa, Portugal
- Full schedule yet to be defined, probably in the next couple of weeks.
- We already have 35 confirmed jammers. If you didn't register yourself, hurry up, there are only 10 places left: https://lu.ma/ob0n7pdy
- If you registered and won't be able to come, please release your place for someone else
- We will create a separate luma event for the closing Party - if you are registered in the main 2 days event, you **don't need** to registry for the party.
- The party is open for all crypto community, so you can share the [party link](https://lu.ma/6monb9f9) with anyone might be interested in JAM.
- ETH Lisbon is 9th-11th - Registry here: https://ethlisbon.org/ for an extra couple of days together

id:
2536
timestamp:
2025-03-07T14:13:45.774Z
sender:
@danicuki:matrix.org
content:
Yes, it was wrong. I fixed it now.

id:
2535
timestamp:
2025-03-07T16:55:14.011Z
sender:
@sourabhniyogi:matrix.org
content:
That's awesome, in the meantime here is a start consistent with the W_G = 4104 choice
https://github.com/jam-duna/jamtestnet/tree/main/erasurecoding
It doesn't have transformations like in Appendix H though so it is desirable to see actual Rust code for the transformation and an explanation of why it is actually necessary in non-full chain specs to actually include the transformation

id:
2534
timestamp:
2025-03-07T18:31:48.766Z
sender:
@greywolve:matrix.org
content:
The test vectors above don't seem to include the Appendix H transforms, looks like it's purely testing the C function that does the actual encoding.

They might be using some sort of pre-transform before encoding that isn't mentioned in the GP and that forms part of C for SIMD purposes. Version 3.0.1 of reed-solomon-simd does [this](https://github.com/AndersTrier/reed-solomon-simd/blob/74f48e20636f80c43d0e8884ff7a87669bfd8be6/src/engine/shards.rs#L36-L73) to handle smaller shards. Maybe there's another way to do that which is why we aren't matching the recovery shards.

id:
1133
timestamp:
2025-03-07T20:23:50.195Z
sender:
@ascriv:matrix.org
content:
> <@gav:polkadot.io> Well, it’s not that it’s not “valid” (don’t forget we’re not using typed logic here, just basic set formalisms). Rather that there can be early certainty that if l is not in N_2^32, then there can certainly be no keys (c, l) in the set (H, N_2^32).

For some cases when lhs and rhs types are different, it’s clear what to do, for example in set inclusion we can safely interpret as evaluating to false. But in other cases where lhs and rhs are different types, like the building of the set that is the new service account, it becomes too ambiguous. One interpretation is that the l component in the new service account should be empty. Another is that a should be \error . I’ve made a PR with the second interpretation 
http://github.com/gavofyork/pull/279

id:
1132
timestamp:
2025-03-07T20:24:55.325Z
sender:
@ascriv:matrix.org
content:
* For some cases when lhs and rhs types are different, it’s clear what to do, for example in set inclusion we can safely interpret as evaluating to false. But in other cases where lhs and rhs are different types, like the building of the set that is the new service account, it becomes too ambiguous. One interpretation is that the l component in the new service account should be empty. Another is that a should be \error . I’ve made a PR with the second interpretation 
http://github.com/gavofyork/graypaper/pull/279

id:
2533
timestamp:
2025-03-08T08:10:14.005Z
sender:
@gav:polkadot.io
content:
I think it would be no bad thing if anyone well-familiar with JAM's internals happened to have a speaking slot at an event ETH Lisbon

id:
1131
timestamp:
2025-03-08T08:13:38.673Z
sender:
@gav:polkadot.io
content:
Yes, that's fair. Merged.

id:
1130
timestamp:
2025-03-08T08:15:54.159Z
sender:
@gav:polkadot.io
content:
you've highlighted regular l, when the preimage dictionary is bold-l.

id:
1129
timestamp:
2025-03-08T08:16:20.543Z
sender:
@gav:polkadot.io
content:
regular l is an dependent field.

id:
1128
timestamp:
2025-03-08T08:18:01.872Z
sender:
@gav:polkadot.io
content:
s* and s are both in `\N_S`. the comparison is just a simple numeric one.

id:
1127
timestamp:
2025-03-08T08:18:09.162Z
sender:
@gav:polkadot.io
content:
* No idea what you're talking about. s\* and s are both in `\N_S`. the comparison is just a simple numeric one.

id:
1126
timestamp:
2025-03-08T08:19:27.956Z
sender:
@gav:polkadot.io
content:
* No idea what you're talking about. s\* and s are both in `\N_S`. the comparison is just a simple numeric one. If you work the logic through, it would be taken is omega_7 = s or omega_7 = 2^64-1.

id:
1125
timestamp:
2025-03-08T08:19:35.347Z
sender:
@gav:polkadot.io
content:
* No idea what you're talking about. s\* and s are both in `\N_S`. the comparison is just a simple numeric one. If you work the logic through, it would be taken if either omega\_7 = s or omega\_7 = 2^64-1.

id:
1124
timestamp:
2025-03-08T08:20:06.926Z
sender:
@gav:polkadot.io
content:
* No idea what you're talking about. s\* and s are both in `\N_S`. the comparison is just a simple numeric one. If you work the logic through, it would be taken if `omega_7 in { s,  2^64-1 }`.

id:
2532
timestamp:
2025-03-08T14:30:48.833Z
sender:
@clearloop:matrix.org
content:
May I ask has any team tested that how many non-ticket sealed blocks could be produced generally in tiny testnet?

I found that I have to author blocks on `forks` first otherwise the chain will never get finalized, or, have to produce ticket sealed tickets only

id:
2531
timestamp:
2025-03-08T14:31:31.039Z
sender:
@clearloop:matrix.org
content:
* May I ask has any team tested that how many non-ticket sealed blocks could be produced generally in tiny testnet?

I found that I have to author blocks on `forks` first otherwise the chain will never get finalized (tickets have distributed in forks which won't get finalized), or, have to produce ticket sealed tickets only

id:
2530
timestamp:
2025-03-08T14:32:19.882Z
sender:
@clearloop:matrix.org
content:
* May I ask has any team tested that how many non-ticket sealed blocks could be produced generally in tiny testnet?

I found that I have to author blocks on `forks` first otherwise the chain will never get finalized (tickets have distributed in forks which won't get finalized)

or, have to produce ticket sealed tickets only and in this case, hardly can maintain the 6-secs timeslot for block production

id:
2529
timestamp:
2025-03-08T14:33:49.800Z
sender:
@clearloop:matrix.org
content:
* May I ask has any team tested that how many non-ticket sealed blocks could be produced generally in tiny testnet?

I found that I have to author blocks on `forks` first otherwise the chain will never get finalized (tickets have distributed in node's own forks which won't get finalized)

or, have to produce ticket sealed tickets only and in this case, hardly can maintain the 6-secs timeslot for block production

id:
2528
timestamp:
2025-03-08T14:34:02.398Z
sender:
@clearloop:matrix.org
content:
* May I ask has any team tested that how many non-ticket sealed blocks could be produced generally in tiny testnet?

I found that I have to author blocks on `forks` first otherwise the chain will never get finalized (tickets have distributed in node's own forks which won't get finalized)

or, have to produce ticket sealed blocks only and in this case, hardly can maintain the 6-secs timeslot for block production

id:
2527
timestamp:
2025-03-08T14:36:14.109Z
sender:
@clearloop:matrix.org
content:
* May I ask has any team tested that how many non-ticket sealed blocks could be produced generally in tiny testnet?

I found that I have to author blocks on `forks` first otherwise the chain will never get finalized (tickets have distributed in nodes' own forks which won't get finalized)

or, have to produce ticket sealed blocks only and in this case, hardly can maintain the 6-secs timeslot for block production

id:
1123
timestamp:
2025-03-08T14:45:21.045Z
sender:
@jay_ztc:matrix.org
content:
Thanks for the response! I'm not sure I'm following though? 't' is referencing an account, and I only see 'l' used in the accounts context when referring to the lookup dict? It would make sense if the metadata returned by the info host call contains some metadata about preimages, maybe just the keys even -> but I'm assuming that would be represented as K{t}_l in the spec if that were the case...

id:
2526
timestamp:
2025-03-08T14:47:02.397Z
sender:
@clearloop:matrix.org
content:
* May I ask has any team tested that how many non-ticket sealed blocks could be produced generally in tiny testnet?

I found that I have to author blocks on `forks` first otherwise the chain will never get finalized (tickets have distributed in nodes' own forks which won't get finalized), while maintaining the state of fork may increase the complexity of state management significantly

or, have to produce ticket sealed blocks only and in this case, hardly can maintain the 6-secs timeslot for block production

id:
2525
timestamp:
2025-03-08T14:47:38.551Z
sender:
@clearloop:matrix.org
content:
* May I ask has any team tested that how many non-ticket sealed blocks could be produced generally in tiny testnet?

I found that I have to author blocks on `forks` first otherwise the chain will never get finalized (tickets have distributed in nodes' own forks which won't get finalized), while maintaining the state of fork may increase the complexity of state management significantly

or, have to produce ticket sealed blocks only and this is obvious incorrect according to the graypaper, since it hardly can maintain the 6-secs timeslot for block production

id:
2524
timestamp:
2025-03-08T14:47:45.886Z
sender:
@clearloop:matrix.org
content:
* May I ask has any team tested that how many non-ticket sealed blocks could be produced generally in tiny testnet?

I found that I have to author blocks on `forks` first otherwise the chain will never get finalized (tickets have distributed in nodes' own forks which won't get finalized), while maintaining the state of fork may increase the complexity of state management significantly

or, have to produce ticket sealed blocks only and this is obvious incorrect according to the graypaper, since it hardly can maintain the 6-secs timeslot of block production

id:
2523
timestamp:
2025-03-08T14:48:07.405Z
sender:
@clearloop:matrix.org
content:
* May I ask has any team tested that how many non-ticket sealed blocks could be produced generally in tiny testnet?

I found that I have to author blocks on `forks` first otherwise the chain will never get finalized (tickets have distributed in nodes' own forks which won't get finalized), while maintaining the state of fork may increase the complexity of state management significantly

or, have to produce ticket sealed blocks only and this is obvious incorrect according to the graypaper since it hardly can maintain the 6-secs timeslot of block production

id:
2522
timestamp:
2025-03-08T14:53:27.223Z
sender:
@ycc3741:matrix.org
content:
I would like to ask if there is a specific required version for PKCS.

id:
1122
timestamp:
2025-03-08T15:02:52.790Z
sender:
@jay_ztc:matrix.org
content:
Thanks for clarifying! Wasn't sure if an account being able to selectively target its *s* vs. *d* context was a valid use case. Also was getting hung up on this being a more verbose way to get to 'a' than in the previous host call (lookup) -> but I see the intermediate value used when querying the storage, so the verbosity makes sense there.

id:
2521
timestamp:
2025-03-08T15:04:37.930Z
sender:
@clearloop:matrix.org
content:
I believe pkcs#8 will be chosen since it's stable with ed25519

id:
2520
timestamp:
2025-03-08T17:37:34.461Z
sender:
@ycc3741:matrix.org
content:
cool. so does our team.

id:
2519
timestamp:
2025-03-08T17:38:36.029Z
sender:
@ycc3741:matrix.org
content:
Also, just curious—I watched the Jam Tour in China and wanted to ask if there’s an estimated timeline for the release of coreVM, or if there’s a place where I can find more details or advanced information?

id:
1121
timestamp:
2025-03-08T20:35:10.879Z
sender:
@jaymansfield:matrix.org
content:
For the bless and assign host calls, what should happen if a non-privileged service tries to update it? Panic? The GP doesn’t really specify.

id:
1120
timestamp:
2025-03-08T20:38:11.221Z
sender:
@jaymansfield:matrix.org
content:
* For the bless and assign host calls, what should happen if a non-privileged service tries to execute it? Panic? The GP doesn’t really specify.

id:
1119
timestamp:
2025-03-08T22:32:15.388Z
sender:
@jaymansfield:matrix.org
content:
* Hello. For the bless and assign host calls, what should happen if a non-privileged service tries to execute it? Panic? The GP doesn’t really specify.

id:
1118
timestamp:
2025-03-09T01:48:05.079Z
sender:
@jaymansfield:matrix.org
content:
* Hello. For the bless and assign host calls, what should happen if a non-privileged service tries to execute it? Panic or HUH? The GP doesn’t really specify.

id:
2518
timestamp:
2025-03-09T12:27:26.783Z
sender:
@danicuki:matrix.org
content:
> <@gav:polkadot.io> I think it would be no bad thing if anyone well-familiar with JAM's internals happened to have a speaking slot at an event ETH Lisbon

Some of us can fill this form with talk proposal:

https://airtable.com/appTaYsacDkPPrePJ/pagd1QKEO2r3i2idK/form

We can also ask for budget to events bounty to sponsor ETHLisbon as Polkadot. I can’t do it myself because I already asked money for the JAMXP event. 

id:
2517
timestamp:
2025-03-10T10:49:58.678Z
sender:
@dvdplas:matrix.org
content:
Does that mean that one core can only run one service per block?

id:
2516
timestamp:
2025-03-10T10:54:05.967Z
sender:
@dvdplas:matrix.org
content:
As for synchronous interaction between parachains in one core, how are they co-located? Can there be synchronous interaction for more than 2 "interactions" (apologies, don't know what the correct terminology for this is), in other words chain x interacts with chain y and responds back to chain x (i.e. three "interactions")

id:
2515
timestamp:
2025-03-10T10:56:41.923Z
sender:
@dvdplas:matrix.org
content:
And how will this look like from the parachains' perspective, how can they ensure their work is scheduled to the same core in the same block?

id:
2514
timestamp:
2025-03-10T10:57:11.044Z
sender:
@dvdplas:matrix.org
content:
* And how will this look like from the parachains' perspective, how can they ensure their work is scheduled to the same core in the same block (to have synchronous interaction)?

id:
2513
timestamp:
2025-03-10T10:57:24.140Z
sender:
@bkchr:parity.io
content:
Such a block would need to be build by a collator that can build for both chains X & Y 

id:
2512
timestamp:
2025-03-10T10:58:36.250Z
sender:
@dvdplas:matrix.org
content:
Something like that doesn't exist / we haven't seen as of now?

id:
2511
timestamp:
2025-03-10T10:58:45.737Z
sender:
@dvdplas:matrix.org
content:
* Something like that doesn't exist / we haven't seen as of now in Polkadot?

id:
2510
timestamp:
2025-03-10T10:59:37.143Z
sender:
@dvdplas:matrix.org
content:
* Something like that doesn't exist / we haven't seen as of now in Polkadot right?

id:
2509
timestamp:
2025-03-10T11:01:31.040Z
sender:
@bkchr:parity.io
content:
Yes we don't have done this yet in Polkadot 

id:
2508
timestamp:
2025-03-10T11:02:07.162Z
sender:
@dvdplas:matrix.org
content:
Do those work items, submitted by this collator, include anything that co-locates those work items or are they submitted in a way that ensures the work items are executed after each other?

id:
2507
timestamp:
2025-03-10T11:02:43.535Z
sender:
@bkchr:parity.io
content:
None of this code exists yet :) 

id:
2506
timestamp:
2025-03-10T11:03:54.988Z
sender:
@bkchr:parity.io
content:
Also parachains are quite heavy in their functionality, so if they get co-located is a little bit questionable 

id:
2505
timestamp:
2025-03-10T11:04:20.165Z
sender:
@dvdplas:matrix.org
content:
And I assume that xcmp is necessary?

In other words, it is impossible to implement this on Polkadot right now?

id:
2504
timestamp:
2025-03-10T11:04:29.262Z
sender:
@bkchr:parity.io
content:
Also AFAIK this is not really planned to develop this feature for parachains. Everyone could for sure, but will probably not be a top feature 

id:
2503
timestamp:
2025-03-10T11:04:49.300Z
sender:
@bkchr:parity.io
content:
To implement what? Co-scheduling? 

id:
2502
timestamp:
2025-03-10T11:05:18.288Z
sender:
@dvdplas:matrix.org
content:
Yes, because with hrmp you need it to be processed through the relay

id:
2501
timestamp:
2025-03-10T11:07:26.821Z
sender:
@dvdplas:matrix.org
content:
I don't understand, is this not one of the big selling  points? Having synchronous composability between chains?

id:
2500
timestamp:
2025-03-10T11:08:40.598Z
sender:
@dvdplas:matrix.org
content:
> Also AFAIK this is not really planned to develop this feature for parachains

For what is it planned if I may ask?

id:
2499
timestamp:
2025-03-10T11:09:00.458Z
sender:
@bkchr:parity.io
content:
If you don't send messages or receive messages you could probably build something like this right now already. 

id:
2498
timestamp:
2025-03-10T11:09:18.611Z
sender:
@bkchr:parity.io
content:
CorePlay. I mean JAM will provide this feature for any service 

id:
2497
timestamp:
2025-03-10T11:10:20.173Z
sender:
@dvdplas:matrix.org
content:
Yes but using what cross chain message protocol 

id:
2496
timestamp:
2025-03-10T11:10:25.779Z
sender:
@dvdplas:matrix.org
content:
* Yes but using what cross chain message protocol ?

id:
2495
timestamp:
2025-03-10T11:12:24.849Z
sender:
@bkchr:parity.io
content:
I mean all of that is quite hypothetical 

id:
2494
timestamp:
2025-03-10T11:20:01.847Z
sender:
@dvdplas:matrix.org
content:
Is CorePlay going to be a service or a feature of JAM?

And will it provide synchronous interaction between services or work items of the same service?

id:
2493
timestamp:
2025-03-10T11:20:20.487Z
sender:
@dvdplas:matrix.org
content:
* Is CorePlay going to be a service or a feature of JAM?

And will it provide synchronous interaction between services or work items of the same service (in one core)?

id:
2492
timestamp:
2025-03-10T11:20:22.811Z
sender:
@bkchr:parity.io
content:
It will be a service 

id:
2491
timestamp:
2025-03-10T11:20:56.787Z
sender:
@bkchr:parity.io
content:
JAM will come with no batteries included :D 

id:
2490
timestamp:
2025-03-10T11:54:30.464Z
sender:
@dvdplas:matrix.org
content:
And will it provide synchronous interaction between services or work items of the same service (in one core)? :):):)

id:
2489
timestamp:
2025-03-10T11:54:36.188Z
sender:
@dvdplas:matrix.org
content:
* And will it provide synchronous interaction between services or work items of the same service (in one core)? 

:):):)

id:
2488
timestamp:
2025-03-10T12:00:02.846Z
sender:
@dvdplas:matrix.org
content:
* And will it provide synchronous interaction between work items of multiple services or work items of the same service (in one core)? 

In other words, can one JAM core only run one type of service per block?

id:
2487
timestamp:
2025-03-10T12:50:08.244Z
sender:
@dvdplas:matrix.org
content:
* And will it provide synchronous interaction between work items of multiple services or work items of the same service (in one core)? 

This comes back to my first question I guess, can one JAM core only run one type of service per block?

id:
1117
timestamp:
2025-03-10T13:25:44.509Z
sender:
@jay_ztc:matrix.org
content:
Happy Monday folks. In the 'new' host function-> the GP doesn't specify what should happen if the preimage blob length is above the valid range required by the account spec (N_l = 2^32). There is a related case where this type of behavior is specified-> In the bless host call the GP specifies a 'WHO' exit when the values aren't within the valid N_s range. Curious to hear what the guidance is here, happy to open a PR if needed.

https://graypaper.fluffylabs.dev/#/5f542d7/31db0031e600?v=0.6.2

id:
2486
timestamp:
2025-03-10T16:08:53.179Z
sender:
@danicuki:matrix.org
content:
Screenshot 2025-03-10 at 13.08.11.png

id:
2485
timestamp:
2025-03-10T16:09:47.797Z
sender:
@danicuki:matrix.org
content:
These are ETHLisbon Sponsorship packages: https://encodeclub.notion.site/ETHLisbon-2025-1436c123e77d801bbf02e2b9dce2b43d

What do you guys think about having a Polkadot / JAM booth? 

id:
2484
timestamp:
2025-03-10T16:18:33.273Z
sender:
@jay_ztc:matrix.org
content:
Has documentation been released yet about the specific scope for M1 conformance? I'm curious what layer the testing will be at- since DA systems are out of scope for M1. Are there any specific APIs we should be conformant with, to make M1 acceptance testing across many implementations more streamlined?

id:
2483
timestamp:
2025-03-10T19:28:20.424Z
sender:
@philipdisarro:matrix.org
content:
Hello all, thanks for the invitation. I just want to clarify a few things. First off, I am a big fan of the enshrined-rollup centric architecture, I think monolithic scaling is a dead-end and a honeypot for centralized chains. I think JAM is an incredible innovation on the forefront of modular scalability (scalability without compromising L1 decentralization), and I think L1s as DA layers with minimal computation capacity (mainly for validating rollup state-commitments and fault proof resolution). That said, is JAM not what I am describing above? A state-of-the-art DA layer with a coherent data-sharing solution that addresses DA fragmentation?  

id:
2482
timestamp:
2025-03-10T19:29:45.631Z
sender:
@philipdisarro:matrix.org
content:
* Hello all, thanks for the invitation. I just want to clarify a few things. First off, I am a big fan of the enshrined-rollup centric architecture, I think monolithic scaling is a dead-end and a honeypot for centralized chains. I think JAM is an incredible innovation on the forefront of modular scalability (scalability without compromising L1 decentralization), and I think L1s as DA layers with minimal computation capacity (mainly for validating rollup state-commitments and fault proof resolution). That said, is JAM not what I am describing above? A state-of-the-art DA layer with a coherent data-sharing solution that addresses DA fragmentation. 

From my perspective, the goal is not to have all logic ran natively with the full security of the L1, it is to instead achieve scalability via enshrined-rollups (in some sense equivalent to sharding) with fault proofs to inherit the full security of the L1 under the 1 honest live actor assumption.  

id:
2481
timestamp:
2025-03-10T19:30:24.733Z
sender:
@philipdisarro:matrix.org
content:
* Hello all, thanks for the invitation. I just want to clarify a few things. First off, I am a big fan of the enshrined-rollup centric architecture, I think monolithic scaling is a dead-end and a honeypot for centralized chains. I think JAM is an incredible innovation on the forefront of modular scalability (scalability without compromising L1 decentralization), and I think L1s as DA layers with minimal computation capacity (mainly for validating rollup state-commitments and fault proof resolution). That said, is JAM not what I am describing above? A state-of-the-art DA layer with a coherent data-sharing solution that addresses DA fragmentation. 

From my perspective, the goal is not to have all logic ran natively with the full security of the L1, it is to instead achieve scalability via enshrined rollups (in some sense equivalent to sharding) with fault proofs to inherit the full security of the L1 under the 1 honest live actor assumption. It is a mechanism for horizonal scalability.

id:
2480
timestamp:
2025-03-10T19:37:08.598Z
sender:
@bkchr:parity.io
content:
But JAM is not using fault proofs

id:
2479
timestamp:
2025-03-10T19:37:18.103Z
sender:
@bkchr:parity.io
content:
It re-executes the candidates 

id:
2478
timestamp:
2025-03-10T19:55:29.912Z
sender:
@philipdisarro:matrix.org
content:
AFAIK, it re-executes the candidates with a subset of L1 validators. The only way to inherit the full base layer security is by raising an issue (ie. fault proof). 

id:
2477
timestamp:
2025-03-10T19:55:44.876Z
sender:
@philipdisarro:matrix.org
content:
* AFAIK, it re-executes the candidates with a subset of L1 validators (enshrined-rollup). The only way to inherit the full base layer security is by raising an issue (ie. fault proof). 

id:
2476
timestamp:
2025-03-10T19:57:01.513Z
sender:
@bkchr:parity.io
content:
https://x.com/jeffburdges/status/1898690774704771498?s=19 did you read this answer? 

id:
2475
timestamp:
2025-03-10T20:04:38.632Z
sender:
@philipdisarro:matrix.org
content:
Yes, and again from what I understand JAM is still a base layer for enshrined rollups. I don't argue against the degree to which those enshrined rollups inherit the base layer security. ELVES seems like a very sane design for rollups, with a very high degree of security inheritance, but at the end of the day there is still an objective difference in security between execution in a rollup and execution directly on the base layer with the security of the entire validator set. The difference in that security with respect to the enshrined rollups is a separate matter entirely, but it does exist.

id:
2474
timestamp:
2025-03-10T20:06:21.875Z
sender:
@philipdisarro:matrix.org
content:
IE ELVES & JAM are an improvement upon other rollup-centric architectures, and the post you have referenced explicitly mentions:

> ELVES could be described as a twisted non-zk cut n choose roll up, which some shorten to pessimistic roll up.

Which again, furthers my understanding that indeed Doom executed on a rollup and not on the base layer.

id:
2473
timestamp:
2025-03-10T20:07:45.935Z
sender:
@philipdisarro:matrix.org
content:
* IE ELVES & JAM are an improvement upon other rollup-centric architectures, and the post you have referenced explicitly mentions:

> ELVES could be described as a twisted non-zk cut n choose roll up, which some shorten to pessimistic roll up.

Which again, furthers my understanding that indeed Doom executed on a rollup and not on the base layer. I don't dispute the degree to which the rollup inherits the base layer security, but it doesn't objectively inherit the full security of L1 validation.

id:
2472
timestamp:
2025-03-10T20:08:00.316Z
sender:
@bkchr:parity.io
content:
It is more likely that you get hit by meteoroid, but we still don't assume that you already got hit :) You are saying because there is a chance that is smaller than getting hit by a meteoroid, the security is less than just running it as part of the state transition function of JAM.  

id:
2471
timestamp:
2025-03-10T20:11:25.016Z
sender:
@philipdisarro:matrix.org
content:
Indeed, it is. There is a difference in security. I don't argue that the difference is large, and relative to the scalability that is achieved I don't argue that this is not a sensible choice, but at the end of the day, the computation is still executing on a rollup and settling to the L1 right? 

As stated: 
>ELVES could be described as a twisted non-zk cut n choose roll up, which some shorten to pessimistic roll up.

It is even described as a rollup. I don't disagree that the rollup inherits maximal security from the L1, but that don't mean it isn't a rollup.

id:
2470
timestamp:
2025-03-10T20:12:27.743Z
sender:
@philipdisarro:matrix.org
content:
Strictly speaking, from my perspective it is correct to say that Doom executed on a rollup. 

id:
2469
timestamp:
2025-03-10T20:13:03.740Z
sender:
@philipdisarro:matrix.org
content:
That rollup inherits maximal security from the L1, and probabilistically equivalent security, but this is not the same as executing natively on the L1.

id:
2468
timestamp:
2025-03-10T20:16:34.237Z
sender:
@philipdisarro:matrix.org
content:
Originally, the only reason I even began this discussion is because someone posted:
https://x.com/csaint02/status/1897864223382011954

and then went on to claim that Hydra Doom does not count because it was executed in a L2, and that on Polkadot it was executed on the base layer. 

id:
2467
timestamp:
2025-03-10T20:17:56.863Z
sender:
@philipdisarro:matrix.org
content:
To me this is an unfair claim. At the end of the day, both Hydra Doom and Doom on Polkadot executed in L2s, it is just that with JAM the degree to which the L2 inherits the L1 security is much higher. 

id:
2466
timestamp:
2025-03-10T20:19:41.732Z
sender:
@bkchr:parity.io
content:
Reading into Hydra, only the results are returned back to the main chain

id:
2465
timestamp:
2025-03-10T20:19:49.283Z
sender:
@bkchr:parity.io
content:
The operations in between are not checked 

id:
2464
timestamp:
2025-03-10T20:19:58.849Z
sender:
@bkchr:parity.io
content:
https://hydra.family/head-protocol/ at least from what I can read here 

id:
2463
timestamp:
2025-03-10T20:21:35.940Z
sender:
@philipdisarro:matrix.org
content:
Agreed, but Hydra is a state channel, not a rollup, the goal of a state channel is for participants of the channel to inherit the full security of the L1 (because the computation is entirely irrelevant for non-participants). That means if you played in the Hydra Doom tournament (ie. were a participant in the Hydra Head) you have equivalent security guarantees of the L1.

id:
2462
timestamp:
2025-03-10T20:21:37.055Z
sender:
@bkchr:parity.io
content:
> By providing more efficient means of processing transactions off-chain for a set of users

id:
2461
timestamp:
2025-03-10T20:22:43.365Z
sender:
@bkchr:parity.io
content:
> That means if you played in the Hydra Doom tournament (ie. were a participant in the Hydra Head) you have equivalent security guarantees of the L1.

This contradicts the beginning of your message

id:
2460
timestamp:
2025-03-10T20:22:50.452Z
sender:
@bkchr:parity.io
content:
How do you get the L1 security guarantess

id:
2459
timestamp:
2025-03-10T20:22:55.475Z
sender:
@philipdisarro:matrix.org
content:
If 10 people bet $100 on a Doom game, and the game is executed in a Hydra head where all ten are participants in the channel, then the game has the full security of the L1, because a participant would never willingly defraud themselves. 

id:
2458
timestamp:
2025-03-10T20:23:02.319Z
sender:
@bkchr:parity.io
content:
If the Hydra head just does the computation off chain? 

id:
2457
timestamp:
2025-03-10T20:23:38.407Z
sender:
@philipdisarro:matrix.org
content:
https://iohk.io/en/research/library/papers/hydra-fast-isomorphic-state-channels/

id:
2456
timestamp:
2025-03-10T20:24:35.123Z
sender:
@philipdisarro:matrix.org
content:
Hydra requires unanimous consensus amongst participants, so as a participant you will never willingly defraud yourself. 

id:
2455
timestamp:
2025-03-10T20:26:56.145Z
sender:
@philipdisarro:matrix.org
content:
I agree that enshrined-rollups are a more general purpose scaling solution, and that for adversarial cases the fact that it doesn't require unanimous consensus amongst participants is obviously a massive strength. I'm not arguing that Hydra is better in any way. I am just arguing that both Hydra, and JAM's enshrined rollups are indeed rollups (L2s).

id:
2454
timestamp:
2025-03-10T20:27:04.640Z
sender:
@philipdisarro:matrix.org
content:
* I agree that enshrined-rollups are a more general purpose scaling solution, and that for adversarial cases the fact that it doesn't require unanimous consensus amongst participants is obviously a massive strength. I'm not arguing that Hydra is better in any way. I am just arguing that both Hydra, and JAM's enshrined rollups are indeed L2s.

id:
2453
timestamp:
2025-03-10T20:27:40.356Z
sender:
@philipdisarro:matrix.org
content:
Personally, I think JAM's enshrined rollups are much better L2s for 99% of domains, but still they are both L2s.

id:
2452
timestamp:
2025-03-10T20:27:50.516Z
sender:
@philipdisarro:matrix.org
content:
* Personally, I think JAM's enshrined rollups are much better L2s for 99% of task domains, but still they are both L2s.

id:
2451
timestamp:
2025-03-10T20:30:53.069Z
sender:
@bkchr:parity.io
content:
As I skimmed the paper it also requires that parties report to the main chain if someone closed a head on a wrong state. 

id:
2450
timestamp:
2025-03-10T20:32:14.048Z
sender:
@bkchr:parity.io
content:
Also this tweet said:

> Cardano recorded DOOM stats on chain

> Polkadot ran the entire MFing game on chain

Which is correct, even from what you said above. (on chain doesn't mean that the on chain was JAM itself and it is more the L2) 

id:
2449
timestamp:
2025-03-10T20:32:41.507Z
sender:
@bkchr:parity.io
content:
But every L2 transition is checked by the JAM validators 

id:
2448
timestamp:
2025-03-10T20:33:08.834Z
sender:
@bkchr:parity.io
content:
With the very very very very.... small likelihood that you paid enough validators to cheat 

id:
2447
timestamp:
2025-03-10T20:33:30.704Z
sender:
@ascriv:matrix.org
content:
Maybe L1.0001?

id:
2446
timestamp:
2025-03-10T20:33:50.160Z
sender:
@bkchr:parity.io
content:
We can agree on that both run on L2's (Cardano + Jam), but each with very different security guarantees. 

id:
2445
timestamp:
2025-03-10T20:34:25.966Z
sender:
@philipdisarro:matrix.org
content:
Closed a head on a previous state*

You cannot close a head on an invalid state because to initiate a close you need to provide the state snapshot that is unanimously signed by all participants, it's just that the snapshot you provided might not be the most recent snapshot, and in that case others can begin a complaint processes which allows them to provide a more recent state (and the most recent state at the end of this process is considered canonical). 

id:
2444
timestamp:
2025-03-10T20:34:53.312Z
sender:
@philipdisarro:matrix.org
content:
Yes, that's the only thing I wanted to clarify.

id:
2443
timestamp:
2025-03-10T20:35:49.750Z
sender:
@philipdisarro:matrix.org
content:
Thank you for your time. I think JAM is a novel of engineering and a clear advancement in horizontal scalability in the blockchain domain as a whole.

id:
2442
timestamp:
2025-03-10T20:36:12.442Z
sender:
@bkchr:parity.io
content:
Yeah maybe you did not like the future output and just stopped at some state. Generally sounds to me like you could build some exploits around it

id:
2441
timestamp:
2025-03-10T20:36:18.101Z
sender:
@bkchr:parity.io
content:
Or I buy my croissant 

id:
2440
timestamp:
2025-03-10T20:36:24.971Z
sender:
@bkchr:parity.io
content:
And then pretend this never happned 

id:
2439
timestamp:
2025-03-10T20:36:30.473Z
sender:
@bkchr:parity.io
content:
* And then pretend this never happened 

id:
2438
timestamp:
2025-03-10T20:36:31.142Z
sender:
@erin:parity.io
content:
> <@philipdisarro:matrix.org> Yes, that's the only thing I wanted to clarify.

state channels are sooooooo far away from rollups though, and even further away from parachains, and even further away from JAM running DOOM on CoreVM. But sure.

id:
2437
timestamp:
2025-03-10T20:37:24.029Z
sender:
@erin:parity.io
content:
If you want to be pedantically correct yes, but if you want to be intellectually honest for the people you are speaking to then realistically no.

id:
2436
timestamp:
2025-03-10T20:38:19.882Z
sender:
@philipdisarro:matrix.org
content:
Right, but that's the purpose of the L1 resolution phase, if you attempt to do so, any of the other participants will just provide the most recent signature. 

id:
2435
timestamp:
2025-03-10T20:39:25.309Z
sender:
@philipdisarro:matrix.org
content:
Again, I am not arguing that Hydra is some amazing general purpose technology. It is a different scaling solution with its own set of tradeoffs, specifically tailored to maximal scalability between non-adversarial participants (ie. B2B micro-payments)

id:
2434
timestamp:
2025-03-10T20:39:39.437Z
sender:
@bkchr:parity.io
content:
I would just dos you until this phase is over :D 

id:
2433
timestamp:
2025-03-10T20:42:30.126Z
sender:
@philipdisarro:matrix.org
content:
I didn't design Hydra haha, I don't even particularly like the design of Hydra, personally I think the versatility of rollups intending to directly inherit L1 security is a much better approach (which is why my firm is building a rollup stack on Cardano).

id:
2432
timestamp:
2025-03-10T20:42:41.631Z
sender:
@philipdisarro:matrix.org
content:
* I didn't design Hydra haha, I don't even particularly like the design of Hydra, personally I think the versatility of rollups intending to directly inherit maximal L1 security is a much better approach (which is why my firm is building a rollup stack on Cardano).

id:
2431
timestamp:
2025-03-10T20:43:12.568Z
sender:
@bkchr:parity.io
content:
You can always come and build some rollup stack on Jam :) 

id:
2430
timestamp:
2025-03-10T20:43:15.065Z
sender:
@philipdisarro:matrix.org
content:
My only point is that the Doom was not executed directly on the L1, it was executed on a rollup that settles to the L1 and inherits maximal security of the L1.

id:
2429
timestamp:
2025-03-10T20:44:00.196Z
sender:
@jam_man:matrix.org
content:
Philip DiSarro:  could you explain how exactly Hydra Heads Ran Doom? Because from my understanding the game logic was not actually run in the heads themselves? 

id:
2428
timestamp:
2025-03-10T20:44:24.984Z
sender:
@bkchr:parity.io
content:
"Settles" sounds so innocent and leaves room for a lot of interpretation.

id:
2427
timestamp:
2025-03-10T20:44:52.285Z
sender:
@philipdisarro:matrix.org
content:
I might take you up on this after we finish the stack on Cardano. Eventually the goal is to have settlement layer be modular so it can be swapped for JAM or EthDA or Celestia or whatever else.

id:
2426
timestamp:
2025-03-10T20:45:39.891Z
sender:
@philipdisarro:matrix.org
content:
Doom was not executed directly on the L1, it was executed on an enshrined rollup that inherits maximal security of the L1.*

id:
2425
timestamp:
2025-03-10T20:45:54.531Z
sender:
@philipdisarro:matrix.org
content:
* > Doom was not executed directly on the L1, it was executed on an enshrined rollup that inherits maximal security of the L1.\*

That better?

id:
2424
timestamp:
2025-03-10T20:52:12.536Z
sender:
@jam_man:matrix.org
content:
So are yo you still sticking to this statement, because I would really love to know if I was wrong in my initial assessment that Hydra Heads did not actually run DOOM. They only verified game states. 

https://x.com/phil_uplc/status/1898142394345787428?s=46

id:
2423
timestamp:
2025-03-10T21:01:09.180Z
sender:
@philipdisarro:matrix.org
content:
Run the game in what sense? Cardano validators are not actors, they cannot do modify state, they simply validate. On Cardano, a chess dApp for instance, takes the move (and the transaction inputs, outputs etc) as inputs, and returns true if the move is legal (and the resulting state is stored in a UTxO), or false if the move is illegal (and the transaction fails). 

In the case of Doom, the game state is stored in UTxOs, and when the player interacts with the game they are sending a transaction to replace that UTxO with a new updated game state and validating that the transition is honest. This ensures that if you play Hydra Doom as a participant, if you are hit by a bullet, it with reflect that in the game state UTxOs, and if you attempt to cheat by violating any of the game rules you will be unable to because your transaction would be rejected (and your character / health etc, will remain where it was prior to the transaction) 

id:
2422
timestamp:
2025-03-10T21:01:33.755Z
sender:
@philipdisarro:matrix.org
content:
* Run the game in what sense? Cardano validators are not actors, they simply validate. On Cardano, a chess dApp for instance, takes the move (and the transaction inputs, outputs etc) as inputs, and returns true if the move is legal (and the resulting state is stored in a UTxO), or false if the move is illegal (and the transaction fails). 

In the case of Doom, the game state is stored in UTxOs, and when the player interacts with the game they are sending a transaction to replace that UTxO with a new updated game state and validating that the transition is honest. This ensures that if you play Hydra Doom as a participant, if you are hit by a bullet, it with reflect that in the game state UTxOs, and if you attempt to cheat by violating any of the game rules you will be unable to because your transaction would be rejected (and your character / health etc, will remain where it was prior to the transaction) 

id:
2421
timestamp:
2025-03-10T21:03:14.790Z
sender:
@philipdisarro:matrix.org
content:
Do you mean like execute the graphics engine onchain? In that case, no, the graphics rendering is not considered, it was only game logic and state that is executed onchain. 

id:
2420
timestamp:
2025-03-10T21:03:25.697Z
sender:
@philipdisarro:matrix.org
content:
* Do you mean like execute the graphics engine onchain? In that case, no, the graphics rendering is not considered, it was only game logic and state that is executed in the Hydra head*. 

id:
2419
timestamp:
2025-03-10T21:04:42.860Z
sender:
@philipdisarro:matrix.org
content:
* Do you mean like execute the graphics engine onchain? In that case, no, the graphics rendering is not considered, it was only game logic and state that is executed in the Hydra head\*.  This ensured that if you play a hydra doom tournament, none of the participants can cheat, all players must obey the game rules (cannot do more damage than they are supposed to, cannot no-clip, cannot move faster or jump higher than the game logic allows, and so forth).

id:
2418
timestamp:
2025-03-10T21:05:04.242Z
sender:
@philipdisarro:matrix.org
content:
* Do you mean like execute the graphics engine onchain? In that case, no, the graphics rendering is not considered, it was only game logic and state that is executed in the Hydra head\*.  This ensured that if you play a hydra doom tournament, none of the participants can cheat, all players must obey the game rules (cannot do more damage than they are supposed to, cannot no-clip, cannot move faster or jump higher than the game logic allows, cannot avoid taking damage when they are supposed to, and so forth).

id:
2417
timestamp:
2025-03-10T21:07:32.938Z
sender:
@jam_man:matrix.org
content:
So you retract the statement that 

“Cardano also ran Doom, the entire 
game, on Hydra, an L2”

id:
2416
timestamp:
2025-03-10T21:07:48.612Z
sender:
@jam_man:matrix.org
content:
Because you spent 2 days making this claim to me and eveyone else 

id:
2415
timestamp:
2025-03-10T21:11:32.355Z
sender:
@philipdisarro:matrix.org
content:
I stated that once in the OP, from there almost everything I said was regarding the definition of native execution vs L2s. 

Did Polkadot run multiplayer Doom entirely on a rollup? As in the graphics engine rendering was done inside the rollup? How is the output rendered? If two players move, it is immediately reflected to both players? How is no-clipping / cheating handled? 

id:
2414
timestamp:
2025-03-10T21:11:51.376Z
sender:
@philipdisarro:matrix.org
content:
* I stated that once in the OP, from there almost everything I said was regarding the definition of native execution vs L2s. 

Did Polkadot run Doom entirely on a rollup? As in the graphics engine rendering was done inside the rollup? How is the output rendered? If two players move, it is immediately reflected to both players? How is no-clipping / cheating handled? 

id:
2413
timestamp:
2025-03-10T21:15:03.306Z
sender:
@philipdisarro:matrix.org
content:
* I stated that once in the OP, from there almost everything I said was regarding the definition of native execution vs L2s. 

Did Polkadot run Doom entirely on a rollup? As in the graphics engine rendering state was done entirely inside the rollup? How is the output rendered? If two players move, it is immediately reflected to both players? How is no-clipping / cheating handled? 

id:
2412
timestamp:
2025-03-10T21:15:17.812Z
sender:
@philipdisarro:matrix.org
content:
* I stated that once in the OP, from there almost everything I said was regarding the definition of native execution vs L2s. 

Did Polkadot run Doom entirely on a rollup? As in the graphics engine rendering state was done entirely inside the rollup? How is the output rendered? If two players move, is it immediately reflected to both players? How is no-clipping / cheating handled? 

id:
2411
timestamp:
2025-03-10T21:21:59.224Z
sender:
@philipdisarro:matrix.org
content:
From what I gather the goal was different, the goal of Hydra Doom was to allow a multiplayer hydra tournament where the blockchain was used to ensure the integrity of the multiplayer tournament (all game logic is obeyed, and cheating is prevented).

From what I gather, JAM Doom was a compilation of Doom to Polkadot execution bytecode without modification? So for instance no-clips / cheating in general was not considered, as the goal was not about ensuring integrity, instead it was about showcasing the computation and bandwidth capacity of JAM services / enshrined rollups.

id:
2410
timestamp:
2025-03-10T21:24:00.595Z
sender:
@jam_man:matrix.org
content:
from my understanding yes, all aspects of the game were run directly on JAM cores (or "enshrined rollups" as you keep calling them). where the game logic was split onto two parallel cores. where both cores were both processing the doom executable, and there was another process running monitoring the JAM chain and the JAM DA, this process was feeding frames out of the DA and displaying them in real-time. 

id:
2409
timestamp:
2025-03-10T21:29:59.825Z
sender:
@philipdisarro:matrix.org
content:
If this is the case, I am happy to write a revised statement on that, as in that case, JAM Doom directly executed the transpiled binary unmodified, where-as Hydra Doom reimplemented the game logic in smart contracts that validated the integrity of game state progression to ensure the security of the multiplayer tournament.

id:
2408
timestamp:
2025-03-10T21:31:09.256Z
sender:
@philipdisarro:matrix.org
content:
That's a very impressive showcase of bandwidth / computational capacity indeed.

id:
2407
timestamp:
2025-03-10T21:33:06.103Z
sender:
@jam_man:matrix.org
content:
I would love to have some other developers back up my conceptual understanding of this. But I am 95% sure that this is how it worked. for now i can point you to this video covering it https://x.com/pala_labs/status/1898040116528238780?s=46

id:
2406
timestamp:
2025-03-10T21:34:09.851Z
sender:
@erin:parity.io
content:
> <@jam_man:matrix.org> from my understanding yes, all aspects of the game were run directly on JAM cores (or "enshrined rollups" as you keep calling them). where the game logic was split onto two parallel cores. where both cores were both processing the doom executable, and there was another process running monitoring the JAM chain and the JAM DA, this process was feeding frames out of the DA and displaying them in real-time. 

JAM services are different than rollups; they do not have to adhere to the properties of a blockchain. Polkadot parachains are only one such possible way to inherit this security, and thus services are something else entirely - they can be more generic than just running rollups as we know them.

id:
2405
timestamp:
2025-03-10T21:43:30.855Z
sender:
@erin:parity.io
content:
And you can run 20-30 DOOMs per core. This is a single core. No splitting was done.

id:
2404
timestamp:
2025-03-10T21:43:46.884Z
sender:
@erin:parity.io
content:
* And you can run 20-30 DOOMs per core (currently). This is a single core. No splitting was done.

id:
2403
timestamp:
2025-03-10T21:45:15.946Z
sender:
@oliver.tale-yazdi:parity.io
content:
Is the logic in those channels a generic WASM blob or is it only a predefined set of games?  


id:
2402
timestamp:
2025-03-10T21:52:06.815Z
sender:
@sourabhniyogi:matrix.org
content:
Philip DiSarro: Its really awesome that you took the time!   In the end, we should go back to "show us the JAM service code" ethos to replace the "If this is the case..." -- with JAM it is now obvious many of us will be able to do this with simpler (but less titillating) demos of JAM's CoreVM/CorePlay services involving tiny code bases where experienced people can understand what is going on in less than a couple of minutes rather than take everyone elses word for it =).  

id:
2401
timestamp:
2025-03-10T21:59:15.257Z
sender:
@philipdisarro:matrix.org
content:
The smart contract logic was implemented by hand, not transpiled, because the Doom multiplayer code itself does not prevent no-clipping / speed hacks / most forms of cheating. The Doom smart contracts used in Hydra took the game logic and implemented them into smart contracts which ensured the correctness of game state progression (player & NPC & bullet location / velocity / direction, health, items etc). 

id:
2400
timestamp:
2025-03-10T22:00:12.351Z
sender:
@philipdisarro:matrix.org
content:
* The smart contract logic was implemented by hand, not transpiled, because the Doom code itself does not prevent no-clipping / speed hacks / most forms of cheating. The Doom smart contracts used in Hydra took the game logic and implemented it into smart contracts which ensured the correctness of game state progression (player & NPC & bullet location / velocity / direction, health, items etc). 

id:
2399
timestamp:
2025-03-10T22:00:34.956Z
sender:
@philipdisarro:matrix.org
content:
* The smart contract logic was implemented by hand, not transpiled, because the Doom multiplayer code itself does not prevent no-clipping / speed hacks / most forms of cheating. The Doom smart contracts used in Hydra took the game logic and implemented it into smart contracts which ensured the correctness of game state progression (player & NPC & bullet location / velocity / direction, health, items etc). 

id:
2398
timestamp:
2025-03-10T22:02:39.926Z
sender:
@philipdisarro:matrix.org
content:
Also cheating aside, it wouldn't be possible to transpile, because there is no WASM to PlutusCore compilation pipeline. 

id:
2397
timestamp:
2025-03-10T22:03:16.929Z
sender:
@philipdisarro:matrix.org
content:
I believe they are currently working on a RISC-V compilation pipeline, so once that is released it may be. 

id:
2396
timestamp:
2025-03-10T22:04:48.943Z
sender:
@xlchen:matrix.org
content:
One of the key takeaway of the demo is that JAM is capable to run arbitrary program. Specifically the one not build for blockchains. This is not the case for all existing blockchain platforms.

id:
2395
timestamp:
2025-03-10T22:08:32.053Z
sender:
@philipdisarro:matrix.org
content:
Yes, that's what I gather, the goals were quite different. JAM Doom is not designed to showcase the use of blockchain to ensure the integrity of the game, it was used to illustrate the ability to run arbitrary WASM programs on the blockchain, and to show-case the capacity and bandwidth of Polkadot services. Doom itself would still be vulnerable to the same exploits that the WASM is, but that's beside the point because the goal had nothing to do with Doom in the first place. 

id:
2394
timestamp:
2025-03-10T22:09:12.383Z
sender:
@erin:parity.io
content:
> <@philipdisarro:matrix.org> Yes, that's what I gather, the goals were quite different. JAM Doom is not designed to showcase the use of blockchain to ensure the integrity of the game, it was used to illustrate the ability to run arbitrary WASM programs on the blockchain, and to show-case the capacity and bandwidth of Polkadot services. Doom itself would still be vulnerable to the same exploits that the WASM is, but that's beside the point because the goal had nothing to do with Doom in the first place. 

JAM does not use WASM.

id:
2393
timestamp:
2025-03-10T22:09:24.799Z
sender:
@xlchen:matrix.org
content:
Yes (except WASM wasn't used, it is PVM / RISC-V)

id:
2392
timestamp:
2025-03-10T22:09:29.838Z
sender:
@philipdisarro:matrix.org
content:
It compiles the WASM to RISC-V to Polkadot VM right?

id:
2391
timestamp:
2025-03-10T22:09:40.436Z
sender:
@philipdisarro:matrix.org
content:
Or was the native implementation used in RISC-V?

id:
2390
timestamp:
2025-03-10T22:10:05.623Z
sender:
@erin:parity.io
content:
> <@philipdisarro:matrix.org> It compiles the WASM to RISC-V to Polkadot VM right?

It compiles C directly to RISC-V.

id:
2389
timestamp:
2025-03-10T22:10:13.006Z
sender:
@xlchen:matrix.org
content:
the source code is compiled to RISC-V and then transcoded to PVM bytecode

id:
2388
timestamp:
2025-03-10T22:11:10.086Z
sender:
@philipdisarro:matrix.org
content:
Ah so C to RISC-V to PVM, not WASM. I didn't know there was a RISC-V backend to C that's cool.

id:
2387
timestamp:
2025-03-10T22:13:16.828Z
sender:
@erin:parity.io
content:
You can target C and Rust directly to PVM which is most of the point. Polkadot has been running WASM for years :). The entire idea is you can compile a program which can be targeted to RISC-V and run it directly (yes directly) on JAM.

id:
2386
timestamp:
2025-03-10T22:16:34.321Z
sender:
@erin:parity.io
content:
Perhaps we have different pedantic opinions on what "directly" means, but JAM is not a strictly L1/L2 defined system, it is a secret third thing in the current understanding of how blockchain systems work.

id:
2385
timestamp:
2025-03-10T22:16:57.621Z
sender:
@philipdisarro:matrix.org
content:
directly* as in on an enshrined rollup right, executed by a subset of validators (equivalent to  a shard in sharded networks), not by the L1 validator set unless you "raise an issue" which is an optimistic fault proof type security mechanism right?

id:
2384
timestamp:
2025-03-10T22:18:19.072Z
sender:
@philipdisarro:matrix.org
content:
Or can you just pay more to directly run it with the full base layer validator set?

id:
2383
timestamp:
2025-03-10T22:19:00.403Z
sender:
@xlchen:matrix.org
content:
it is somewhat correct but for most existing optimistic fault proof system, there is no guarantee someone will check it. we have an audit system and with proof that the result will be checked

id:
2382
timestamp:
2025-03-10T22:19:11.742Z
sender:
@emilkietzman:matrix.org
content:
> <@philipdisarro:matrix.org> Ah so C to RISC-V to PVM, not WASM. I didn't know there was a RISC-V backend to C that's cool.

I wrote a thread on this today, there are more languages available

https://x.com/emilkietzman/status/1899048987237224711?s=46

id:
2381
timestamp:
2025-03-10T22:19:20.507Z
sender:
@xlchen:matrix.org
content:
it is possible (but that lose the points)

id:
2380
timestamp:
2025-03-10T22:19:35.034Z
sender:
@erin:parity.io
content:
> <@philipdisarro:matrix.org> directly* as in on an enshrined rollup right, executed by a subset of validators (equivalent to  a shard in sharded networks), not by the L1 validator set unless you "raise an issue" which is an optimistic fault proof type security mechanism right?

It is not optimistic nor is it fault proof based. It is checked every single state transition and this is how finality is reached so quickly on polkadot vs Ethereum. As for the execution, it depends on the service; the service running DOOM here is CoreVM, which is different than polkadot and how polkadot runs parachains.

id:
2379
timestamp:
2025-03-10T22:21:07.007Z
sender:
@philipdisarro:matrix.org
content:
It is still a form of fault proofs right? Someone "raises an issue" claiming that there was fault with the result confirmed by the subset of validators that re-executed the rollup state-commitment, and once an issue is raised, the full security of the L1 consensus is invoked to re-execute the state commitment and determine the correct result?

id:
2378
timestamp:
2025-03-10T22:21:25.873Z
sender:
@philipdisarro:matrix.org
content:
* It is still a form of fault proofs right? Someone "raises an issue" claiming that there was fault with the result confirmed by the subset of validators that re-executed the rollup state-commitment, and once an issue is raised, the full security of the L1 consensus is invoked (all validators) to re-execute the state commitment and determine the correct result?

id:
2377
timestamp:
2025-03-10T22:22:33.511Z
sender:
@philipdisarro:matrix.org
content:
The result will be checked by whom? The subset of validators that re-execute it or the entire L1 validator set, or just a proof that one honest live party checked it? 

id:
2376
timestamp:
2025-03-10T22:23:34.504Z
sender:
@philipdisarro:matrix.org
content:
Is the assigned subset of validators responsible for raising an issue or is it open to all validators in the base layer (even those not assigned in the re-execution subset)?

id:
2375
timestamp:
2025-03-10T22:23:38.204Z
sender:
@erin:parity.io
content:
A group of randomized validators (rotated frequently) checks the STF directly and if it doesn't match, it raises a dispute, which is further checked by all othe validators. This is how polkadot works today. Perhaps people more close to JAM protocol implementation can chime in here.

id:
2374
timestamp:
2025-03-10T22:23:57.127Z
sender:
@erin:parity.io
content:
The rotation is also random.

id:
2373
timestamp:
2025-03-10T22:24:06.091Z
sender:
@erin:parity.io
content:
* A group of randomized validators (rotated frequently) checks the STF directly and if it doesn't match, it raises a dispute, which is further checked by all other validators. This is how polkadot works today. Perhaps people more close to JAM protocol implementation can chime in here.

id:
2372
timestamp:
2025-03-10T22:24:11.542Z
sender:
@xlchen:matrix.org
content:
assume > 1/3 honest validators, a random set of validators will be selected to audit incore executions

id:
2371
timestamp:
2025-03-10T22:25:12.840Z
sender:
@philipdisarro:matrix.org
content:
So the subset needs to come to consensus to raise an issue or any validator in that subset can raise an issue?

id:
2370
timestamp:
2025-03-10T22:25:16.528Z
sender:
@xlchen:matrix.org
content:
with right parameters, we can calculated that the chance of missing proof check

id:
2369
timestamp:
2025-03-10T22:25:48.665Z
sender:
@philipdisarro:matrix.org
content:
Also are we assuming 1/3 honest validators total or 1/3 honest validators in any random subset of size N? 

id:
2368
timestamp:
2025-03-10T22:26:07.716Z
sender:
@philipdisarro:matrix.org
content:
* Also are we assuming 1/3 honest validators total or 1/3 honest validators in most random subsets of size N? 

id:
2367
timestamp:
2025-03-10T22:26:55.124Z
sender:
@philipdisarro:matrix.org
content:
Probabilistically if 33% of total validators are honest, you might randomly have chosen a subset where only 30% of validators are honest will that subset still be able to raise an issue? 

id:
2366
timestamp:
2025-03-10T22:26:57.201Z
sender:
@xlchen:matrix.org
content:
it is a 2 stage thing. there is 1023 validators, 3 validators per core. for anything goes to the chain, it needs 2 signatures for the out of the 3 random selected one. and then another ~30 random validators will be selected to do audit

id:
2365
timestamp:
2025-03-10T22:27:39.233Z
sender:
@xlchen:matrix.org
content:
I didn't do the math but here is a paper showing it is secure. let me see if I can find it

id:
2364
timestamp:
2025-03-10T22:28:08.980Z
sender:
@philipdisarro:matrix.org
content:
So, a single validator cannot raise an issue? It needs to be done via consensus of the subset of randomly selected validators assigned for the audit?

id:
2363
timestamp:
2025-03-10T22:28:29.481Z
sender:
@xlchen:matrix.org
content:
I think it is this one https://eprint.iacr.org/2024/961

id:
2362
timestamp:
2025-03-10T22:29:09.716Z
sender:
@xlchen:matrix.org
content:
I think any single validator can raise issue, but a random set of validators are required to do the check and submit a statement that they did the check

id:
2361
timestamp:
2025-03-10T22:30:51.338Z
sender:
@xlchen:matrix.org
content:
if a work is not required/incentivized, then we can only assume it won't be done

id:
2360
timestamp:
2025-03-10T22:31:10.788Z
sender:
@xlchen:matrix.org
content:
* if a work is not required/incentivized, then we can only assume it won't be done (which is the issue of many existing optimistic system)

id:
2359
timestamp:
2025-03-10T22:32:26.308Z
sender:
@philipdisarro:matrix.org
content:
That's great then, so the assumption is only 1 honest validator in the randomly selected subset.

id:
2358
timestamp:
2025-03-10T22:35:44.270Z
sender:
@xlchen:matrix.org
content:
in order to perform an attack, all 3 validators in the core must be malicious (or 2 malicious one down due to DDOS or some other attack). And then the auditors are still going to identify the bad work report and slash it

id:
2357
timestamp:
2025-03-10T22:36:00.568Z
sender:
@xlchen:matrix.org
content:
* in order to perform an attack, all 3 validators in the core must be malicious (or 2 malicious and last one down due to DDOS or some other attack). And then the auditors are still going to identify the bad work report and slash it

id:
2356
timestamp:
2025-03-10T22:40:58.856Z
sender:
@dave:parity.io
content:
> <@xlchen:matrix.org> in order to perform an attack, all 3 validators in the core must be malicious (or 2 malicious and last one down due to DDOS or some other attack). And then the auditors are still going to identify the bad work report and slash it

This isn't really true; 2 need to be malicious, they can simply not send anything to the third. Though as you say the security comes from auditing anyway

id:
2355
timestamp:
2025-03-10T22:45:50.311Z
sender:
@bkchr:parity.io
content:
> <@bkchr:parity.io> https://x.com/jeffburdges/status/1898690774704771498?s=19 did you read this answer? 

We are turning in cycles 

id:
2354
timestamp:
2025-03-10T22:47:37.642Z
sender:
@philipdisarro:matrix.org
content:
That seems incorrect, if the elves paper is to be believed it does not assume 2/3 honesty (at-least for correctness) it assumes at-least 1 honest party in the fixed size audit committee.

id:
2353
timestamp:
2025-03-10T22:47:58.992Z
sender:
@philipdisarro:matrix.org
content:
Because if one auditor determines fault, they raise a dispute.

id:
2352
timestamp:
2025-03-10T22:50:12.886Z
sender:
@philipdisarro:matrix.org
content:
IE. the same extremely valuable, "secure under the assumption of at-least one live honest actor monitoring the state" assumption of optimistic rollups, except improved with the notion that now there is an incentive scheme for "honest live actors" and that it actually forces at-least N actors (audit committee size) to review the state and made a claim (so if at-least 1 of N of those is honest then security is guaranteed).

id:
2351
timestamp:
2025-03-10T22:52:28.559Z
sender:
@philipdisarro:matrix.org
content:
From that I see though, it appears that only those randomly elected in the subcommittee are able to make a claim (a validator that was not elected in the audit committee or a random third party cannot initiate a dispute). I'm sure it could be extended to allow non-auditors to initiate a dispute, and it was just designed like that for simplicity. 

id:
2350
timestamp:
2025-03-10T22:53:35.726Z
sender:
@dave:parity.io
content:
> <@dave:parity.io> This isn't really true; 2 need to be malicious, they can simply not send anything to the third. Though as you say the security comes from auditing anyway

If you're referring to this then I wasn't talking about the auditing process but the guaranteeing process by which things-to-be-audited are introduced. For auditing yes you need only one of the randomly selected parties to be honest

id:
2349
timestamp:
2025-03-10T22:55:00.703Z
sender:
@dave:parity.io
content:
Moreover the random set cannot be determined in advance, so as an attacker you can't wait until the slim odds are in your favour

id:
2348
timestamp:
2025-03-10T23:02:00.713Z
sender:
@dave:parity.io
content:
> <@philipdisarro:matrix.org> From that I see though, it appears that only those randomly elected in the subcommittee are able to make a claim (a validator that was not elected in the audit committee or a random third party cannot initiate a dispute). I'm sure it could be extended to allow non-auditors to initiate a dispute, and it was just designed like that for simplicity. 

In JAM it is possible for any validator to initiate a dispute. So plausibly if you had spare processing power as a validator you could use it to check reports that you haven't actually been assigned. I expect you wouldn't receive any compensation for this so not sure it would be a great idea

id:
2347
timestamp:
2025-03-10T23:04:30.594Z
sender:
@philipdisarro:matrix.org
content:
I see. The solution in other rollup-centric ecosystems is that the publisher of a state commitment must be bonded (ie. have slashable value locked) and if a third party (not even necessarily validator) decides to review state commitments of a given rollup and detects fraud and submits a successful fraud proof then the publisher of that state commitment has their bond slashed and the party who successfully submitted the fault proof receives a portion of it as a reward.

id:
2346
timestamp:
2025-03-10T23:05:56.187Z
sender:
@philipdisarro:matrix.org
content:
I guess in the above case, you could reward the validator (with spare compute who decided to review the incoming state commitment associated with a rollup that they were particularly interested in) with a portion of the bond only if fraud indeed did occur and the audit committee did not report it (very low chance).

id:
2345
timestamp:
2025-03-10T23:07:46.568Z
sender:
@philipdisarro:matrix.org
content:
A lot of the time though, third parties (non-validators) will likely be reviewing the state commitments of a given rollup anyway for a myriad of reasons (ie. they are an indexer, or web-based blockchain explorer, or dApp, or otherwise running any service that depends on monitoring the state of that rollup).

id:
2344
timestamp:
2025-03-10T23:09:02.400Z
sender:
@dave:parity.io
content:
In JAM validators are both the "publishers of state commitments" _and_ the parties responsible for reviewing these. When acting as publishers we call them guarantors and when acting as reviewers we call them auditors, but they are the same actors. Validators in practice will be slashable, for misbehaviour in either role

id:
2343
timestamp:
2025-03-10T23:10:12.286Z
sender:
@dave:parity.io
content:
I say "in practice" because slashing is not really part of JAM itself but will be managed by a service running on JAM

id:
2342
timestamp:
2025-03-11T07:17:19.475Z
sender:
@xlchen:matrix.org
content:
each work package can have up to 4 work item, and the work items can be different services

id:
2341
timestamp:
2025-03-11T10:51:52.626Z
sender:
@gav:polkadot.io
content:
Well, the point that Elves brings is bounded security.

id:
2340
timestamp:
2025-03-11T10:55:51.991Z
sender:
@gav:polkadot.io
content:
Because there only needs to be one honest auditor to raise a dispute, and because Elves is able to properly reason about the chances that each and every work-report (roll-up state-delta commitment) has been audited by at least one such validator, we are able to reach a statistical certainty over the correctness of these work-reports fast.

id:
2339
timestamp:
2025-03-11T10:56:36.334Z
sender:
@gav:polkadot.io
content:
* Because there only needs to be one honest auditor to raise a dispute, because the fork-choice and finality subsystems are explicitly tied into the disputes mechanism, and because Elves is able to properly reason about the chances that each and every work-report (roll-up state-delta commitment) has been audited by at least one such validator, we are able to reach a statistical certainty over the correctness of these work-reports fast.

id:
2338
timestamp:
2025-03-11T10:57:01.817Z
sender:
@gav:polkadot.io
content:
* Because there only needs to be one honest auditor to raise a dispute, because the fork-choice and finality subsystems are explicitly tied into the disputes mechanism, and because Elves is able to properly reason about the chances that each and every work-report (roll-up state-delta commitment) has been audited by at least one such validator, we are able to reach a statistical certainty over the correctness of all such work-reports fast.

id:
2337
timestamp:
2025-03-11T10:58:00.426Z
sender:
@gav:polkadot.io
content:
* Because there only needs to be one honest auditor to raise a dispute, because the fork-choice and finality subsystems are explicitly tied into the disputes mechanism, and because Elves is able to properly reason about the chances that each and every work-report (roll-up state-delta commitment) has been audited by at least one such validator, we are able to reach a statistical certainty over the correctness of all such work-reports fast and ensure that any finalised chain contains only correct state-changes.

id:
2336
timestamp:
2025-03-11T10:58:26.430Z
sender:
@gav:polkadot.io
content:
* Because there only needs to be one honest auditor to raise a dispute, because the fork-choice and finality subsystems are explicitly tied into the disputes mechanism, and because Elves is able to properly reason about the chances that each and every work-report (roll-up state-delta commitment) has been audited by at least one such validator, we are able to reach a statistical certainty (within economic bounds, as with all crypto-economic systems) over the correctness of all such work-reports fast and ensure that any finalised chain contains only correct state-changes.

id:
2335
timestamp:
2025-03-11T10:58:52.525Z
sender:
@gav:polkadot.io
content:
Fast here means almost always within 8 seconds.

id:
2334
timestamp:
2025-03-11T10:58:55.048Z
sender:
@gav:polkadot.io
content:
* "Fast" here means almost always within 8 seconds.

id:
2333
timestamp:
2025-03-11T10:59:32.757Z
sender:
@gav:polkadot.io
content:
* "Fast" here means generally within 10 seconds after the work was initially done.

id:
2332
timestamp:
2025-03-11T11:00:59.254Z
sender:
@gav:polkadot.io
content:
As Dave says, this only works because it's all happening within a closed network of validators whose behaviour we can reason about and thus can actually model these probabilities correctly.

id:
2331
timestamp:
2025-03-11T11:09:03.797Z
sender:
@gav:polkadot.io
content:
> From my perspective, the goal is not to have all logic ran natively with the full security of the L1

This very much depends on what is meant by "full security". I would argue there's not really a useful way of reasoning about this. The security of an L1, any L1, is not insurmountable. Like all crypto-economic systems, the best we can do is measure *the cost* of breaking it. And there are multiple ways to break a system (censorship, stalling, reversion, invalid state transition, to name just four). The costs of breaking it change between L1s and over the course of time of an L1. So replicating any particular aspect of any particular L1's security guarantees at any particular time is is pretty much meaningless.

id:
2330
timestamp:
2025-03-11T11:09:37.698Z
sender:
@gav:polkadot.io
content:
What is meaningful is to be able to state the actual cost of breaking a system, in what way, and with what assumptions.

id:
2329
timestamp:
2025-03-11T11:17:03.244Z
sender:
@gav:polkadot.io
content:
In any case, Elves is specifically designed not only to construe the security of the L1 to ensure correctness of co-processing units, but to ensure that those who attempt to thwart it are heavily punished.

id:
2328
timestamp:
2025-03-11T11:17:27.445Z
sender:
@gav:polkadot.io
content:
* In any case, Elves is specifically designed not only to construe the security of the L1 to ensure correctness of co-processing units, but also to ensure that those who attempt to thwart it are heavily punished.

id:
2327
timestamp:
2025-03-11T11:17:56.344Z
sender:
@gav:polkadot.io
content:
* Because there only needs to be one honest auditor to raise a dispute, because the fork-choice and finality subsystems are explicitly tied into the disputes mechanism, and because Elves is able to properly reason about the chances that each and every work-report of a block (roll-up state-delta commitment) has been audited by at least one such validator, we are able to reach a statistical certainty (within economic bounds, as with all crypto-economic systems) over the correctness of all such work-reports fast and ensure that any finalised chain contains only correct state-changes.

id:
2326
timestamp:
2025-03-11T11:18:47.006Z
sender:
@gav:polkadot.io
content:
* Because there only needs to be one honest auditor to raise a dispute, because the fork-choice and finality subsystems are explicitly tied into the disputes mechanism, and because Elves is able to properly reason about the chances that each and every work-report of a block (roll-up state-delta commitment) has been audited by at least one such validator, we are able to reach a statistical certainty (within economic bounds, as with all crypto-economic systems) over the correctness of any given block's state-transition fast and ensure that any finalised chain contains only correct state-changes. And we can do this without forcing all validators to compute everything.

id:
2325
timestamp:
2025-03-11T11:19:48.558Z
sender:
@gav:polkadot.io
content:
* > From my perspective, the goal is not to have all logic ran natively with the full security of the L1

This very much depends on what is meant by "full security". I would argue there's not really a useful way of reasoning about this. The security of an L1, any L1, is not insurmountable. Like all crypto-economic systems, the best we can do is measure _the cost_ of breaking it. And there are multiple ways to break a system (censorship, stalling, reversion, invalid state transition, to name just four). The costs of breaking each of these are typically different and change between L1s and over the course of time of an L1. So the concept of inheriting any particular aspect of any particular L1's security guarantees at any particular time is pretty much meaningless.

id:
1116
timestamp:
2025-03-11T17:55:32.644Z
sender:
@gav:polkadot.io
content:
Exactly what the GP specifies.

id:
1115
timestamp:
2025-03-11T17:59:26.685Z
sender:
@gav:polkadot.io
content:
Ahh yes, l subscript should actually be o subscript.

id:
1114
timestamp:
2025-03-11T17:59:35.252Z
sender:
@gav:polkadot.io
content:
* Ahh yes, `l` subscript should actually be `o` subscript.

id:
1113
timestamp:
2025-03-11T17:59:54.311Z
sender:
@gav:polkadot.io
content:
* Ahh yes, `t_l` should actually be `t_o`.

id:
1112
timestamp:
2025-03-11T18:00:03.295Z
sender:
@gav:polkadot.io
content:
* Ahh yes, `t_l` should actually be `t_o`. Feel free to post an issue or make a PR:)

id:
1111
timestamp:
2025-03-11T18:00:15.841Z
sender:
@gav:polkadot.io
content:
* Exactly what the GP specifies. It returns OK, but ultimately will be ignored.

id:
1110
timestamp:
2025-03-11T18:00:26.989Z
sender:
@gav:polkadot.io
content:
* Exactly what the GP specifies. It returns OK (but ultimately will be ignored).

id:
1109
timestamp:
2025-03-11T18:02:21.155Z
sender:
@gav:polkadot.io
content:
This was previously reported and is addressed in `main` branch.

id:
1108
timestamp:
2025-03-11T18:02:47.768Z
sender:
@gav:polkadot.io
content:
* Exactly what the GP specifies. It returns OK but has no effect on the system's state.

id:
1107
timestamp:
2025-03-11T18:11:44.631Z
sender:
@jaymansfield:matrix.org
content:
> <@gav:polkadot.io> Exactly what the GP specifies. It returns OK but has no effect on the system's state.

Thanks!

id:
1106
timestamp:
2025-03-11T18:12:37.784Z
sender:
@jay_ztc:matrix.org
content:
https://github.com/gavofyork/graypaper/pull/284

id:
1105
timestamp:
2025-03-11T18:13:02.850Z
sender:
@ascriv:matrix.org
content:
> <@jay_ztc:matrix.org> Happy Monday folks. In the 'new' host function-> the GP doesn't specify what should happen if the preimage blob length is above the valid range required by the account spec (N_l = 2^32). There is a related case where this type of behavior is specified-> In the bless host call the GP specifies a 'WHO' exit when the values aren't within the valid N_s range. Curious to hear what the guidance is here, happy to open a PR if needed.
> 
> https://graypaper.fluffylabs.dev/#/5f542d7/31db0031e600?v=0.6.2

I made a change which results in a panic if that happens

id:
1104
timestamp:
2025-03-11T18:15:56.347Z
sender:
@jay_ztc:matrix.org
content:
I see it now, thanks!

id:
1103
timestamp:
2025-03-11T22:05:11.589Z
sender:
@ascriv:matrix.org
content:
I see in the on transfer and accumulate invocations we check if a service account’s code hash is “without value” but according to the service account type the code has must have a value. Is the service account type wrong or are these checks wrong?

id:
1102
timestamp:
2025-03-11T22:05:26.139Z
sender:
@ascriv:matrix.org
content:
* I see in the on transfer and accumulate invocations we check if a service account’s code hash is “without value” but according to the service account type the code hash must have a value. Is the service account type wrong or are these checks wrong?

id:
1101
timestamp:
2025-03-12T09:49:57.422Z
sender:
@gav:polkadot.io
content:
what makes you think that the code hash must have a value?

id:
1100
timestamp:
2025-03-12T09:50:13.721Z
sender:
@subotic:matrix.org
content:
The accumulate invocation returns unsigned gas (N_G): https://graypaper.fluffylabs.dev/#/85129da/2e3e002e5a00?v=0.6.3

Shouldn't the return value be signed gas (Z_G), as Psi_M is returning signed gas? The same also for C, where now the input and output should then probably be both Z_G? Or should C instead return 0 for the out-of-gas case?

id:
2324
timestamp:
2025-03-12T10:52:50.780Z
sender:
@alambicco:matrix.org
content:
hi all. Is there a testnet for JAM ?

id:
1099
timestamp:
2025-03-12T12:19:27.119Z
sender:
@ascriv:matrix.org
content:
> <@gav:polkadot.io> what makes you think that the code hash must have a value?

In 9.3 the code hash is of type H, I think if we want it to be able to be valueless it should be of type “H?” ?

id:
1098
timestamp:
2025-03-12T15:41:45.806Z
sender:
@gav:polkadot.io
content:
ahh right, sure the hash is non-empty.

id:
1097
timestamp:
2025-03-12T15:42:27.426Z
sender:
@gav:polkadot.io
content:
* ahh right, sure the hash is non-empty. but there may not be a preimage.

id:
1096
timestamp:
2025-03-12T15:42:38.339Z
sender:
@gav:polkadot.io
content:
regular c and bold c are not the same.

id:
1095
timestamp:
2025-03-12T15:43:50.917Z
sender:
@gav:polkadot.io
content:
Yes indeed. I suspect this may have been incorrectly corrected recently.

id:
1094
timestamp:
2025-03-12T15:43:56.666Z
sender:
@gav:polkadot.io
content:
Feel free to make a PR/issue.

id:
2323
timestamp:
2025-03-12T15:44:07.822Z
sender:
@gav:polkadot.io
content:
Coming soon(tm)

id:
1093
timestamp:
2025-03-12T15:50:45.104Z
sender:
@ascriv:matrix.org
content:
> <@gav:polkadot.io> regular c and bold c are not the same.

Yep, misread as regular c. Thanks 

id:
2322
timestamp:
2025-03-12T16:25:37.960Z
sender:
@alambicco:matrix.org
content:
let's say I want to start building a service ... where to start ?

id:
2321
timestamp:
2025-03-12T16:41:43.224Z
sender:
@sourabhniyogi:matrix.org
content:
https://docs.rs/jam-pvm-common/latest/jam_pvm_common/index.html

id:
1092
timestamp:
2025-03-12T16:49:07.561Z
sender:
@subotic:matrix.org
content:
Sure, for C to return 0 in case of negative gas or for change to Z_G? If change to Z_G, then this will also spill over into Psi_A and the accumulation chapter, where N_G is expected.

id:
2320
timestamp:
2025-03-12T16:51:35.645Z
sender:
@sourabhniyogi:matrix.org
content:
If this seems bizarre, and you want to build user facing applications, with probability 1, there will be a "CoreVM" service written with the above and you just write a normal-ish [Rust/...] program as a guest program of *that* general service serving users.   I believe JAM ran DOOM (and maybe will Quake next haha!) as an illustration of this idea in a proto-CoreVM -- we should all like to know how to demonstrate this with our friends!

id:
2319
timestamp:
2025-03-12T17:57:52.819Z
sender:
@alambicco:matrix.org
content:
is there any resource to understand what a "service" is ? 

id:
2318
timestamp:
2025-03-12T17:58:48.581Z
sender:
@alambicco:matrix.org
content:
from a philosophical and design perspective

id:
2317
timestamp:
2025-03-12T18:34:18.967Z
sender:
@danicuki:matrix.org
content:
https://graypaper.fluffylabs.dev/#/85129da/0ba2000ba200?v=0.6.3

id:
2316
timestamp:
2025-03-12T18:57:17.198Z
sender:
@alambicco:matrix.org
content:
ok tahnks

id:
2315
timestamp:
2025-03-12T22:32:11.196Z
sender:
@ycc3741:matrix.org
content:
I’m very curious about how JAM runs DOOM without delay.
What I mean is that JAM itself has a six-second finalization time.
So my inputs go through:
Input (mouse or keyboard) → corevm → JAM → corevm → Output (monitor)
Logically, wouldn’t that result in a six-second delay?

id:
2314
timestamp:
2025-03-12T22:33:19.158Z
sender:
@ycc3741:matrix.org
content:
Or did I misunderstand something?

id:
2313
timestamp:
2025-03-12T22:40:04.302Z
sender:
@sourabhniyogi:matrix.org
content:
My guess is that the DOOM demo has the "user" computing the work packages FIRST (and as a result, knowing the exact exportCount), and then what you see in the demo is the stream of work packages submitted by the user into a core (or some number of them), the "frames" going through DA, and the claim that "JAM is running DOOM" is actually "JAM is doing guaranteeing/assuring/auditing AFTER the user has submitted their work package" -- its a streaming demo and a test of N cores doing (a) DA throughput (of the "frames") (b) the same computational process in refine that the user.  So, no jerkiness, but yes, a 6-12 second delay technically between what the user has done in creating N exported segments (representing video frames) and polkavm replicating the same computation.

Just a guess, hope someone can correct it 

id:
2312
timestamp:
2025-03-12T22:41:55.206Z
sender:
@ycc3741:matrix.org
content:
yeah

id:
2311
timestamp:
2025-03-12T22:42:14.107Z
sender:
@ycc3741:matrix.org
content:
there should not be jerkiness

id:
2310
timestamp:
2025-03-12T22:42:37.300Z
sender:
@ycc3741:matrix.org
content:
but lantency might be a issue QQ

id:
2309
timestamp:
2025-03-12T22:44:31.710Z
sender:
@ycc3741:matrix.org
content:
Because when I saw Dr. Wood's demonstration, he didn't directly play DOOM but instead showcased the output. So, I'm not sure how the delay issue was handled.

id:
2308
timestamp:
2025-03-12T22:45:24.658Z
sender:
@ascriv:matrix.org
content:
Maybe more accurate to say it ran some tool-assisted gameplay playback

id:
2307
timestamp:
2025-03-12T22:46:17.134Z
sender:
@sourabhniyogi:matrix.org
content:
Its just a playback of the user-generated work packages from a real user (or a bot, doesn't matter) done at some time before.  

id:
2306
timestamp:
2025-03-12T22:47:44.118Z
sender:
@ycc3741:matrix.org
content:
yeah. I think so, too. 

id:
2305
timestamp:
2025-03-12T22:50:15.479Z
sender:
@sourabhniyogi:matrix.org
content:
You can eliminate the user input with https://playgameoflife.com/ - we'll have this as a corevm-like refine test vector dataset that is easy to turn into a hellow world and more exciting than likecomputing primes.  Not a sexy as shooting things though. 

id:
2304
timestamp:
2025-03-12T22:51:12.008Z
sender:
@sourabhniyogi:matrix.org
content:
* You can eliminate the user input with https://playgameoflife.com/ - we'll have this as a corevm-like refine test vector dataset that is easy to turn into a hellow world and more exciting than likecomputing primes.  Not as sexy as shooting things though. 

id:
2303
timestamp:
2025-03-13T10:15:26.344Z
sender:
@ultracoconut:matrix.org
content:
Is posible to do a mini Jam toaster with Raspberry pi?

id:
2302
timestamp:
2025-03-13T10:16:18.601Z
sender:
@ultracoconut:matrix.org
content:
1000001546.webp

id:
2301
timestamp:
2025-03-13T10:19:37.933Z
sender:
@ultracoconut:matrix.org
content:
I guess the arm architecture is a problem🤔

id:
2300
timestamp:
2025-03-13T10:23:33.565Z
sender:
@ultracoconut:matrix.org
content:
* Is possible to build a mini Jam toaster with Raspberry pi?

id:
2299
timestamp:
2025-03-13T10:39:58.591Z
sender:
@danicuki:matrix.org
content:
The architecture should not be a problem. If it is a smaller validator set, with less capacity, I believe it is possible to run. The assumed node capacity for a full network is "a modern 16 core cpu with 64gb ram, 8tb secondary storage and 0.5gbe networking." 

id:
2298
timestamp:
2025-03-13T11:06:36.428Z
sender:
@ultracoconut:matrix.org
content:
> <@danicuki:matrix.org> The architecture should not be a problem. If it is a smaller validator set, with less capacity, I believe it is possible to run. The assumed node capacity for a full network is "a modern 16 core cpu with 64gb ram, 8tb secondary storage and 0.5gbe networking." 

Great!

id:
2297
timestamp:
2025-03-13T11:40:57.582Z
sender:
@wabkebab:matrix.org
content:
this could be a good open source hacker/enthusiast project

id:
2296
timestamp:
2025-03-13T11:41:05.798Z
sender:
@wabkebab:matrix.org
content:
* this could be a good open-source hacker/enthusiast project

id:
2295
timestamp:
2025-03-13T11:44:29.345Z
sender:
@jan:parity.io
content:
Would be way too slow for production, but as a fun test net, sure.

id:
2294
timestamp:
2025-03-13T12:00:48.020Z
sender:
@ultracoconut:matrix.org
content:
It is for fun🥳 Could 6 Raspberry Pi 5s 16gb ram model run Jam faster than a single Mac Notebook?🤔

id:
1091
timestamp:
2025-03-13T12:17:47.313Z
sender:
@gav:polkadot.io
content:
Yes, in fact it was confusing the gas used (which is the PVM gas counter and can be negative in the case of an underrun) with gas remaining (which is the value used by the higher level accumulation functions). Should make more sense with https://github.com/gavofyork/graypaper/pull/288.

id:
1090
timestamp:
2025-03-13T12:18:06.233Z
sender:
@gav:polkadot.io
content:
* Yes, in fact it was confusing the gas remaining (which is the PVM gas counter and can be negative in the case of an underrun) with gas used (which is the value used by the higher level accumulation functions). Should make more sense with https://github.com/gavofyork/graypaper/pull/288.

id:
1089
timestamp:
2025-03-13T12:18:25.204Z
sender:
@gav:polkadot.io
content:
* Yes, in fact it was confusing the gas remaining (which is the PVM gas counter and can be negative in the case of an underrun) with gas used (which is the value used by the higher level accumulation functions and cannot be negative). Should make more sense with https://github.com/gavofyork/graypaper/pull/288.

id:
2293
timestamp:
2025-03-13T12:23:56.720Z
sender:
@gav:polkadot.io
content:
> Logically, wouldn’t that result in a six-second delay?

Substantially more than a 6-second delay; the Work Package must first be computed, send to a guarantor; two of them must then compute the work-report and send it to the author; they must then place the report in the block and accumulate it.

id:
2292
timestamp:
2025-03-13T12:24:19.908Z
sender:
@gav:polkadot.io
content:
* > Logically, wouldn’t that result in a six-second delay?

Substantially more than a 6-second delay; the Work Package must first be computed, send to a guarantor; two of them must then compute the work-report and send it to the author; they must then place the report in the block and accumulate it. Overall the pipeline will normally take somewhere between 10-15 seconds.

id:
2291
timestamp:
2025-03-13T12:25:11.358Z
sender:
@gav:polkadot.io
content:
So if you wanted to actually run DOOM *through* JAM and watch JAM's output (i.e. the display) before making your next input (i.e. shooting), you'd experience a crazy amount of lag which would make a game like DOOM unplayable.

id:
2290
timestamp:
2025-03-13T12:26:03.216Z
sender:
@gav:polkadot.io
content:
However, it's conceivable that certain kinds of games (e.g. turn-based, card games &c.) could make use of a pipeline with this kind of latency.

id:
2289
timestamp:
2025-03-13T12:27:00.407Z
sender:
@gav:polkadot.io
content:
The demo was to show off the generality, computation & DA capabilities of JAM. JAM is not going to put Nintendo out of business.

id:
2288
timestamp:
2025-03-13T12:27:59.275Z
sender:
@gav:polkadot.io
content:
JAM could also possibly be "overclocked" (i.e. have a reduced block-time), which would further increase playability of games needing lower latency.

id:
2287
timestamp:
2025-03-13T12:28:52.314Z
sender:
@gav:polkadot.io
content:
And, of course, nothing stops you from peeking at DOOM's output at the package-builder stage and devising sensible inputs "just-in-time" then.

id:
2286
timestamp:
2025-03-13T12:30:20.552Z
sender:
@gav:polkadot.io
content:
* And, of course, nothing stops you from peeking at DOOM's output at the package-builder stage and devising sensible inputs "just-in-time" then. That wouldn't involve JAM as a critical part of the "look-aim-shoot" cycle, but would utilize JAM as a guarantor of game-correctness and could be quite an interesting possibility for things like gaming tournaments where you want players' (inter-)actions checked by a neutral platform.

id:
2285
timestamp:
2025-03-13T12:32:50.641Z
sender:
@gav:polkadot.io
content:
We're already preparing the tooling and code so that anyone with a vaguely viable M2 implementation will be able to demo it on their own testnet.

id:
2284
timestamp:
2025-03-13T12:33:15.521Z
sender:
@gav:polkadot.io
content:
For us, it runs fine in the PVM interpreter; no recompiler needed.

id:
2283
timestamp:
2025-03-13T12:33:42.049Z
sender:
@gav:polkadot.io
content:
* > Logically, wouldn’t that result in a six-second delay?

Substantially more than a 6-second delay; the Work Package must first be computed, sent to a guarantor; two of them must then compute the work-report and send it to the author; they must then place the report in the block and accumulate it. Overall the pipeline will normally take somewhere between 10-15 seconds.

id:
2282
timestamp:
2025-03-13T12:35:15.717Z
sender:
@gav:polkadot.io
content:
* > Logically, wouldn’t that result in a six-second delay?

Substantially more than a 6-second delay; the Work Package must first be computed, sent to a guarantor; two of them must then compute the work-report and send it to the author; they must then place the report in the block and accumulate it. Overall the pipeline will normally take somewhere around 12 seconds, though if refinement is very fast and you get lucky on the block phase and there's not much to accumulate, it could be less than a second.

id:
2281
timestamp:
2025-03-13T12:35:41.133Z
sender:
@gav:polkadot.io
content:
* > Logically, wouldn’t that result in a six-second delay?

Substantially more than a 6-second delay; the Work Package must first be computed, sent to a guarantor; two of them must then compute the work-report and send it to the author; they must then place the report in the block and accumulate it. At peak usage the pipeline will normally take somewhere around 12-18 seconds, though if refinement is very fast and you get lucky on the block phase and there's not much to accumulate, it could be less than a second.

id:
2280
timestamp:
2025-03-13T12:36:09.915Z
sender:
@gav:polkadot.io
content:
* > Logically, wouldn’t that result in a six-second delay?

Substantially more than a 6-second delay; the Work Package must first be computed, sent to a guarantor; two of them must then compute the work-report and send it to the author; they must then place the report in the block and accumulate it. At peak usage the pipeline will normally take somewhere around 12-18 seconds, though if refinement is very fast, you have core affinity, you get lucky on the block phase and there's not much to accumulate, it could be less than a second.

id:
2279
timestamp:
2025-03-13T12:36:43.033Z
sender:
@gav:polkadot.io
content:
* So if you wanted to actually run DOOM _through_ JAM and watch JAM's output (i.e. the display) before making your next input (i.e. shooting), you'd probably experience too much lag for an enjoyable session on a game like DOOM.

id:
2278
timestamp:
2025-03-13T12:38:23.414Z
sender:
@gav:polkadot.io
content:
I'd recommend you watch one of my recent talks; I spend some time talking about services.

id:
2277
timestamp:
2025-03-13T12:39:14.639Z
sender:
@gav:polkadot.io
content:
You can think of a service as a highly-parallelisable, pipelined and high-throughput smart-contract.

id:
2276
timestamp:
2025-03-13T12:40:18.575Z
sender:
@gav:polkadot.io
content:
It has a somewhat atypical multi-stage execution model, which makes it highly optimal to be executed on a decentralised, distributed platform like JAM.

id:
2275
timestamp:
2025-03-13T12:41:14.027Z
sender:
@gav:polkadot.io
content:
As sourabhniyogi says, writing a service directly is probably not particularly sensible for anyone wanting to make anything user-facing.

id:
2274
timestamp:
2025-03-13T12:41:57.137Z
sender:
@gav:polkadot.io
content:
You can think of JAM as the decentralised equivalent of a bare-metal supercomputer.

id:
2273
timestamp:
2025-03-13T12:43:02.759Z
sender:
@gav:polkadot.io
content:
Services are the basic operating systems that sit on top of JAM. Most of them will allow programmable code to be run (like DOS, Windows, Linux &c). A few might be highly domain-specific with fixed-functionality (e.g. a storage or bridge service).

id:
2272
timestamp:
2025-03-13T12:44:08.243Z
sender:
@gav:polkadot.io
content:
* Services are the basic operating systems that sit on top of JAM. Most of them will allow programmable code to be "loaded" and executed in richer or more "normal" execution model (much like like DOS, Windows, Linux &c provide a richer execution environment for software to run than the bare metal of a machine). A few might be highly domain-specific with fixed-functionality (e.g. a storage or bridge service).

id:
2271
timestamp:
2025-03-13T12:44:13.492Z
sender:
@gav:polkadot.io
content:
* Services are the basic operating systems that sit on top of JAM. Most of them will allow programmable code to be "loaded" and executed in richer or more "normal" execution model (much like like DOS, Windows, Linux &c provide a richer execution environment for software to run than the bare metal of a machine). A few services might be highly domain-specific with fixed-functionality (e.g. a storage or bridge service).

id:
2270
timestamp:
2025-03-13T12:45:29.624Z
sender:
@gav:polkadot.io
content:
We can already see some services like CoreVM which provides a docker-like continuous execution VM environment, or CoreChains which provides an environment for L2 blockchains to execute.

id:
2269
timestamp:
2025-03-13T12:45:46.052Z
sender:
@gav:polkadot.io
content:
* We can already see some services like CoreVM which provides a docker-like continuous execution VM environment, and imagine a CoreChains service which provides an environment for L2 blockchains to execute.

id:
2268
timestamp:
2025-03-13T12:45:53.132Z
sender:
@gav:polkadot.io
content:
* We can already see some services like CoreVM which provides a docker-like continuous execution VM environment, and imagine a CoreChains service which provides an environment for L2 blockchains to execute, much like Polkadot's Relay-chain.

id:
2267
timestamp:
2025-03-13T12:46:36.651Z
sender:
@gav:polkadot.io
content:
Much like there are far fewer operating systems than there are pieces of software which run on them, we'll likely not see so many services compared to the amount of software deployed on them.

id:
2266
timestamp:
2025-03-13T12:46:59.129Z
sender:
@gav:polkadot.io
content:
* Much like there are far fewer operating systems than there are pieces of software which run on them, we'll likely not see so many services compared to the amount of software deployed on them (smart contracts, CoreVM inner software, L2 chains).

id:
2265
timestamp:
2025-03-13T12:47:13.859Z
sender:
@gav:polkadot.io
content:
* Much like there are far fewer operating systems than there are pieces of software which run on them, we'll likely not see so many services compared to the amount of software deployed on to them (smart contracts, CoreVM inner software, L2 chains).

id:
2264
timestamp:
2025-03-13T12:49:44.911Z
sender:
@gav:polkadot.io
content:
* > Logically, wouldn’t that result in a six-second delay?

Probably more than a 6-second delay; the Work Package must first be computed, sent to a guarantor; two of them must then compute the work-report and send it to the author; they must then place the report in the block and accumulate it. At peak usage the pipeline will normally take somewhere around 12-18 seconds, though if refinement is very fast, you have core affinity, you get lucky on the block phase and there's not much to accumulate, it could be less than a second.

id:
2263
timestamp:
2025-03-13T20:56:25.437Z
sender:
@xlchen:matrix.org
content:
where can I find the service code showed in the JAM talk?

id:
2262
timestamp:
2025-03-13T22:34:24.253Z
sender:
@ultracoconut:matrix.org
content:
> <@danicuki:matrix.org> The architecture should not be a problem. If it is a smaller validator set, with less capacity, I believe it is possible to run. The assumed node capacity for a full network is "a modern 16 core cpu with 64gb ram, 8tb secondary storage and 0.5gbe networking." 

I think I said it wrong. What I wanted to build was actually a core jam. One validator per raspberry pi.

id:
2261
timestamp:
2025-03-13T22:35:49.959Z
sender:
@xlchen:matrix.org
content:
for a small testnet with reduced gas limit and related parameters, I see no reason why it wouldn't work

id:
2260
timestamp:
2025-03-14T08:55:22.256Z
sender:
@olanod:virto.community
content:
A small hardware can be a helpful development tool, a tiny rpi powered testnet sounds good, like a plug and deploy device? To be familiar with bare metal RV64 for service development I'm also thinking of a smaller "service emulator". e.g. this [board](https://wiki.sipeed.com/hardware/en/maix/m1s/m1s_dock.html) I got with a RV64GCV core at 480MHz will be helpful while developing a service.

id:
2259
timestamp:
2025-03-14T09:02:06.561Z
sender:
@olanod:virto.community
content:
And speaking of services, JAM will be an interesting platform for new kinds of operating systems. If anyone has any thoughts, I wrote a bit about [VOS](https://forum.polkadot.network/t/virtual-operating-system-for-jam-program-with-wink-and-zink/11973), an OS to run WASI programs, the idea is to appeal to non blockchain developers and attract them to our ecosystem(e.g. the many Rust devs that hate to just hear the word blockchain mentioned ;P)

id:
2258
timestamp:
2025-03-14T09:09:56.470Z
sender:
@jan:parity.io
content:
We're not bound by WASM's limitations, so you don't necessarily need WASI here. With a proper libc port and a service that emulates a subset of Linux syscalls you could run essentially any normal program unmodified just by recompiling it. (Of course that'd probably not be very useful because you still need JAM-specific I/O to do anything useful.)

id:
2257
timestamp:
2025-03-14T09:25:00.219Z
sender:
@olanod:virto.community
content:
What I like about WASI is the ecosystem, it will have better tooling, compiler support, etc. Wouldn't it be great if we can mix the good parts of WASI with PVM? the SI without the WA ... wasmtime supports no_std not too long ago, the prerequisite is to precompile WASM ahead of time to the target architecture, I think that limitation could be an advantage as we could transpile WASM to PVM so no WASM interpreter is used in the JAM service ... haven't got there yet but it's worth a try ;)

id:
2256
timestamp:
2025-03-14T09:39:02.110Z
sender:
@olanod:virto.community
content:
I find it very convenient to enable a regular rust program with a main function that reads from stdin or a file and writes an output with regular os primitives for example, any non blockchain dev can do that, it's a program that can be `cargo run` without any extra set up. Wouldn't it be great to enable this kind of development? We could try to contribute a standard library to rust for a `riscv32emac-unknown-someos-polkavm` target but if somebody already did the work with WASI why not use that instead? 

id:
2255
timestamp:
2025-03-14T09:49:10.907Z
sender:
@olanod:virto.community
content:
like a fake libc, VOS would translate things like reading/writing from files into things that could make sense in the context of JAM, if like Linux we abuse the *everything is a file* to expose JAM I/O as "files" then it can become useful :)

id:
2254
timestamp:
2025-03-14T09:59:06.438Z
sender:
@ultracoconut:matrix.org
content:
> <@olanod:virto.community> A small hardware can be a helpful development tool, a tiny rpi powered testnet sounds good, like a plug and deploy device? To be familiar with bare metal RV64 for service development I'm also thinking of a smaller "service emulator". e.g. this [board](https://wiki.sipeed.com/hardware/en/maix/m1s/m1s_dock.html) I got with a RV64GCV core at 480MHz will be helpful while developing a service.

Great idea to use risc-v hardware. The Core Jam Pi could be a cluster of 3 Raspberry Pi 5s  16 GB RAM model, each with its own 1TB NVME SSD. Each would function as a validator. What do you think?

id:
2253
timestamp:
2025-03-14T10:06:47.209Z
sender:
@jan:parity.io
content:
Yes, that's literally what a libc port + emulation service would do, but without the WASM -> PVM recompilation step. It is a tradeoff - with a WASM -> PVM recompiler you could use existing WASI backends, but you'd get more bloated binaries and worse performance; with a libc port you'd have to reconfigure each compiler to use it (which is extra work, although it's relatively easy to do) but you'd get higher performance and smaller binaries.

id:
2252
timestamp:
2025-03-14T10:07:16.254Z
sender:
@jan:parity.io
content:
But, in general, I'd love to see a WASM -> PVM recompiler. If you write one please make sure to ping me and let me know. (:

id:
2251
timestamp:
2025-03-14T10:23:18.372Z
sender:
@ultracoconut:matrix.org
content:
1000001561.jpg

id:
2250
timestamp:
2025-03-14T12:56:41.528Z
sender:
@olanod:virto.community
content:
I'll first focus on the developer experience, i.e. wink!(we'll use it for regular backend services) but will see what can be done with Cranelift that supports riscv64. Any special considerations you foresee? 

id:
2249
timestamp:
2025-03-14T12:57:46.949Z
sender:
@jan:parity.io
content:
You won't be able to use Cranelift, as it doesn't support rv64e.

id:
2248
timestamp:
2025-03-14T13:00:10.091Z
sender:
@jan:parity.io
content:
You'd have to add support for rv64e to Cranelift (which probably shouldn't be too hard, as it's mostly just forcing it to not use the upper 16 registers), but even then it probably won't be a simple job to emit something that can be turned into PVM bytecode (you need to use a very specific memory model with full relocations, and I have no idea whether Cranelift supports all that).

id:
2247
timestamp:
2025-03-14T13:00:54.240Z
sender:
@jan:parity.io
content:
A much simpler way to do it would be to go through the wasm2c route, that is, convert the WASM bytecode into Rust or C and compile it with a normal compiler.

id:
2246
timestamp:
2025-03-14T13:01:47.260Z
sender:
@olanod:virto.community
content:
thanks that's helpful!

id:
2245
timestamp:
2025-03-14T13:34:36.812Z
sender:
@jay_ztc:matrix.org
content:
also related:

https://www.sifive.com/boards

https://github.com/chipsalliance/rocket-chip (much lower layer than our use case but love to see this direction in the hdl scene)

id:
2244
timestamp:
2025-03-14T13:38:28.984Z
sender:
@jay_ztc:matrix.org
content:
makes me wonder if there's any gas-metering designs on opencores, integrating this into rocket core would be sick

id:
2243
timestamp:
2025-03-14T13:38:42.586Z
sender:
@jay_ztc:matrix.org
content:
* makes me wonder if there's any gas-metering designs on opencores, integrating one into rocket core would be sick

id:
2242
timestamp:
2025-03-14T13:39:42.886Z
sender:
@jay_ztc:matrix.org
content:
* makes me wonder if there's any gas-metering designs on opencores, integrating one into rocket core would be sick (bit beyond this specific scope, but cool to dream about)

id:
2241
timestamp:
2025-03-14T15:57:38.015Z
sender:
@ultracoconut:matrix.org
content:
I have more ideas for marketing the Core Jam Pi. If anyone is interested, send me a DM on Twitter.✌️

id:
1088
timestamp:
2025-03-15T19:11:07.845Z
sender:
@ascriv:matrix.org
content:
In the outer accumulation function, is it intended that the free accumulation services dict be zeroed out after the first iteration? Couldn’t there be work reports in the remaining ones which was made by a free accumulation service?

id:
2240
timestamp:
2025-03-16T10:20:18.024Z
sender:
@ultracoconut:matrix.org
content:
Well, I have an update on the concept. Mermelada cubes. Each cube is a Jam core pi. You can add cores by connecting cubes.This is useful if your application needs more than one core.

id:
2239
timestamp:
2025-03-16T10:23:35.760Z
sender:
@ultracoconut:matrix.org
content:
For example, you can create your rollup and test it in 3 local physical cores.

id:
1087
timestamp:
2025-03-17T04:41:10.005Z
sender:
@clearloop:matrix.org
content:
* hey teams, curious about in your implementations, after genesis, can you ensure there will be no empty slots in the local testnet? e.g.

there are valid blocks on each of the time slots and all of them get full finalized with all of the nodes within 6 seconds 

id:
1086
timestamp:
2025-03-17T04:43:30.652Z
sender:
@clearloop:matrix.org
content:
* hey teams, curious about in your implementations, after genesis, can you ensure there will be no empty slots in the local testnet? e.g. there are valid blocks on each of the time slots and all of them get full finalized with all of the nodes within 6 seconds

id:
1085
timestamp:
2025-03-17T05:22:45.103Z
sender:
@0xjunha:matrix.org
content:
My understanding is that the always-accumulate services are privileged to be processed first, followed by other services. And there is enough gas to run all the always-accumulate services in the initial round, so those getting accumulated later than the initial round would never happen.

id:
1084
timestamp:
2025-03-17T05:28:22.333Z
sender:
@0xjunha:matrix.org
content:
* My understanding is that the always-accumulate services should be processed first, followed by other services. And there is enough gas to run all the always-accumulate services in the initial round, so those getting accumulated later than the initial round would never happen.

id:
2238
timestamp:
2025-03-17T08:03:57.064Z
sender:
@clearloop:matrix.org
content:
hey teams, may I ask if you are requesting the full blocks right after receive its valid header?

I found that for only storing headers & communicating with neighbours, we may miss sort of blocks when we need for authoring blocks, e.g. after receiving a valid block header, as a validator, we announce it & request the full block data

id:
2237
timestamp:
2025-03-17T08:05:24.632Z
sender:
@clearloop:matrix.org
content:
* hey teams, may I ask if you are requesting the full blocks right after receive its valid header?

I found that for only storing headers & communicating with neighbours, we may miss sort of blocks when are about to author blocks (in tiny, 3 of the neighours only have the headers, they may do best chain selection later than, but when they got the expected block right after we authoring a new block), e.g. after receiving a valid block header, as a validator, we announce it & request the full block data

id:
2236
timestamp:
2025-03-17T08:05:46.752Z
sender:
@clearloop:matrix.org
content:
* hey teams, may I ask if you are requesting the full blocks right after receive its valid header?

I found that for only storing headers & communicating with neighbours, we may miss sort of blocks when are about to author blocks (in tiny, 3 of the neighours only have the headers, they may do best chain selection later than, but when they got the expected block right after we authoring a new block), e.g. 

after receiving a valid block header, as a validator: we announce it & request the full block data

id:
2235
timestamp:
2025-03-17T08:05:59.351Z
sender:
@clearloop:matrix.org
content:
* hey teams, may I ask if you are requesting the full blocks right after receiving their valid header?

I found that for only storing headers & communicating with neighbours, we may miss sort of blocks when are about to author blocks (in tiny, 3 of the neighours only have the headers, they may do best chain selection later than, but when they got the expected block right after we authoring a new block), e.g. 

after receiving a valid block header, as a validator: we announce it & request the full block data

id:
2234
timestamp:
2025-03-17T08:06:05.254Z
sender:
@clearloop:matrix.org
content:
* hey teams, may I ask if you are requesting the full blocks right after receiving their valid headers?

I found that for only storing headers & communicating with neighbours, we may miss sort of blocks when are about to author blocks (in tiny, 3 of the neighours only have the headers, they may do best chain selection later than, but when they got the expected block right after we authoring a new block), e.g. 

after receiving a valid block header, as a validator: we announce it & request the full block data

id:
2233
timestamp:
2025-03-17T08:06:44.163Z
sender:
@clearloop:matrix.org
content:
* hey teams, may I ask if you are requesting the full blocks right after receiving their valid headers?

I found that for only storing headers & communicating with neighbours, we may miss sort of blocks when are about to author blocks (in tiny network, 3 of the neighours only have the headers, they may do best chain selection later than, but when they may get the expected block right after we authoring a new block), e.g. 

after receiving a valid block header, as a validator: we announce it & request the full block data

id:
2232
timestamp:
2025-03-17T08:07:31.108Z
sender:
@clearloop:matrix.org
content:
* hey teams, may I ask if you are requesting the full blocks right after receiving their valid headers?

I found that for only storing headers & communicating with neighbours, we may miss sort of blocks when are about to author blocks (in tiny network, 3 of the neighours only have the headers, they may do best chain selection later than, but when they get the expected block the current node may have already authored a new block), e.g. 

after receiving a valid block header, as a validator: we announce it & request the full block data

id:
2231
timestamp:
2025-03-17T08:09:54.807Z
sender:
@clearloop:matrix.org
content:
* hey teams, may I ask if you are requesting the full blocks right after receiving their valid headers?

I found that for only storing headers & communicating with neighbours, we may miss sort of blocks when are about to author blocks (in tiny network, 3 of the neighours only have the headers, they may do best chain selection later than, but when they get the expected block the current node may have already authored a new block), and then, fork occurs even in my local testnet ))

e.g. after receiving a valid block header, as a validator: we announce it & request the full block data

id:
2230
timestamp:
2025-03-17T08:13:55.933Z
sender:
@clearloop:matrix.org
content:
* hey teams, may I ask if you are requesting the full blocks right after receiving their valid headers?

I found that for only storing headers & communicating with neighbours, we may miss sort of blocks when are about to author blocks (in tiny network, 3 of the neighours only have the headers, they may do best chain selection later then, but when they get the expected block the current node may have already authored a new block), and then, fork occurs even in my local testnet ))

e.g. after receiving a valid block header, as a validator: we announce it & request the full block data

id:
2229
timestamp:
2025-03-17T09:16:24.040Z
sender:
@clearloop:matrix.org
content:
okay we just got it is caused by our implementation of ce128

id:
1083
timestamp:
2025-03-17T10:04:45.850Z
sender:
@prematurata:matrix.org
content:
I think I've found something interesting when handling the `ecalli` pvm fn. According to the gp `ΨH|A.34` calls `Ψ()|A.1` and handles the  `ε′ = hxh`. 

The `ecalli` fn ( https://graypaper.fluffylabs.dev/#/85129da/25ff0025ff00?v=0.6.3 )  is set to modify just `ε` leaving the default `ı′ = ı + 1 + skip(ı)` defined in A.7 in place. 

Then the host call gets executed inside `ΨH|A.34`. If all goes ok and `f` returs (▸, ....) then we should recursively call `ΨH` but with `ı′′` which is defined as `ı′′ = ı′ + 1 + skip(ı′)`. Now since `i'` is what is being retrned by `Ψ` and is already skipping an instruction, then `ΨH` gets called by skipping one instruction (due to the 2x skips being applied).

Now i think this is either a misinterpretation of mine or an error in the graypaper as in my point of view it makes no sense to skip 1 instruction after a successful hostcall execution via ecalli. Multiple implementations seems to agree with this considering there are multiple implementors passing the duna testnet.

id:
1082
timestamp:
2025-03-17T10:06:07.003Z
sender:
@prematurata:matrix.org
content:
* I think I've found something interesting when handling the `ecalli` pvm fn. According to the gp `ΨH|A.34` calls `Ψ()|A.1` and handles the  `ε′ = hxh`.

The `ecalli` fn ( https://graypaper.fluffylabs.dev/#/85129da/25ff0025ff00?v=0.6.3 )  is set to modify just `ε` leaving the default `ı′ = ı + 1 + skip(ı)` defined in A.7 in place.

Then the host call gets executed inside `ΨH|A.34`. If all goes ok and `f` returs (▸, ....) then we should recursively call `ΨH` but with `ı′′` which is defined as `ı′′ = ı′ + 1 + skip(ı′)`. Now since `i'` is what is being retrned by `Ψ` and is already skipping an instruction, then `ΨH` gets called by skipping another instruction (due to the 2x skips being applied).

Now i think this is either a misinterpretation of mine or an error in the graypaper as in my point of view it makes no sense to skip 1 instruction after a successful hostcall execution via ecalli. Multiple implementations seems to agree with this considering there are multiple implementors passing the duna testnet.

id:
1081
timestamp:
2025-03-17T10:52:50.527Z
sender:
@prematurata:matrix.org
content:
* I think I've found something interesting when handling the `ecalli` pvm fn. According to the gp `ΨH|A.34` calls `Ψ()|A.1` and handles the  `ε′ = hxh`.

The `ecalli` fn ( https://graypaper.fluffylabs.dev/#/85129da/25ff0025ff00?v=0.6.3 )  is set to modify just `ε` leaving the default `ı′ = ı + 1 + skip(ı)` defined in A.7 in place.

Then the host call gets executed inside `ΨH|A.34`. If all goes ok and `f` returns (▸, ....) then we should recursively call `ΨH` but with `ı′′` which is defined as `ı′′ = ı′ + 1 + skip(ı′)`. Now since `i'` is what is being returned by `Ψ` and is already skipping an instruction, then `ΨH` gets called by skipping another instruction (due to the 2x skips being applied).

Now i think this is either a misinterpretation of mine or an error in the graypaper as in my point of view it makes no sense to skip 1 instruction after a successful hostcall execution via ecalli. Multiple implementations seems to agree with this considering there are multiple implementors passing the duna testnet.

id:
2228
timestamp:
2025-03-17T13:38:50.822Z
sender:
@mnaamani:matrix.org
content:
is Jam prize rule no.20 - "Prizes are paid to the earliest Polkadot/Kusama account IDs stated in the repository's README. In the case of a tie, payment is split equally" Does that mean that a prize is only allocated to a specific language?

So if I'm considering starting a new implementation it would make more sense to pick a language that is not many teams are currently working on?

id:
2227
timestamp:
2025-03-17T13:40:00.932Z
sender:
@mnaamani:matrix.org
content:
* from Jam prize rule no.20 - "Prizes are paid to the earliest Polkadot/Kusama account IDs stated in the repository's README. In the case of a tie, payment is split equally" - Does that mean that a prize is only allocated to a specific language?

So if I'm considering starting a new implementation, would it make more sense to pick a language that not many teams are currently working on?

id:
1080
timestamp:
2025-03-17T14:50:06.434Z
sender:
@gav:polkadot.io
content:
You mean this, right? https://github.com/gavofyork/graypaper/issues/247

id:
1079
timestamp:
2025-03-17T14:51:13.829Z
sender:
@prematurata:matrix.org
content:
yes. i didnt see this issue daaamn. but essentially yeah its 247

id:
1078
timestamp:
2025-03-17T14:51:22.348Z
sender:
@prematurata:matrix.org
content:
* yes. i didnt see this issue daaamn. but essentially yeah its the same as i am reporting

id:
1077
timestamp:
2025-03-17T14:56:07.261Z
sender:
@gav:polkadot.io
content:
https://github.com/gavofyork/graypaper/pull/292

id:
1076
timestamp:
2025-03-17T15:01:34.264Z
sender:
@prematurata:matrix.org
content:
perfect thanks

id:
2226
timestamp:
2025-03-17T15:02:42.631Z
sender:
@gav:polkadot.io
content:
No and yes.

id:
2225
timestamp:
2025-03-17T15:03:25.585Z
sender:
@gav:polkadot.io
content:
Prize pool is split between language sets.

id:
2224
timestamp:
2025-03-17T15:04:03.203Z
sender:
@gav:polkadot.io
content:
* Prize pool is split between language sets, so it makes sense to select a language from one of the less popular language sets (C and D, currently).

id:
2223
timestamp:
2025-03-17T15:04:53.017Z
sender:
@gav:polkadot.io
content:
The account ID is there to ensure that someone doesn't wait for the implementation to be nearly complete, fork the repo, finish the impl and submit it to collect the entire prize.

id:
2222
timestamp:
2025-03-17T15:04:59.258Z
sender:
@gav:polkadot.io
content:
* The account ID rule is there to ensure that someone doesn't wait for the implementation to be nearly complete, fork the repo, finish the impl and submit it to collect the entire prize.

id:
2221
timestamp:
2025-03-17T15:05:06.766Z
sender:
@gav:polkadot.io
content:
* The account ID rule is there to ensure that someone doesn't wait for an implementation to be nearly complete, fork the repo, finish the impl and submit it to collect the entire prize.

id:
2220
timestamp:
2025-03-17T15:08:14.069Z
sender:
@gav:polkadot.io
content:
As already stated, the JAM prize is intended to incentivize and seed a diverse, distributed, decentralised knowledge base of the JAM protocol. This precludes the substantive use of AI to generate or translate code. We already have some rules to preclude the substantive use of AI, but others may be introduced to help police against it and ensure that implementations are actually clean-room.

id:
2219
timestamp:
2025-03-17T15:08:47.964Z
sender:
@gav:polkadot.io
content:
> Any Jam-implementation code which is viewed before or during implementation must be declared.

Since AI models may be trained on code from other implementations, this precludes the substantive use of AI-generated code.

id:
2218
timestamp:
2025-03-17T15:10:05.314Z
sender:
@gav:polkadot.io
content:
> Interview may be requested after submission to ensure team members are the legitimate authors of the code.

Again, here for the sake of argument "legitimate" means without the assistance of AI.

id:
2217
timestamp:
2025-03-17T15:10:11.264Z
sender:
@gav:polkadot.io
content:
* > Interview may be requested after submission to ensure team members are the legitimate authors of the code.

Again, here for the sake of argument "legitimate" means without the assistance of generative AI.

id:
2216
timestamp:
2025-03-17T15:10:48.380Z
sender:
@gav:polkadot.io
content:
Remember: the purpose is to decentralise. AI is fundamentally centralising.

id:
2215
timestamp:
2025-03-17T15:12:51.979Z
sender:
@gav:polkadot.io
content:
Expect a hard grilling at interview. I (and perhaps others in the Fellowship) will want to see that not only did you write the code but that you *understand* the code and how it relates to the GP. Being able to produce a conformant implementation is not enough to win the prize. It must be convincingly written from scratch by the team behind it without assistance from others, not even if intermediated by an AI model.

id:
2214
timestamp:
2025-03-17T15:15:14.541Z
sender:
@gav:polkadot.io
content:
Happy to answer any questions.

id:
2213
timestamp:
2025-03-17T15:15:18.222Z
sender:
@gav:polkadot.io
content:
* Happy to answer any questions on the above.

id:
2212
timestamp:
2025-03-17T15:27:58.312Z
sender:
@jan:parity.io
content:
In general when working on something novel AI is pretty much useless in the vast majority of cases. If you need to write a bunch of boilerplate code that was already done a million times - that's where using an LLM shines, but if you're doing novel work it's either going to sabotage you because the code will just plainly be wrong, or waste a bunch of your time because you'll have to rewrite it anyway. Not sure if Gav would agree, but personally I wouldn't care if you'd use AI to help you write some boilerplate unrelated to JAM (e.g. spinning up an RPC server in your language of choice; that has been done a million times already and I see no reason to waste time reinventing it yet again) or get it to generate some unit tests for you, but everything related to the protocol itself and its implementation should be written entirely manually.

id:
2211
timestamp:
2025-03-17T15:29:19.230Z
sender:
@jan:parity.io
content:
* In general when working on something novel AI is pretty much useless in the vast majority of cases. If you need to write a bunch of boilerplate code that was already done a million times - that's where using an LLM shines, but if you're doing novel work it's either going to sabotage you because the code will just plainly be wrong, or waste a bunch of your time because you'll have to rewrite it anyway. Not sure if Gav would agree, but personally I wouldn't care if you'd use AI to help you write some boilerplate unrelated to JAM (e.g. spinning up an RPC server in your language of choice; that has been done a million times already and I see no reason to waste time reinventing it yet again - without an LLM you'd copy-paste it from the library's docs or StackOverflow anyway) or get it to generate some unit tests for you, but everything related to the protocol itself and its implementation should be written entirely manually.

id:
2210
timestamp:
2025-03-17T18:55:10.979Z
sender:
@mnaamani:matrix.org
content:
Thanks that was very clear and comprehensive, Glad I asked because I was considering a less popular language from set B (D lang), but given that I'm a bit late to the game, it might make most sense to pick something even more rare from the "Mad" category? But if I was not solely focused on the prize, an implementation in D could still be valuable. I'll ponder on it.

id:
1075
timestamp:
2025-03-17T19:54:07.440Z
sender:
@jaymansfield:matrix.org
content:
Hey, I have a question relating to guaranteeing/auditing timing. After a work package is guaranteed and included on-chain, assurers will normally start assuring its availability in the next slot. Is there any reason to wait for these assurances to be posted, or can the auditors (if they are assigned to that specific core) just immediately request shards from the original guarantors? My implementation currently doesn't wait for the assurances and just wanted to see if thats okay, or if i should change the timing around.

id:
2209
timestamp:
2025-03-17T22:30:22.986Z
sender:
@ascriv:matrix.org
content:
In the case that N > 5 teams submit milestone 1s at the same time for the same language set, is the prize pool split N ways or are the 5 best chosen, or some other method?

id:
1074
timestamp:
2025-03-18T00:01:59.898Z
sender:
@jaymansfield:matrix.org
content:
* Hey, I have a question relating to guaranteeing/auditing timing. After a work package is guaranteed and included on-chain, assurers will normally start assuring its availability in the next slot. Is there any reason to wait for these assurances to be posted, or can the auditors (if they are assigned to that specific core) just immediately request shards from the original guarantors? My implementation currently doesn't wait for the assurances and just wanted to see if thats okay, or if i should change the timing around. What are the best practices here?

id:
1073
timestamp:
2025-03-18T02:30:30.911Z
sender:
@shwchg:matrix.org
content:
https://graypaper.fluffylabs.dev/#/85129da/1b6d001b8800?v=0.6.3
Is equation (14.14) using J_x as defined in (E.5) or the PagedProof P described in (14.10)?

From the preceding equations, it appears to use J_x, but the paragraph stating that “such a vast amount of data is not generally needed as the justification can be derived through a single PagedProof” suggests that using P could also be reasonable. Could we get a clarification on this?

id:
1072
timestamp:
2025-03-18T17:53:59.256Z
sender:
@vinsystems:matrix.org
content:
* What doeas [`t_o`](https://graypaper.fluffylabs.dev/#/85129da/307f02307f02?v=0.6.3) mean in the info host\_call function?

In the graypaper reader version 3 March 2025 this term is `t_l`, but in the latest version of the GP 0.6.3 March 13 2025 this term is `t_o`.

id:
1071
timestamp:
2025-03-18T17:54:06.763Z
sender:
@vinsystems:matrix.org
content:
* What does [`t_o`](https://graypaper.fluffylabs.dev/#/85129da/307f02307f02?v=0.6.3) mean in the info host\_call function?

In the graypaper reader version 3 March 2025 this term is `t_l`, but in the latest version of the GP 0.6.3 March 13 2025 this term is `t_o`.

id:
1070
timestamp:
2025-03-18T18:26:35.072Z
sender:
@gav:polkadot.io
content:
Not sure what you mean: see section 17.3; audit selection happens on all work-reports "pending which have just become available".



id:
1069
timestamp:
2025-03-18T18:26:59.416Z
sender:
@gav:polkadot.io
content:
So you'll never be self-selecting to audit a work-report until you, at least, believe it is available.

id:
1068
timestamp:
2025-03-18T18:27:55.933Z
sender:
@jaymansfield:matrix.org
content:
Thanks this answers the question. 

id:
1067
timestamp:
2025-03-18T18:30:24.942Z
sender:
@gav:polkadot.io
content:
14.14 defines regular-J, using caligraphic-J, itself defined in E.5

id:
1066
timestamp:
2025-03-18T18:32:18.136Z
sender:
@gav:polkadot.io
content:
* 14.14 defines regular-J, using caligraphic-J (subscripted with 0), itself defined in E.5.

id:
1065
timestamp:
2025-03-18T18:33:02.567Z
sender:
@gav:polkadot.io
content:
14.14 does not use the paged-proof(regular-P, 14.10) at all.

id:
1064
timestamp:
2025-03-18T18:33:35.750Z
sender:
@gav:polkadot.io
content:
* The formulation of 14.14 does not use the paged-proof(regular-P, 14.10) at all.

id:
1063
timestamp:
2025-03-18T18:34:28.917Z
sender:
@gav:polkadot.io
content:
Now, please remember as I've now countless times, the Gray Paper defines *observable behaviour*. It doesn't necessarily tell you how to create that behaviour.

id:
1062
timestamp:
2025-03-18T18:34:35.053Z
sender:
@gav:polkadot.io
content:
* Now, please remember as I've now said countless times, the Gray Paper defines _observable behaviour_. It doesn't necessarily tell you how to create that behaviour.

id:
1061
timestamp:
2025-03-18T18:36:32.013Z
sender:
@gav:polkadot.io
content:
In this case, as the text states, you'll need to combine this formulation with the page-proof formulation and see that you can create the correct behaviour without  requiring all of the data in the segment tree.

id:
1060
timestamp:
2025-03-18T18:38:08.398Z
sender:
@gav:polkadot.io
content:
That's indeed the whole point of the paged-proofs. They're a very efficient means of storing proofs across nodes for proving the correctness of exported data when the time comes to import them.

id:
1059
timestamp:
2025-03-18T18:38:30.429Z
sender:
@gav:polkadot.io
content:
* That's indeed the whole point of the paged-proofs. They're a very efficient means of storing proofs across nodes for showing the correctness of exported data segments when the time comes to import them.

id:
1058
timestamp:
2025-03-18T18:38:45.894Z
sender:
@gav:polkadot.io
content:
* The formulation of 14.14 does not use the paged-proof(regular-P, 14.10) formulation at all.

id:
1057
timestamp:
2025-03-18T18:39:05.429Z
sender:
@gav:polkadot.io
content:
* In this case, as the text states, you'll need to combine this formulation with the page-proof formulation and see that you can create the correct behaviour without requiring all of the data in the segment tree.

id:
1056
timestamp:
2025-03-18T18:42:26.920Z
sender:
@gav:polkadot.io
content:
So you'll need to study (14.10) and understand that for any export, a relevant proof-page can be used and information from within the two parts of it can be (quite easily) selected and combined in order to produce a full import-justification.

id:
1055
timestamp:
2025-03-18T18:42:55.035Z
sender:
@gav:polkadot.io
content:
* So you'll need to study (14.10) and understand that for any export, a relevant proof-page can be used and information from within the two parts of it can be (quite easily) selected and combined in order to produce a full import-justification of the form needed by (14.14 J).

id:
1054
timestamp:
2025-03-18T18:43:20.239Z
sender:
@gav:polkadot.io
content:
* So you'll need to study (14.10) and understand that for any export, a relevant proof-page can be fetched and information from within the two parts of it can be (quite easily) selected and combined in order to produce a full import-justification of the form needed by (14.14 J).

id:
1053
timestamp:
2025-03-18T18:43:32.321Z
sender:
@gav:polkadot.io
content:
* So you'll need to study (14.10) and understand that for any export, a relevant proof-page can be fetched from the Segments DA and information from within the two parts of it can be (quite easily) selected and combined in order to produce a full import-justification of the form needed by (14.14 J).

id:
1052
timestamp:
2025-03-18T19:24:58.642Z
sender:
@gav:polkadot.io
content:
@room [v0.6.4 is released](https://github.com/gavofyork/graypaper/releases/tag/v0.6.4). Many minor fixes and clarifications in here; the only real changes are:
- [Activity statistics](https://github.com/gavofyork/graypaper/pull/285)
- [Assurances are checked with the prior validator set (rather than posterior)](https://github.com/gavofyork/graypaper/pull/281)
- [Include ed25519 keys in the epoch marker](https://github.com/gavofyork/graypaper/pull/282)

id:
1051
timestamp:
2025-03-18T19:25:45.542Z
sender:
@gav:polkadot.io
content:
Activity statistics should help us understand what our JAM instances are actually doing and would be a nice thing to monitor and visualise on a web/cli tool if anyone's making any.

id:
1050
timestamp:
2025-03-18T21:11:40.847Z
sender:
@sourabhniyogi:matrix.org
content:
Is it reasonable to use [CE139](https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-139140-segment-shard-request) to fetch the relevant-proof page from "Segments DA", where the `Segment Index` of CE139 is _greater_ than the export count?

id:
1049
timestamp:
2025-03-18T21:12:28.066Z
sender:
@sourabhniyogi:matrix.org
content:
* Is it reasonable to use [CE139](https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-139140-segment-shard-request) to fetch the relevant-proof page (or rather shards of the proof page, so as to reconstruct the proof page) from "Segments DA", where the `Segment Index` of CE139 is _greater_ than the export count?

id:
1048
timestamp:
2025-03-18T21:16:38.452Z
sender:
@dave:parity.io
content:
Yes, that's intended even

id:
1047
timestamp:
2025-03-18T21:26:35.900Z
sender:
@eclesiomelo:matrix.org
content:
hey guys! I have a question regards the test service in the Accumulation test vector, we have tried to parse it using the definition A.37. I would like to confirm that this PVM blob is formatted according to this definition and not simply A.2 (deblob), is this correct?

id:
1046
timestamp:
2025-03-19T00:36:29.572Z
sender:
@shwchg:matrix.org
content:
ok! Thanks for replying me!

id:
1045
timestamp:
2025-03-19T04:15:03.884Z
sender:
@qiwei:matrix.org
content:
yes, it's a standard program blob

id:
1044
timestamp:
2025-03-19T07:19:46.612Z
sender:
@boymaas:matrix.org
content:
I am double-checking my implementation against the GP, and a question arose. I conclude that if we have a configuration with a W* such that the sum of gas consumption for W* is greater than G_a * C, then we would be overspending the provided gas limit (https://graypaper.fluffylabs.dev/#/85129da/176503176f03?v=0.6.3) with our always accumulate services (https://graypaper.fluffylabs.dev/#/85129da/175901175901?v=0.6.3). I searched for a condition that would prevent this but could not find one. Furthermore, this line from the Gray Paper suggests that such a scenario is possible:

	“We work with a limited amount of gas per block and therefore may not be able to process all items in W∗ in a single block.”
(https://graypaper.fluffylabs.dev/#/85129da/160e02161102?v=0.6.3)

If my conclusion is correct, is this overspending allowed by design?

id:
1043
timestamp:
2025-03-19T10:00:48.093Z
sender:
@gav:polkadot.io
content:
> then we would be overspending the provided gas limit

No.

id:
1042
timestamp:
2025-03-19T10:01:28.708Z
sender:
@gav:polkadot.io
content:
W* is the *accumulatable* WRs; we don't accumulate all of them if the gas limit doesn't allow.

id:
1041
timestamp:
2025-03-19T10:03:02.126Z
sender:
@gav:polkadot.io
content:
As for always-accumulate services, see (12.20) - there's always enough gas to accumulate those.

id:
1040
timestamp:
2025-03-19T10:04:04.212Z
sender:
@gav:polkadot.io
content:
* As for always-accumulate services, see (12.20) - there's always enough gas to accumulate those in addition to the usual amount for each core.

id:
1039
timestamp:
2025-03-19T10:08:23.563Z
sender:
@gav:polkadot.io
content:
There's an additional note placed on G_T in definitions advising to account for the always-accumulate services, but because of (12.20) G_T formally only places a lower-limit on the amount of gas used; if there's more always-accumulate gas than would fit into G_T, we run "over" G_T and honour the always-accumulate gas.

id:
2208
timestamp:
2025-03-19T10:10:08.995Z
sender:
@gav:polkadot.io
content:
It's a first-come-first-serve basis.

id:
1038
timestamp:
2025-03-19T11:29:13.626Z
sender:
@boymaas:matrix.org
content:
Thank you, gav . I am going to take another look later today. I was aware of 12.20 with the addition of the always accumulate services. I was imagining a scenario where we could get a set of accumulatable WRs whose total gas consumption would be more than Gₐ * C. Where fe an immediate WR, would resolve a bunch of queued WRs. In a tiny network setup with 2 cores for example. Since g is used to select a subset of the accumulable WRs I though it could eat into the reserve for the always accumulate services.

id:
2207
timestamp:
2025-03-19T12:09:43.868Z
sender:
@prematurata:matrix.org
content:
> <@gav:polkadot.io> It's a first-come-first-serve basis.

Was it always like this? I think I remember a discussion about splitting in case more than 5 submit a valid impl.

Wouldn't a change (if any) towards a first come first serve cause a "click day" basically adding more value to the "social posting timing" than the implementation itself?

id:
1037
timestamp:
2025-03-19T12:17:14.087Z
sender:
@vinsystems:matrix.org
content:
Should [this](https://graypaper.fluffylabs.dev/#/85129da/2dff012dff01?v=0.6.3) be `H?`

id:
1036
timestamp:
2025-03-19T12:44:37.164Z
sender:
@vinsystems:matrix.org
content:
* Should [this](https://graypaper.fluffylabs.dev/#/85129da/2dff012dff01?v=0.6.3) be `H?` ?

id:
2206
timestamp:
2025-03-19T13:11:21.662Z
sender:
@jay_ztc:matrix.org
content:
I think you're making an assumption that M1 secures future payments. Based on the submission rules & milestone delivery repo, completing m1 doesn't "reserve your spot" for future milestones. Currently there is (up to) 100k dot per milestone and 2.5 mil dot per lang. It could be the case that 10 teams for a single pool complete & collect m1 and m2 payments- meaning 20x100k dot has been paid out (75% of it linearly vesting). In that case there would only be 500k left in that prize pool.

Someone please correct me if I'm wrong.

https://github.com/w3f/jam-milestone-delivery/blob/main/README.md?plain=1#L31

id:
2205
timestamp:
2025-03-19T13:13:12.935Z
sender:
@jay_ztc:matrix.org
content:
* I think you're making an assumption that M1 secures future payments. Based on the submission rules & milestone delivery repo, completing m1 doesn't "reserve your spot" for future milestones. Currently there is (up to) 100k dot per milestone and 2.5 mil dot per lang pool. It could be the case that 10 teams for a single pool complete & collect m1 and m2 payments- meaning 20x100k dot has been paid out (75% of it linearly vesting). In that case there would only be 500k left in that prize pool.

Someone please correct me if I'm wrong.

https://github.com/w3f/jam-milestone-delivery/blob/main/README.md?plain=1#L31

id:
2204
timestamp:
2025-03-19T13:23:22.487Z
sender:
@ascriv:matrix.org
content:
My question is more like, what do we do in the (maybe unlikely) case that we get 15 valid M1s right when M1 submissions start getting accepted? Or are we tiebreaking by millisecond timing of submission?

id:
2203
timestamp:
2025-03-19T13:23:51.037Z
sender:
@ascriv:matrix.org
content:
* My question is more like, what do we do in the (maybe unlikely) case that we get 15 (lang set A) valid M1s right when M1 submissions start getting accepted? Or are we tiebreaking by millisecond timing of submission?

id:
2202
timestamp:
2025-03-19T13:25:51.427Z
sender:
@jay_ztc:matrix.org
content:
Doesn't say anywhere about the prize being evenly split between milestones. I assume (up to, pending judging) 100k paid to 15 teams meaning only 1 mil dot left in that pool.

Again, not a fellow or w3f member- someone correct me if I'm wrong.

id:
2201
timestamp:
2025-03-19T13:29:50.042Z
sender:
@dakkk:matrix.org
content:
Would be fun if after working on Jampy for 8 months full time I miss the timing for submission because I was sleeping 

id:
2200
timestamp:
2025-03-19T14:05:36.371Z
sender:
@clearloop:matrix.org
content:
spacejam-localnet.mov

id:
2199
timestamp:
2025-03-19T14:05:39.831Z
sender:
@clearloop:matrix.org
content:
spacejam can finally maintain the block finalization in localnet, so much pain debugging with logs these days 😭

id:
1035
timestamp:
2025-03-19T14:42:15.203Z
sender:
@gav:polkadot.io
content:
Always-accumulate services should be in the first "batch" of WRs accumulated (i.e. among the first items of W*) so they will, indeed, always accumulate.

id:
1034
timestamp:
2025-03-19T14:44:08.562Z
sender:
@gav:polkadot.io
content:
Should *what* be `H`?

id:
1033
timestamp:
2025-03-19T14:44:25.778Z
sender:
@gav:polkadot.io
content:
Not at all clear what you're pointing at or the reasoning behind your suggestion/

id:
1032
timestamp:
2025-03-19T14:44:27.938Z
sender:
@gav:polkadot.io
content:
* Not at all clear what you're pointing at or the reasoning behind your suggestion.

id:
1031
timestamp:
2025-03-19T14:44:37.574Z
sender:
@eclesiomelo:matrix.org
content:
okay, so given the definition A.37 the first 3 bytes encodes the |o|, however the first 3 bytes in the test service is 0x47000c which decodes to 786503 in decimal. 



id:
1030
timestamp:
2025-03-19T14:44:45.186Z
sender:
@eclesiomelo:matrix.org
content:
image.png

id:
1029
timestamp:
2025-03-19T14:45:26.111Z
sender:
@eclesiomelo:matrix.org
content:
* okay, so given the definition A.37 the first 3 bytes encodes the |o|, however the first 3 bytes in the test service is 0x47000c which decodes to 786503 in decimal which is greater than the test service total bytes



id:
2198
timestamp:
2025-03-19T14:45:30.965Z
sender:
@gav:polkadot.io
content:
Well, each language set can support the M1 payout of 25 teams.

id:
2197
timestamp:
2025-03-19T14:45:33.109Z
sender:
@jaymansfield:matrix.org
content:
Congrats!

id:
2196
timestamp:
2025-03-19T14:45:41.908Z
sender:
@gav:polkadot.io
content:
* Well, each language set can support the M1 payout of 25 teams, so I don't think there's going to be significant competition there.

id:
2195
timestamp:
2025-03-19T14:46:24.206Z
sender:
@gav:polkadot.io
content:
And this would assume all submissions passed all (secret) M1 conformance tests perfectly first time.

id:
2194
timestamp:
2025-03-19T14:49:05.855Z
sender:
@gav:polkadot.io
content:
The published test vectors are obviously incomplete and I find it quite unlikely that all candidates will pass all conformance tests first time. 

id:
1028
timestamp:
2025-03-19T14:57:41.291Z
sender:
@vinsystems:matrix.org
content:
[Eq B.6](https://graypaper.fluffylabs.dev/#/85129da/2def012d0102?v=0.6.3) defines the result context of the accumulate invocation `X`. The last term of this equation is `y ∈ ¿H`. Should this term be `y ∈ H?` instead of `y ∈ ¿H` since the operator `¿` is used for serialisable terms?

id:
1027
timestamp:
2025-03-19T14:59:04.824Z
sender:
@vinsystems:matrix.org
content:
* [Eq B.6](https://graypaper.fluffylabs.dev/#/85129da/2def012d0102?v=0.6.3) defines the result context `X` of the accumulate invocation. The last term of this equation is `y ∈ ¿H`. Should this term be `y ∈ H?` instead of `y ∈ ¿H` since the operator `¿` is used for serialisable terms?

id:
1026
timestamp:
2025-03-19T15:00:16.313Z
sender:
@eclesiomelo:matrix.org
content:
I am using the inverse of integer encoding, definition C.5, and where I pass [47, 00, 0c] and as output I got 786503, which I think I am missing something.

id:
1025
timestamp:
2025-03-19T15:03:20.527Z
sender:
@vinsystems:matrix.org
content:
* [Eq B.6](https://graypaper.fluffylabs.dev/#/85129da/2def012d0102?v=0.6.3) defines the result context `X` of the accumulate invocation. The last term of this equation is `y ∈ ¿H`. Should this term be `y ∈ H?` instead of `y ∈ ¿H` since the operator `¿` is used for serializable terms?

id:
1024
timestamp:
2025-03-19T15:14:17.806Z
sender:
@gav:polkadot.io
content:
Ahh I see; yes.

id:
1023
timestamp:
2025-03-19T15:17:31.432Z
sender:
@gav:polkadot.io
content:
https://github.com/gavofyork/graypaper/pull/299

id:
1022
timestamp:
2025-03-19T17:12:59.833Z
sender:
@sourabhniyogi:matrix.org
content:
C(13) in Eq D.2 needs to get the current / last validator stats (pi_V, pi_L) back in I think --

id:
1021
timestamp:
2025-03-19T17:13:18.534Z
sender:
@sourabhniyogi:matrix.org
content:
* C(13) in Eq D.2 needs to get the current / last validator stats (pi\_V, pi\_L) back in (from 13.1/13.2)

id:
2193
timestamp:
2025-03-19T17:35:24.149Z
sender:
@gav:polkadot.io
content:
* The published test vectors are obviously not comprehensive (nor are they designed to be) and I find it quite unlikely that all candidates will pass all conformance tests first time. 

id:
1020
timestamp:
2025-03-19T17:37:04.407Z
sender:
@gav:polkadot.io
content:
Please explain/

id:
1019
timestamp:
2025-03-19T17:37:06.434Z
sender:
@gav:polkadot.io
content:
* Please explain

id:
1018
timestamp:
2025-03-19T17:38:51.696Z
sender:
@sourabhniyogi:matrix.org
content:
The subscripts in C(13) in D.2 only have { C, S } but should have { C, S, V, L } to cove both core/service activity (the new elements of C(13)) and validator statistics activity (the old elements of C(13)), that's all.

id:
1017
timestamp:
2025-03-19T17:39:12.816Z
sender:
@gav:polkadot.io
content:
Not sure what you're looking at

id:
1016
timestamp:
2025-03-19T17:39:17.437Z
sender:
@gav:polkadot.io
content:
`main` branch currently reads:

id:
1015
timestamp:
2025-03-19T17:39:23.908Z
sender:
@sourabhniyogi:matrix.org
content:
image.png

id:
1014
timestamp:
2025-03-19T17:39:26.764Z
sender:
@gav:polkadot.io
content:
image.png

id:
1013
timestamp:
2025-03-19T17:41:48.181Z
sender:
@gav:polkadot.io
content:
I suspect the problem you're describing was fixed by https://github.com/gavofyork/graypaper/pull/298

id:
1012
timestamp:
2025-03-19T17:43:16.215Z
sender:
@gav:polkadot.io
content:
> The subscripts in C(13) in D.2 only have { C, S }

(They actually had C, L and S, but were indeed missing V as the typo had C twice.

id:
1011
timestamp:
2025-03-19T17:43:19.611Z
sender:
@gav:polkadot.io
content:
* > The subscripts in C(13) in D.2 only have { C, S }

(They actually had C, L and S, but were indeed missing V as the typo had C twice.)

id:
1010
timestamp:
2025-03-19T17:44:00.418Z
sender:
@sourabhniyogi:matrix.org
content:
* The subscripts in C(13) in D.2 only have { C, S, L } but should have { C, S, V, L } to cover both core/service activity (the new elements of C(13)) and validator statistics activity (the old elements of C(13)), that's all.

id:
1009
timestamp:
2025-03-19T17:47:11.237Z
sender:
@sourabhniyogi:matrix.org
content:
Would it be ok to seed a JAM implementer DAO on this repo
 https://github.com/w3f-webops/graypaper-website/pull/96

id:
1008
timestamp:
2025-03-19T18:59:52.404Z
sender:
@sourabhniyogi:matrix.org
content:
How did you end up with W_M = 3072 (The maximum number of imports and exports in a work-package.) which implies at most 48 proof pages and 12.6MB of CoreVM memory -- is this due to bandwidth or networking considerations?    With 4104 byte proof pages, you can fit many more export segments.  If we solved the 14.10/14.14 "puzzle", we have only 64*32+192 byte = 2240 bytes out the 4104 bytes being used.  You can definitely fit many many more segments with the unused proof page space of 4104-2240=1864 bytes

id:
1007
timestamp:
2025-03-19T19:00:51.340Z
sender:
@sourabhniyogi:matrix.org
content:
* How did you end up with W\_M = 3072 (The maximum number of imports and exports in a work-package.) which implies at most 48 proof pages and 12.6MB of CoreVM memory -- is this due to bandwidth or networking considerations?    With 4104 bytes usable per proof page, you can fit many many more import/export segments than 3072.   If we solved the 14.10/14.14 "puzzle", we have only 64\*32+192 byte = 2240 bytes out the 4104 bytes being used.  You can definitely fit many many more segments with the unused proof page space of 4104-2240=1864 bytes

id:
1006
timestamp:
2025-03-19T19:02:21.483Z
sender:
@dave:parity.io
content:
I didn't come up with these numbers; Gav did. I think they're based primarily on bandwidth usage, as discussed in the discussion section at the end of the GP

id:
1005
timestamp:
2025-03-19T19:08:52.891Z
sender:
@dave:parity.io
content:
Re proof segment usage, think one reason for the choice of 64 is that it's a power-of-2 which makes things simple -- you fit a complete subtree in a page. Could probably fit more page proofs in but it would get more complicated as you would have a partial subtree. Not sure if there are other reasons.

id:
1004
timestamp:
2025-03-19T19:10:01.311Z
sender:
@dave:parity.io
content:
* Re proof segment usage, think one reason for the choice of 64 is that it's a power-of-2 which makes things simple -- you fit a complete subtree in a segment. Could probably fit more page proofs in but it would get more complicated as you would have a partial subtree. Not sure if there are other reasons.

id:
1003
timestamp:
2025-03-19T19:10:16.918Z
sender:
@dave:parity.io
content:
* Re proof segment usage, think one reason for the choice of 64 is that it's a power-of-2 which makes things simple -- you fit a complete subtree in a segment. Could probably fit more proofs in but it would get more complicated as you would have a partial subtree. Not sure if there are other reasons.

id:
1002
timestamp:
2025-03-19T19:10:48.393Z
sender:
@dave:parity.io
content:
* Re proof segment usage, think one reason for the choice of 64 is that it's a power-of-2 which makes things simple -- you fit a complete subtree in a segment. Could probably fit more proofs in but it would get more complicated as you would have a partial subtree (or multiple subtrees depending on how you look at it). Not sure if there are other reasons.

id:
1001
timestamp:
2025-03-19T19:21:38.923Z
sender:
@sourabhniyogi:matrix.org
content:
Did you get "full" erasure coding decoding in the JAM Toaster yet (or some serious fraction of it) --  I've been bothered by doing all this networking just to get a measly 12 bytes  and was wondering if instead of W_G=4104 pages we'd like something slightly bigger than 64K or 128K instead (thus getting 16x-32x as much).   Then the 3072 could gets 16 or 32x times as much memory but of course 16x - 32x as much bandwidth usage.  The idea would be that you can get your CoreVM loaded with fewer segments and network calls, but maybe someone will want a CoreVM to have not 12.6MB or memory but 200MB or 400MB instead.  Not sure if W_G = 4104 is chosen to be compatible with some RISC-V related considerations?

id:
1000
timestamp:
2025-03-19T19:25:02.497Z
sender:
@sourabhniyogi:matrix.org
content:
* Did you get "full" erasure coding decoding in the JAM Toaster yet (or some serious fraction of it) --  I've been bothered by doing all this networking just to get a measly 12 bytes  and was wondering if instead of W\_G=4104 pages we'd like something slightly bigger than 64K or 128K instead (thus getting 16x-32x as much).   Then the 3072 could gets 16 or 32x times as much memory but of course 16x - 32x as much bandwidth usage.  The idea would be that you can get your CoreVM loaded with fewer segments and network calls, but maybe someone will want a CoreVM to have not 12.6MB of memory but 200MB or 400MB instead.  Not sure if W\_G = 4104 is chosen to be compatible with some RISC-V related considerations?

id:
999
timestamp:
2025-03-19T19:25:11.825Z
sender:
@dave:parity.io
content:
W_G=4104 for 4k pages for eg CoreVM. Bigger page sizes are not great, as it means way more read/write amplification in the worst case. 4k is already really too big for a number of use cases, we just can't really go smaller as 4k is the min page size on modern HW

id:
998
timestamp:
2025-03-19T19:28:49.296Z
sender:
@dave:parity.io
content:
Please note that the 12 bytes is _per segment shard requested_. You should absolutely be batching these requests so that in the ideal case you make ~340 network requests for all of the segment shards you need for a WP. This is still a lot of requests of course. In the full network protocol we may add a fast path for requesting the original data directly from guarantors.

id:
997
timestamp:
2025-03-19T19:32:19.938Z
sender:
@dave:parity.io
content:
12MB is a limit on the number of pages read/written _per WP_. It is _not_ the limit on the CoreVM memory. This can't really be made larger as it is constrained by bandwidth. Remember that all this data needs to be read/written from/to the DA system for every core every slot

id:
996
timestamp:
2025-03-19T19:53:08.537Z
sender:
@sourabhniyogi:matrix.org
content:
Understood about the batching of `segmentIndex` in JAMNP taking us from 12 bytes to some multiple of that.   My mental model is that the majority of segments will come from one primary work package and occasionally a few segments come in from another few "foreign" wps but I understand that's just one use case, where my mental model is formed by what I imagine a lazy CoreVM programmer would do (if you tell him its like normal programs).   

By "read/write amplification" I believe you mean the idea that we load up all these pages (yes, in a single WP) from Segments DA and only a tiny % of the pages are updated and only a tiny fraction of each page at that, right?  So it looks like a tradeoff between that and the number of networking calls, even with batched `segmentIndex`.   Thanks for explaining!

id:
995
timestamp:
2025-03-19T20:50:09.528Z
sender:
@sourabhniyogi:matrix.org
content:
* How did you end up with W\_M = 2048 (The maximum number of imports and exports in a work-package.) which implies at most 32 proof pages and 8.4MB of CoreVM memory -- is this due to bandwidth or networking considerations?    With 4104 bytes usable per proof page, you can fit many many more import/export segments than 2048.   If we solved the 14.10/14.14 "puzzle", we have only 64\*32+5*32 byte = 2208 bytes out the 4104 bytes being used.  You can definitely fit many many more segments with the unused proof page space of 4104-2240=1900 bytes

id:
994
timestamp:
2025-03-19T20:51:00.507Z
sender:
@sourabhniyogi:matrix.org
content:
* Did you get "full" erasure coding decoding in the JAM Toaster yet (or some serious fraction of it) --  I've been bothered by doing all this networking just to get a measly 12 bytes  and was wondering if instead of W\_G=4104 pages we'd like something slightly bigger than 64K or 128K instead (thus getting 16x-32x as much).   Then the 2048 could gets 16 or 32x times as much memory but of course 16x - 32x as much bandwidth usage.  The idea would be that you can get your CoreVM loaded with fewer segments and network calls, but maybe someone will want a CoreVM to have not 8.4MB of memory but 134MB or 268MB instead.  Not sure if W\_G = 4104 is chosen to be compatible with some RISC-V related considerations?

id:
993
timestamp:
2025-03-19T20:53:47.078Z
sender:
@sourabhniyogi:matrix.org
content:
* How did you end up with W\_M = 2048 (The maximum number of imports and exports in a work-package.) which implies at most 32 proof pages and 8.4MB of CoreVM memory -- is this due to bandwidth or networking considerations?    With 4104 bytes usable per proof page, you can fit many many more import/export segments than 2048.   If we solved the 14.10/14.14 "puzzle", we have only 64\*32+5\*32 byte = 2208 bytes out the 4104 bytes being used.  You can definitely fit many many more segments with the unused proof page space of 4104-2208=1896 bytes

id:
992
timestamp:
2025-03-19T20:56:11.650Z
sender:
@sourabhniyogi:matrix.org
content:
* How did you end up with W\_M = 2048 (The maximum number of imports and exports in a work-package.) [or 3072 in recent versions] which implies at most 32 proof pages and 8.4MB of CoreVM memory -- is this due to bandwidth or networking considerations?    With 4104 bytes usable per proof page, you can fit many many more import/export segments than 2048.   If we solved the 14.10/14.14 "puzzle", we have only 64\*32+5\*32 byte = 2208 bytes out the 4104 bytes being used.  You can definitely fit many many more segments with the unused proof page space of 4104-2208=1896 bytes

id:
991
timestamp:
2025-03-19T20:56:49.343Z
sender:
@sourabhniyogi:matrix.org
content:
* How did you end up with W\_M = 2048 (The maximum number of imports and exports in a work-package.) \[or 3072 in recent versions\] which implies at most 32 proof pages and 8.4MB of CoreVM memory -- is this due to bandwidth or networking considerations?    With 4104 bytes usable per proof page, you can fit many many more import/export segments than 2048.   If we solved the 14.10/14.14 "puzzle", we have only 64\*32+5\*32 byte = 2208 bytes out the 4104 bytes being used [or 2240 in recent versions].  You can definitely fit many many more segments with the unused proof page space of 4104-2208=1896 bytes

id:
990
timestamp:
2025-03-20T03:43:34.956Z
sender:
@sourabhniyogi:matrix.org
content:
What is the expected strategy to have the CoreVM service identify which pages in the child VM have been written to (say, marked with a "dirty" bit) so that those pages (and only those pages) are exported to Segment DA by the parent VM?    

id:
2192
timestamp:
2025-03-20T04:33:40.319Z
sender:
@waterreptile19:matrix.org
content:
hey all... how do I get started? Is there any codebase I can refer to?

id:
2191
timestamp:
2025-03-20T07:06:07.694Z
sender:
@mnaamani:matrix.org
content:
There is no codebase, the whole idea is that there is a detailed specification, they gray paper, based on which you would independently create an implementation of jam. https://graypaper.com/

id:
989
timestamp:
2025-03-20T08:47:33.290Z
sender:
@gav:polkadot.io
content:
Yes, CoreVM supports this.

id:
988
timestamp:
2025-03-20T08:47:59.773Z
sender:
@gav:polkadot.io
content:
* Yes, CoreVM will support this via the inner PVM.

id:
987
timestamp:
2025-03-20T08:48:02.052Z
sender:
@gav:polkadot.io
content:
* Yes, CoreVM will support this via the inner PVM API.

id:
986
timestamp:
2025-03-20T08:48:04.522Z
sender:
@gav:polkadot.io
content:
* Yes, CoreVM will support this via the inner PVM host API.

id:
985
timestamp:
2025-03-20T08:49:14.069Z
sender:
@gav:polkadot.io
content:
* Yes, CoreVM will support this via the inner PVM host API. We don't yet have that host call API in place but I believe it's on Jan Bujak 's TODO list.

id:
984
timestamp:
2025-03-20T08:56:23.235Z
sender:
@gav:polkadot.io
content:
One additional thing to note for segment reconstruction is that actually fetching each 12 byte piece from a unique set of 342 nodes and reconstructing is the very worst case. There are four better cases: that some or all of the segments are provided to the guarantors by the builder (because it could be the same builder or builder-network whose package created them in the first place). In this case they could have the proof-pages or just be content indexed and passed as extrinsics. Or the guarantor could already have the pages in their own cache because it was they who guaranteed the exporting package. Or the guarantor could fetch them from the exporting guarantor as whole segments. Or that the guarantor batches them because they were exported by the same package as other segments which are also needed for import. And in the worst case, the system wouldn't *break*; it would just mean that packages which exposed this worst-case behaviour would take potentially a little to make their way through the pipeline.

id:
983
timestamp:
2025-03-20T08:57:04.189Z
sender:
@gav:polkadot.io
content:
* One additional thing to note for segment reconstruction is that actually fetching each 12 byte piece from a unique set of 342 nodes and reconstructing is the very worst case. There are four better cases: that some or all of the segments are provided to the guarantors by the builder (because it could be the same builder or builder-network whose package created them in the first place). In this case they could have the proof-pages or just be content indexed and passed as extrinsics. Or the guarantor could already have the pages in their own cache because it was they who guaranteed the exporting package. Or the guarantor could fetch them from the exporting guarantor as whole segments. Or that the guarantor batches them because they were exported by the same package as other segments which are also needed for import. And in the worst case, the system wouldn't _break_; it would just mean that packages which exposed this worst-case behaviour would take potentially a little longer to make their way through the pipeline.

id:
982
timestamp:
2025-03-20T10:10:24.336Z
sender:
@dave:parity.io
content:
By read/write amplification I mean even if you want to read or write just 1 byte you still have to import/export an entire page/segment; you end up reading/writing 4000 times as much. Of course that is the worst case, usually it will not be quite as bad

id:
981
timestamp:
2025-03-20T10:26:06.653Z
sender:
@danicuki:matrix.org
content:
The encoding for newly created Work Result fields (C.23) do not specify integer sizes for xu, xi, xx, xz, xe. Are they all 1 byte values? 

id:
2190
timestamp:
2025-03-20T16:13:43.316Z
sender:
@eclesiomelo:matrix.org
content:
hey guys, we are fixing some bugs on our PVM implementation and one of it is related to the standard program initialization (definition A.36), more specifically we are trying to run the accumulate test vectors and its test service blob starts with the bytes `0x47000c` (here is one example -> https://github.com/davxy/jam-test-vectors/blob/038bd899ff7e387d61a46a8d509486179ef2efac/accumulate/tiny/accumulate_ready_queued_reports-1.json#L418) and in definition A.37 the 3 first bytes should be the lenght of the `o` term (`E3(|o|)`), but when  decoding them using a normal integer decoding function we get decimal `786503` which is bigger then the accumulate test service blob, did anyone face the same problem? The graypapper says `Given some p which is appropriately encoded together with some argument a`, this argument is also in the test service blob? I would appreciate any clarifications, thanks in advance.

id:
2189
timestamp:
2025-03-20T16:22:25.253Z
sender:
@eclesiomelo:matrix.org
content:
* hey guys, we are fixing some bugs on our PVM implementation and one of it is related to the standard program initialization (definition A.36), more specifically we are trying to run the accumulate test vectors and its test service blob starts with the bytes `0x47000c` (here is one example -> https://github.com/davxy/jam-test-vectors/blob/038bd899ff7e387d61a46a8d509486179ef2efac/accumulate/tiny/accumulate\_ready\_queued\_reports-1.json#L418) and in definition A.37 the 3 first bytes should be the lenght of the `o` term (`E3(|o|)`), but when  decoding them using a normal integer decoding function we get decimal `786503` which is bigger then the accumulate test service blob, so we cannot read the bytes relative to `o`, did anyone face the same problem? The graypapper says `Given some p which is appropriately encoded together with some argument a`, this argument is also in the test service blob? I would appreciate any clarifications, thanks in advance.

id:
980
timestamp:
2025-03-20T16:32:33.244Z
sender:
@prematurata:matrix.org
content:
I have a couple of questions/remarks about accumulation. Specifically how jam should handle dependencies when accumulating a service. Let's consider the following scenario:
  - WorkReport A contains 2 results: service 1 and service 2
  - WorkReport B has a dependency on A `((WB)_x)_p = ((WA)_s)_h` and contains one result of service 3.

So `W!` contains [1, 2], lets assume `WQ` is empty... But `W*` contains also 3 so ` W* = [1, 2, 3]`.

Now `∆∗` which is called "parallelized accumulation", is being called with W* on 12.21 https://graypaper.fluffylabs.dev/#/68eaa1f/17b50317c103?v=0.6.4 .

question 1: But since there is no ordering enforced (that i can see) when executing `∆∗`, then 3 might be executed before it's dependencies. 

question 2: lets say i am wrong in the previous question. Lets say Service 1 writes in it's storage a key that Service 3 needs to read from (via hostcall read). service 3 won't receive the updated storage when `∆1` gets called with s=3 

I guess both questions/remarks are wrong and that Service3 needs to be indeed gets executed after 1,2 (which can be parallelized) and that it should be accumulated using the updated service accounts from the 1,2 accumulation. but i can't see where this is enforced in the GP.

id:
979
timestamp:
2025-03-20T16:34:31.940Z
sender:
@prematurata:matrix.org
content:
* I have a couple of questions/remarks about accumulation. Specifically how jam should handle dependencies when accumulating a service. Let's consider the following scenario:

- WorkReport A contains 2 results: service 1 and service 2
- WorkReport B has a dependency on A `((WB)_x)_p = ((WA)_s)_h` and contains one result of service 3.

So `W!` contains \[1, 2\], lets assume `WQ` is empty... But `W*` contains also 3 so ` W* = [1, 2, 3]`.

Now `∆∗` which is called "parallelized accumulation", is being called with W\* on 12.21 https://graypaper.fluffylabs.dev/#/68eaa1f/17b50317c103?v=0.6.4 .

question 1: But since there is no ordering enforced (that i can see) when executing `∆∗`, then 3 might be executed before it's dependencies.

question 2: lets say i am wrong in the previous question. Lets say Service 1 writes in it's storage a key that Service 3 needs to read from (via hostcall read). service 3 does not seem to receive the updated storage when `∆1` gets called with s=3

I guess both questions/remarks are wrong and that Service3 needs to be indeed gets executed after 1,2 (which can be parallelized) and that it should be accumulated using the updated service accounts from the 1,2 accumulation. but i can't see where this is enforced in the GP.

id:
978
timestamp:
2025-03-20T17:08:57.452Z
sender:
@jan:parity.io
content:
Indeed, as Gav said, we will most likely add a way for the parent VM to fetch this information in an efficient manner. It's on my TODO list; we just don't want to spec it before implementing it to make sure the performance is good.

id:
977
timestamp:
2025-03-20T17:09:07.701Z
sender:
@prematurata:matrix.org
content:
* I have a couple of questions/remarks about accumulation. Specifically how jam should handle dependencies when accumulating a service. Let's consider the following scenario:

- WorkReport A contains 2 results: service 1 and service 2
- WorkReport B has a dependency on A `((WB)_x)_p = ((WA)_s)_h` and contains one result of service 3.

So `W!` contains \[1, 2\], lets assume `WQ` is empty... But `W*` contains also 3 so ` W* = [1, 2, 3]`.

Now `∆∗` which is called "parallelized accumulation", is being called with W\* on 12.21 (after ∆+) https://graypaper.fluffylabs.dev/#/68eaa1f/17b50317c103?v=0.6.4 .

question 1: But since there is no ordering enforced (that i can see) when executing `∆∗`, then 3 might be executed before it's dependencies.

question 2: lets say i am wrong in the previous question. Lets say Service 1 writes in it's storage a key that Service 3 needs to read from (via hostcall read). service 3 does not seem to receive the updated storage when `∆1` gets called with s=3

I guess both questions/remarks are wrong and that Service3 needs to be indeed gets executed after 1,2 (which can be parallelized) and that it should be accumulated using the updated service accounts from the 1,2 accumulation. but i can't see where this is enforced in the GP.

id:
976
timestamp:
2025-03-20T18:34:28.195Z
sender:
@sourabhniyogi:matrix.org
content:
That's totally great.  A `page-manifest`  host function call of both (A) what pages MUST be exported because it was written to (a "dirty bit" per page) and (B) what page MUST be imported because it was read from (a "accessed" bit per page) appears necessary for programmers to not have to think much about how their child "guest program" uses memory and the parent CoreVM service will need this manifest to poke and peek pages so the programmer doesn't have to. 

But since guest programs will vary widely in terms of how many "accessed" and "dirty" bits they get before they hit some refine gas limit OR the W_M=3072 (12MB) limit on imported+exported segments which applies across ALL the VMs, parent and child.   It seems the builder should be able to run with the whole 2^32 VMs always fully loaded and then use this bookkeeper's to dump work packages out right before it runs out of gas, to break up long running computations into work packages.   We would thus want "builder" mode refining (having tens of thousands maybe the 1MM page) and "guarantor" model refining (W_M) working on just the subset the builder wants.  Is that the way to think about designing this host function?

id:
975
timestamp:
2025-03-20T18:35:52.739Z
sender:
@sourabhniyogi:matrix.org
content:
* That's totally great.  A `page-manifest`  host function call of both (A) what pages MUST be exported because it was written to (a "dirty bit" per page) and (B) what page MUST be imported because it was read from (a "accessed" bit per page) appears necessary for programmers to not have to think much about how their child "guest program" uses memory and the parent CoreVM service will need this manifest to poke and peek pages so the programmer doesn't have to.

But since guest programs will vary widely in terms of how many "accessed" and "dirty" bits they get before they hit some refine gas limit OR the W\_M=3072 (12MB) limit on imported+exported segments which applies across ALL the VMs, parent and child, it seems the builder should be able to run with the whole 2^32 VMs always fully loaded ... and *then* use this bookkeeper to dump work packages out right before it runs out of gas or hit the W_M, to effectively break up long running computations into work packages that the guarantors can legit handle in accordance with JAM spec.   We would thus want "builder" mode refining (having tens of thousands maybe the 1MM page) and "guarantor" model refining (W\_M) working on just the subset the builder wants.  Is that the way to think about designing this host function?  How do performance considerations figure in?

id:
974
timestamp:
2025-03-20T18:37:52.436Z
sender:
@gav:polkadot.io
content:
They're all variable.

id:
973
timestamp:
2025-03-20T18:38:33.796Z
sender:
@gav:polkadot.io
content:
* They're all variable. See C.6

id:
972
timestamp:
2025-03-20T18:42:51.918Z
sender:
@gav:polkadot.io
content:
1. WIs for services 1, 2 and 3 would be executed in the same batch. From the perspective of each they would be executed under the same prior state. If #3 inspected the state of #2 it would see its prior state; if #2 inspected #3, it would also see #3's prior state.
2. No. #3 would not see the storage change.

id:
971
timestamp:
2025-03-20T18:42:58.861Z
sender:
@gav:polkadot.io
content:
* 1. WIs for services 1, 2 and 3 would be executed in the same batch. From the perspective of each they would be executed under the same prior state. If #3 inspected the state of #2 it would see its prior state; if #2 inspected #3, it would also see #3's prior state.
2. No. #3 would not (necessarily) see the storage change.

id:
970
timestamp:
2025-03-20T18:43:43.006Z
sender:
@gav:polkadot.io
content:
The thing to understand is that JAM services are very much designed to be only asynchronously interactive, at least at the accumulation stage.

id:
969
timestamp:
2025-03-20T18:44:22.961Z
sender:
@gav:polkadot.io
content:
The prerequisite functionality is there to ensure that a package doesn't get accumulated before another package is known to be accumulatable.

id:
968
timestamp:
2025-03-20T18:44:35.539Z
sender:
@gav:polkadot.io
content:
It is not there to force a total ordering on its constituent work items.

id:
967
timestamp:
2025-03-20T18:45:07.062Z
sender:
@gav:polkadot.io
content:
This would create a potentially troublesome pattern and may over-extend the queue system.

id:
966
timestamp:
2025-03-20T18:45:51.234Z
sender:
@gav:polkadot.io
content:
A total ordering is possible within a single service.

id:
965
timestamp:
2025-03-20T18:46:43.929Z
sender:
@gav:polkadot.io
content:
* A total ordering is possible within a single service; by creating dependent packages, you can be certain that certain WIs will not be accumulated in batches before others. You may end up with a dependency in the same batch, but then that's up to the service code to apply the appropriate ordering.

id:
964
timestamp:
2025-03-20T18:47:15.487Z
sender:
@gav:polkadot.io
content:
If you're looking to synchronise between services, then you'll need to use `transfer`s..

id:
963
timestamp:
2025-03-20T18:47:16.796Z
sender:
@gav:polkadot.io
content:
* If you're looking to synchronise between services, then you'll need to use `transfer`s.

id:
962
timestamp:
2025-03-20T18:49:29.448Z
sender:
@gav:polkadot.io
content:
Transfers can be combined with co-scheduling at the Refine stage (e.g. sharing the same WP) and it becomes possible to create causal entanglement between the WIs of multiple services which can be enforced at the accumulation stage.

id:
961
timestamp:
2025-03-20T18:49:58.684Z
sender:
@gav:polkadot.io
content:
* Transfers can be combined with co-scheduling at the Refine stage (e.g. sharing the same WP) and it becomes possible to create causal entanglement between the WIs of multiple services which can be enforced at the accumulation stage, so the entangled effects only get integrated into state when both sides are known to be completable.

id:
960
timestamp:
2025-03-20T18:50:49.795Z
sender:
@gav:polkadot.io
content:
* It is not there to force a total ordering over its constituent work items; and certainly not over multiple services.

id:
959
timestamp:
2025-03-20T18:51:13.928Z
sender:
@gav:polkadot.io
content:
* This would create a potentially troublesome pattern and may over-extend the queue system and reduce potential parallelisability for accumulate.

id:
958
timestamp:
2025-03-20T18:53:46.780Z
sender:
@gav:polkadot.io
content:
Accumulation is designed with a view to becoming parallelisable. At some later revision of JAM we may e.g. increase the number of cores to 682 with a requirement that a single service cannot regularly have more accumulation gas than is possible with 341 of them. We can't squeeze more gas into the synchronous pipeline, so this would be made viable through CPU parallelism executing multiple service's simultaneously.

id:
957
timestamp:
2025-03-20T18:54:45.773Z
sender:
@gav:polkadot.io
content:
This model breaks more as work-items between services become orderable and synchronous dependencies - chains of execution - start to become the norm.

id:
956
timestamp:
2025-03-20T18:54:56.321Z
sender:
@gav:polkadot.io
content:
So it's something I'm really trying to avoid with this design.

id:
955
timestamp:
2025-03-20T18:56:01.130Z
sender:
@gav:polkadot.io
content:
In short synchronous dependencies are evil and the death of scalability. We want to keep them off-chain.

id:
954
timestamp:
2025-03-20T18:56:28.550Z
sender:
@gav:polkadot.io
content:
* In short, cross-service execution dependencies force synchroneity are evil and the death of scalability. We want to keep them off-chain.

id:
953
timestamp:
2025-03-20T18:56:34.689Z
sender:
@gav:polkadot.io
content:
* In short, cross-service execution dependencies force synchroneity and are evil and the death of scalability. We want to keep them off-chain.

id:
2188
timestamp:
2025-03-20T18:58:22.108Z
sender:
@gav:polkadot.io
content:
Which version of GP are you working from - 0.6.3 saw the introduction of the metadata prefix to program blobs.

id:
952
timestamp:
2025-03-20T19:03:55.037Z
sender:
@prematurata:matrix.org
content:
perfect gavin. Thanks for this indepth explanation. it was very much needed for me... I was trying to make both the "dependency system"/prerequisite and parallelism work together. 



id:
2187
timestamp:
2025-03-20T19:09:04.828Z
sender:
@eclesiomelo:matrix.org
content:
We are using the most updated version, 0.6.4, and the definition is this one https://graypaper.fluffylabs.dev/#/68eaa1f/2bc1022bc102?v=0.6.4, right?

id:
2186
timestamp:
2025-03-20T19:29:17.395Z
sender:
@gav:polkadot.io
content:
That’s the program blob but not purely what is stored. There is also a metadata prefix 

id:
2185
timestamp:
2025-03-20T19:31:04.357Z
sender:
@gav:polkadot.io
content:
You can see in eg eq. (9.4)

id:
2184
timestamp:
2025-03-20T22:30:48.036Z
sender:
@eclesiomelo:matrix.org
content:
Oh, we have missed that prefixed metadata, after reading it from the blob we are able to correctly parse the standard program defined at A.37! Really thanks! 😊

id:
951
timestamp:
2025-03-21T14:34:53.924Z
sender:
@tvvkk7:matrix.org
content:
Hello, I'm implementing PVM invocations, but I'm curious about how we get the actual service code, or in other words the standard program codes. 
Take [on-transfer invocation](https://graypaper.fluffylabs.dev/#/68eaa1f/2fc8002fca00?v=0.6.4) for example, we input service codeHash into argument invocations. But, service codeHash is a 32-octet value. How do we get the program codes through service codeHash ? 

id:
950
timestamp:
2025-03-21T14:54:42.633Z
sender:
@gav:polkadot.io
content:
There’s a function, big Lambda. This should define how to derive the hash preimage. 

id:
949
timestamp:
2025-03-21T14:54:51.145Z
sender:
@gav:polkadot.io
content:
It’s all well defined. 

id:
948
timestamp:
2025-03-21T14:55:07.510Z
sender:
@gav:polkadot.io
content:
You’ll need to utilise the preimage lookup map. 

id:
947
timestamp:
2025-03-21T14:57:43.031Z
sender:
@tvvkk7:matrix.org
content:
Much appreciated 

id:
2183
timestamp:
2025-03-21T15:29:07.772Z
sender:
@emielsebastiaan:matrix.org
content:
ima_792adcf.jpeg

id:
2182
timestamp:
2025-03-21T15:29:37.088Z
sender:
@emielsebastiaan:matrix.org
content:
Big day!! Today we achieved full milestone 1 conformance for the JAM Implementers Prize.

PyJAMaz will be open-sourced when the prize rules allow for it.

https://x.com/jamdottech/status/1903106367826677887?s=46&t=ThX7Y87rr1MIKyk6af4OXg

id:
2181
timestamp:
2025-03-21T17:11:16.360Z
sender:
@gav:polkadot.io
content:
> <@emielsebastiaan:matrix.org> Big day!! Today we achieved full milestone 1 conformance for the JAM Implementers Prize.
> 
> PyJAMaz will be open-sourced when the prize rules allow for it.
> 
> https://x.com/jamdottech/status/1903106367826677887?s=46&t=ThX7Y87rr1MIKyk6af4OXg

We’re at 0.6.4?:))

id:
2180
timestamp:
2025-03-21T17:11:27.294Z
sender:
@gav:polkadot.io
content:
> <@emielsebastiaan:matrix.org> Big day!! Today we achieved full milestone 1 conformance for the JAM Implementers Prize.
> 
> PyJAMaz will be open-sourced when the prize rules allow for it.
> 
> https://x.com/jamdottech/status/1903106367826677887?s=46&t=ThX7Y87rr1MIKyk6af4OXg

* We’re at 0.6.4!:))

id:
2179
timestamp:
2025-03-21T17:49:16.096Z
sender:
@emielsebastiaan:matrix.org
content:
> <@gav:polkadot.io> We’re at 0.6.4!:))

Development branches for 0.6.3 & 0.6.4 are pending review. They did not make our weekly ‘Merge Friday’. 

id:
946
timestamp:
2025-03-21T22:19:06.506Z
sender:
@celadari:matrix.org
content:
Hi guys, small questions:  
- Is ε(tₐ, tᵦ, tₜ, tᵧ, tₘ, tₒ, tᵢ) used in the definition of the host call function info Ωᵢ the same encoding as **𝐚 ∼ 𝓔₈(𝐚ᵦ, 𝐚ᵧ, 𝐚ₘ, 𝐚ₒ) ∼ 𝓔₄(𝐚ᵢ)**?
- Perhaps there's something I don't see, but in equation A.43 we define **u = ρ − max(ρ′, 0)**.  
  If ρ′ is negative, then **u = ρ**, which means the gas doesn't change.  
  So the service would have run code, but the gas stays the same — is that correct?  
  If so, is that the intended behavior?

id:
945
timestamp:
2025-03-22T04:56:38.996Z
sender:
@clearloop:matrix.org
content:
may I ask if this is part of the PVM tests or the stf of accumulation? I see everybody is talking about this however we haven't meet this yet 😅

id:
944
timestamp:
2025-03-22T04:56:46.843Z
sender:
@clearloop:matrix.org
content:
* may I ask if this is part of the PVM tests or the stf of accumulation? I see everybody is talking about this however we haven't met this yet 😅

id:
943
timestamp:
2025-03-22T04:57:01.120Z
sender:
@clearloop:matrix.org
content:
* may I ask if this is part of the new PVM tests or the stf of accumulation? I see everybody is talking about this however we haven't met this yet 😅

id:
942
timestamp:
2025-03-22T05:55:13.493Z
sender:
@gav:polkadot.io
content:
> <@celadari:matrix.org> Hi guys, small questions:  
> - Is ε(tₐ, tᵦ, tₜ, tᵧ, tₘ, tₒ, tᵢ) used in the definition of the host call function info Ωᵢ the same encoding as **𝐚 ∼ 𝓔₈(𝐚ᵦ, 𝐚ᵧ, 𝐚ₘ, 𝐚ₒ) ∼ 𝓔₄(𝐚ᵢ)**?
> - Perhaps there's something I don't see, but in equation A.43 we define **u = ρ − max(ρ′, 0)**.  
>   If ρ′ is negative, then **u = ρ**, which means the gas doesn't change.  
>   So the service would have run code, but the gas stays the same — is that correct?  
>   If so, is that the intended behavior?

On the second point, no. u is gas used. By ensuring the second term (gas counter) is never negative we just ensure that the gas used is never greater than the gas limit. 

id:
941
timestamp:
2025-03-22T05:58:35.445Z
sender:
@gav:polkadot.io
content:
On the first point, no. The former encoding uses the variable size numeric encodings. 

id:
940
timestamp:
2025-03-22T07:40:20.530Z
sender:
@celadari:matrix.org
content:
Oh thank you, I hadn't inderstood that u was used gas, makes sense

id:
939
timestamp:
2025-03-22T08:31:30.677Z
sender:
@celadari:matrix.org
content:
I see => thanks : )

id:
938
timestamp:
2025-03-22T09:01:53.901Z
sender:
@celadari:matrix.org
content:
Capture d’écran du 2025-03-22 10-28-25.png

id:
937
timestamp:
2025-03-22T09:01:55.163Z
sender:
@celadari:matrix.org
content:
Capture d’écran du 2025-03-22 10-29-08.png

id:
936
timestamp:
2025-03-22T09:02:18.398Z
sender:
@celadari:matrix.org
content:
Sorry actually you were asking about the first question or the second question ?

id:
935
timestamp:
2025-03-22T09:06:32.653Z
sender:
@clearloop:matrix.org
content:
sry I'm just curious about the host call part, I'm now updating our PVM tests while I don't see tests with host calls, so I assume it belongs to the accumulation stf?

id:
934
timestamp:
2025-03-22T11:21:14.993Z
sender:
@greywolve:matrix.org
content:
Is the explicit encoding of the [tuple in 5.6](https://graypaper.fluffylabs.dev/#/68eaa1f/0ce0000cf200?v=0.6.4) redundant since that's going to just remain an octet sequence after the outer encoding?  Or is there something special I'm missing?

id:
933
timestamp:
2025-03-22T11:24:14.489Z
sender:
@greywolve:matrix.org
content:
* Is the explicit encoding of the [tuple in 5.6](https://graypaper.fluffylabs.dev/#/68eaa1f/0ce0000cf200?v=0.6.4) redundant since that's going to just remain an octet sequence after the outer encoding?  Or is there something special I'm missing? (i.e is it pretty much the same as the regular serialization in the appendix only the work report replaced with the hashed work report instead)

id:
2178
timestamp:
2025-03-22T11:25:05.139Z
sender:
@davxy:matrix.org
content:
Test vectors updated to track GP 0.6.4

https://github.com/w3f/jamtestvectors/pull/28


id:
2177
timestamp:
2025-03-22T11:32:10.529Z
sender:
@davxy:matrix.org
content:
https://github.com/davxy/jam-test-vectors/pull/30

id:
932
timestamp:
2025-03-22T11:36:51.656Z
sender:
@celadari:matrix.org
content:
Question regarding this:

i'' was removed in the definition of Psi_H in version 6.4 of the GP so my question   =>  ¿ do we advance the counter after ecalli instruction or not when we exit ecalli for the host call ?

Looking at this line looks like we don't advance it https://graypaper.fluffylabs.dev/#/68eaa1f/246700247200?v=0.6.4

but then it conflicts with idea of using i' (not using i'') during Psi_H call (https://graypaper.fluffylabs.dev/#/68eaa1f/2b9d012b9d01?v=0.6.4) where it would mean that i' is advanced after ecalli instruction for the host call ?

Thanks in advance for the clarification 🙏

id:
931
timestamp:
2025-03-22T11:38:37.276Z
sender:
@celadari:matrix.org
content:
PS: I tag Ivan Subotic  so he gets notified as well

id:
930
timestamp:
2025-03-22T11:56:39.900Z
sender:
@gav:polkadot.io
content:
> <@greywolve:matrix.org> Is the explicit encoding of the [tuple in 5.6](https://graypaper.fluffylabs.dev/#/68eaa1f/0ce0000cf200?v=0.6.4) redundant since that's going to just remain an octet sequence after the outer encoding?  Or is there something special I'm missing? (i.e is it pretty much the same as the regular serialization in the appendix only the work report replaced with the hashed work report instead)

It is done this way to avoid having to send all guarantees with the header. Merkle proofs can be provided for those which are sent on other channels. 

id:
929
timestamp:
2025-03-22T11:58:57.583Z
sender:
@gav:polkadot.io
content:
> <@celadari:matrix.org> Question regarding this:
> 
> i'' was removed in the definition of Psi_H in version 6.4 of the GP so my question   =>  ¿ do we advance the counter after ecalli instruction or not when we exit ecalli for the host call ?
> 
> Looking at this line looks like we don't advance it https://graypaper.fluffylabs.dev/#/68eaa1f/246700247200?v=0.6.4
> 
> but then it conflicts with idea of using i' (not using i'') during Psi_H call (https://graypaper.fluffylabs.dev/#/68eaa1f/2b9d012b9d01?v=0.6.4) where it would mean that i' is advanced after ecalli instruction for the host call ?
> 
> Thanks in advance for the clarification 🙏

ecalli is no different to other instructions regarding i’: i’ still represents the instruction immediately following and as per the definition of PsiH, we advance to it once the host call is resolved. 

id:
928
timestamp:
2025-03-22T13:10:28.899Z
sender:
@subotic:matrix.org
content:
Ahh, now I understand it. The program counter always advances as per (A.7) and additionally in the case of `ecalli`,  `epsilon is h x v_x` instead of `play`. Thanks!

id:
2176
timestamp:
2025-03-22T15:58:04.138Z
sender:
@ultracoconut:matrix.org
content:
 Mermelada Cubes 

🍓 Mermelada Core → The main validator cube (Core JAM Pi).
🫐 Mermelada Parachain → Additional cube with a collator for parachain testing.
🍊 Mermelada RPC → RPC node to interact with the network and visualize data.
🔥 Mermelada Extra! → A high-performance version with upgraded hardware, storage, and connectivity.
😋😅



id:
2175
timestamp:
2025-03-22T16:23:56.368Z
sender:
@danicuki:matrix.org
content:
> <@davxy:matrix.org> Test vectors updated to track GP 0.6.4
> 
> https://github.com/w3f/jamtestvectors/pull/28
> 

Awesome. Thanks for this. I see that you didn’t put any statistics for cores and services besides zeroes. Do you plan to add some vectors for those as well? 

id:
2174
timestamp:
2025-03-22T16:26:16.344Z
sender:
@danicuki:matrix.org
content:
> <@davxy:matrix.org> Test vectors updated to track GP 0.6.4
> 
> https://github.com/w3f/jamtestvectors/pull/28
> 

Did you update the Bandersnatch library in these tests? Many of our safrole tests are failing now. 

id:
2173
timestamp:
2025-03-22T17:12:53.851Z
sender:
@yu2c:matrix.org
content:
Small question: Why do you use `U64` for the [gas_used](https://github.com/davxy/jam-test-vectors/blob/fb3ce3ffa82833cc780338e9ab128e834e72b26e/jam-types-asn/jam-types.asn#L184) type instead of `Gas`, even though they have the same meaning? I know in the Graypaper, it's noted as `uint64`

id:
2172
timestamp:
2025-03-22T17:30:58.787Z
sender:
@jaymansfield:matrix.org
content:
Hey davxy, was it intentional that "exports" in CoreActivityRecord is U16, but "exports" in ServiceActivityRecord is U32? Just wanted to make it wasn't an oversight.

id:
2171
timestamp:
2025-03-22T19:25:55.792Z
sender:
@gav:polkadot.io
content:
It is intentional

id:
2170
timestamp:
2025-03-22T19:26:53.618Z
sender:
@gav:polkadot.io
content:
* It is intentional, though not important at present

id:
2169
timestamp:
2025-03-22T19:45:21.381Z
sender:
@davxy:matrix.org
content:
> <@danicuki:matrix.org> Awesome. Thanks for this. I see that you didn’t put any statistics for cores and services besides zeroes. Do you plan to add some vectors for those as well? 

Probably yes. But that is not on the top of my task list right now

id:
2168
timestamp:
2025-03-22T19:46:51.255Z
sender:
@davxy:matrix.org
content:
> <@danicuki:matrix.org> Did you update the Bandersnatch library in these tests? Many of our safrole tests are failing now. 

Yes that is expected. Please update to v0.1.2 (now also available on crates.io)

id:
2167
timestamp:
2025-03-22T19:48:41.834Z
sender:
@davxy:matrix.org
content:
> <@yu2c:matrix.org> Small question: Why do you use `U64` for the [gas_used](https://github.com/davxy/jam-test-vectors/blob/fb3ce3ffa82833cc780338e9ab128e834e72b26e/jam-types-asn/jam-types.asn#L184) type instead of `Gas`, even though they have the same meaning? I know in the Graypaper, it's noted as `uint64`

You mean in the asn1 syntax file? I suppose because in that context it is just an alias. I can change it btw 

id:
2166
timestamp:
2025-03-22T19:49:19.763Z
sender:
@davxy:matrix.org
content:
> <@yu2c:matrix.org> Small question: Why do you use `U64` for the [gas_used](https://github.com/davxy/jam-test-vectors/blob/fb3ce3ffa82833cc780338e9ab128e834e72b26e/jam-types-asn/jam-types.asn#L184) type instead of `Gas`, even though they have the same meaning? I know in the Graypaper, it's noted as `uint64`

* Do you mean in the asn1 syntax file? I suppose because in that context it is just an alias. I can change it btw

id:
2165
timestamp:
2025-03-22T19:50:27.827Z
sender:
@davxy:matrix.org
content:
> <@danicuki:matrix.org> Did you update the Bandersnatch library in these tests? Many of our safrole tests are failing now. 

* Yes that is expected. Please update to v0.1.2 (now also available on crates.io). See https://github.com/davxy/jam-test-vectors/blob/polkajam-vectors/safrole/README.md#%EF%B8%8F-warning-%EF%B8%8F

id:
2164
timestamp:
2025-03-22T20:22:50.278Z
sender:
@davxy:matrix.org
content:
> <@danicuki:matrix.org> Awesome. Thanks for this. I see that you didn’t put any statistics for cores and services besides zeroes. Do you plan to add some vectors for those as well? 

* Probably yes. But that is not at the top of my task list right now

id:
2163
timestamp:
2025-03-23T06:14:32.612Z
sender:
@yu2c:matrix.org
content:
No, I think it works well as it is. Thanks for the clarification!

id:
927
timestamp:
2025-03-24T09:56:27.477Z
sender:
@dakkk:matrix.org
content:
gav: ﻿what is the rationale of having core and service statistics into the protocol? While validators' statistics are useful as explained in the GP, there are no information of the usefulness of core and service statistics, and I'm unable to figure it out by myself

id:
926
timestamp:
2025-03-24T10:10:31.693Z
sender:
@emilkietzman:matrix.org
content:
Secondary markets of Agile Coretime like RegionX or Lastic - You could check Core utilizations in different projects and sell unused Coretime

id:
925
timestamp:
2025-03-24T11:16:19.133Z
sender:
@oliver.tale-yazdi:parity.io
content:
It was mentioned in the OpenDev call https://www.youtube.com/live/5kpgs7eb95M?si=AQu819sfQUwxgCRP&t=395

id:
924
timestamp:
2025-03-24T11:21:15.279Z
sender:
@dakkk:matrix.org
content:
I missed that, I'll watch it. Thank you

id:
2162
timestamp:
2025-03-24T12:35:24.863Z
sender:
@amritj:matrix.org
content:
A thought, not important for JAM development, but still want to send it out:


People do mock blockchains for not having many use cases except maybe cryptocurrency. The reason for success of cryptocurrencies and what made them different is there is a singular verifiable database, not millions of small databases for each bank that require and trust third-party solutions like SWIFT to securely transfer information between these databases.

But that's only one type of data being transferred daily - health care records, educational records, financial records, property deeds, and million other fucking important records that are still broken up in these mini databases. 

All these Important information is still fragmented, still requiring trust on these third party solutions.

Companies have been trying to build private blockchains for this, but their problem is the same - they are private, most of them are not more than a few nodes of the same company in different locations. They are not secure; an attack on just few nodes not only bring the whole network down but also compromise the integrity of all the data.

JAM is what they need. JAM is what will allow all these private blockchains to be secure and interconnected. These private blockchains could remain private and borrow the security guarantees from JAM by using ZK proofs or maybe some other tech. Even if some company nodes get compromised, it will only make the data of that specific company public but not have any affect on the integrity of the data 

I believe this is what required next.


I know this is not related to JAM development, but just a thought for other devs building on top of it to build strong tooling and support to make this possible, and maybe also for the marketing team :)

id:
2161
timestamp:
2025-03-24T12:35:47.656Z
sender:
@amritj:matrix.org
content:
* A thought, not important for JAM development, but still want to send it out:


People do mock blockchains for not having any use cases except maybe cryptocurrency. The reason for success of cryptocurrencies and what made them different is there is a singular verifiable database, not millions of small databases for each bank that require and trust third-party solutions like SWIFT to securely transfer information between these databases.

But that's only one type of data being transferred daily - health care records, educational records, financial records, property deeds, and million other fucking important records that are still broken up in these mini databases. 

All these Important information is still fragmented, still requiring trust on these third party solutions.

Companies have been trying to build private blockchains for this, but their problem is the same - they are private, most of them are not more than a few nodes of the same company in different locations. They are not secure; an attack on just few nodes not only bring the whole network down but also compromise the integrity of all the data.

JAM is what they need. JAM is what will allow all these private blockchains to be secure and interconnected. These private blockchains could remain private and borrow the security guarantees from JAM by using ZK proofs or maybe some other tech. Even if some company nodes get compromised, it will only make the data of that specific company public but not have any affect on the integrity of the data 

I believe this is what required next.


I know this is not related to JAM development, but just a thought for other devs building on top of it to build strong tooling and support to make this possible, and maybe also for the marketing team :)

id:
2160
timestamp:
2025-03-24T14:42:36.325Z
sender:
@decentration:matrix.org
content:
in `reports.results.refine_load` the zero values encode to 1 byte. For `gas-used` it is u64, given that 0 encodes to 1 byte, can you confirm that we now encode with C.6 serializer? without serializer i am expected there to be 8 bytes of zeroes. 

id:
2159
timestamp:
2025-03-24T17:20:43.821Z
sender:
@jaymansfield:matrix.org
content:
I think you have to use compact encoding for those new fields

id:
2158
timestamp:
2025-03-24T19:51:26.259Z
sender:
@danicuki:matrix.org
content:
Are you a JAM implementer and PBA alumni? Let me know!  

id:
923
timestamp:
2025-03-24T23:32:51.333Z
sender:
@celadari:matrix.org
content:
Question regarding host call functions Omeja_J(n=reject)

- if we apply it to an account `a` of index `s` => it is supposed to eliminate from the database only the components `a_c`, `a_b`,` a_g`, `a_m`, `a_o`, `a_i` (represented by C(255, s) in the trie) or `a_l`, `a_p`, `a_s` as well ? 

id:
2157
timestamp:
2025-03-25T02:11:41.264Z
sender:
@qiwei:matrix.org
content:
where in GP does it say these fields need to use compact encoding?

id:
2156
timestamp:
2025-03-25T10:52:28.532Z
sender:
@danicuki:matrix.org
content:
We are planning to have a Polkadot / JAM booth at ETHLisbon. We will need JAM implementors volunteers to be there to spread the word of JAM. React with ❤️ and reach out to me if you are willing to help.

id:
2155
timestamp:
2025-03-25T11:29:47.901Z
sender:
@wabkebab:matrix.org
content:
In general, is this something that JAM implementors would could help representing JAM in different events?

id:
2154
timestamp:
2025-03-25T11:29:58.861Z
sender:
@wabkebab:matrix.org
content:
* In general, is this something that JAM implementors would like to  help representing JAM in different events?

id:
2153
timestamp:
2025-03-25T11:30:41.212Z
sender:
@wabkebab:matrix.org
content:
I mean _you_ are making JAM a reality also

id:
2152
timestamp:
2025-03-25T11:31:28.122Z
sender:
@wabkebab:matrix.org
content:
we are drafting some "JAM evangelist" initiative we would share soon

id:
2151
timestamp:
2025-03-25T12:47:13.168Z
sender:
@wabkebab:matrix.org
content:
* we - Pala Labs, organisers of JAM Tour - are drafting some "JAM evangelist" initiative we would share soon

id:
922
timestamp:
2025-03-25T14:55:52.482Z
sender:
@jay_ztc:matrix.org
content:
Is the ordering of preimages fully defined in the GP? 12.35 specifies that the preimages extrinsic should be ordered by the (account, preimage) tuples, but doesn't go into further detail. Looking at the tuple & sequence notation sections in section 3, there's not a default tuple ordering defined there either. 

 https://graypaper.fluffylabs.dev/#/68eaa1f/181001181001?v=0.6.4

id:
921
timestamp:
2025-03-25T14:56:06.118Z
sender:
@jay_ztc:matrix.org
content:
* Is the ordering of the preimages extrinsic fully defined in the GP? 12.35 specifies that the preimages extrinsic should be ordered by the (account, preimage) tuples, but doesn't go into further detail. Looking at the tuple & sequence notation sections in section 3, there's not a default tuple ordering defined there either. 

 https://graypaper.fluffylabs.dev/#/68eaa1f/181001181001?v=0.6.4

id:
920
timestamp:
2025-03-25T15:00:16.270Z
sender:
@gav:polkadot.io
content:
Tuples are ordered as is obvious. 

id:
919
timestamp:
2025-03-25T15:02:25.960Z
sender:
@gav:polkadot.io
content:
* Tuples are ordered in the usual, obvious way. 

id:
918
timestamp:
2025-03-25T15:03:09.218Z
sender:
@gav:polkadot.io
content:
Eg (1,1), (1,2), (2,1), (2,2)

id:
917
timestamp:
2025-03-25T15:09:42.257Z
sender:
@jay_ztc:matrix.org
content:
got it, thx for clarifying

id:
916
timestamp:
2025-03-25T16:31:34.034Z
sender:
@sourabhniyogi:matrix.org
content:
With v0.5's PVM 64-bit change (and a irreversible commitment to not supporting 32-bit PVM), it is reasonable to adjust https://graypaper.fluffylabs.dev/#/68eaa1f/2c15002c1500?v=0.6.4 to have a memory address space beyond 4GB?   If not, what are the technical reasons for continuing with this 32-bit layout?  

I believe we should extend it to include "accessed" (i) and "dirty" (e) bits so as to map into import and export host calls, and get a convention on how corevm services use the 4104-4096=8 bytes to keep the page number and these metadata bits -- are there others?

id:
915
timestamp:
2025-03-25T16:52:15.767Z
sender:
@sourabhniyogi:matrix.org
content:
* With v0.5's PVM 64-bit change (and a irreversible commitment to not supporting 32-bit PVM), it is reasonable to adjust https://graypaper.fluffylabs.dev/#/68eaa1f/2c15002c1500?v=0.6.4 to have a memory address space beyond 4GB?   If not, what are the technical reasons for continuing with this 32-bit layout?  

I believe we should extend it to include "accessed" (i) and "dirty" (e) bits so as to map into import and export host calls, and get a convention on how corevm services use the 4104-4096=8 bytes to keep the page number and these additional metadata bits -- are there additional metadata bits candidates per segment under consideration?

id:
914
timestamp:
2025-03-25T16:53:08.349Z
sender:
@sourabhniyogi:matrix.org
content:
* With v0.5's PVM 64-bit change (and a irreversible commitment to not supporting 32-bit PVM), it is reasonable to adjust https://graypaper.fluffylabs.dev/#/68eaa1f/2c15002c1500?v=0.6.4 to have a memory address space beyond 4GB?   If not, what are the technical reasons for continuing with this 32-bit layout?  

I believe we should extend it to include "accessed" (i) and "dirty" (e) bits so as to map into import and export host calls, and get a convention on how corevm services use the 4104-4096=8 bytes to keep the page number and these additional metadata bits -- are there additional metadata bits candidates per segment we should consider when designing CoreVM + Coreplay services?  

id:
913
timestamp:
2025-03-25T16:54:27.743Z
sender:
@sourabhniyogi:matrix.org
content:
* With v0.5's PVM 64-bit change (and a irreversible commitment to not supporting 32-bit PVM), it is reasonable to adjust https://graypaper.fluffylabs.dev/#/68eaa1f/2c15002c1500?v=0.6.4 to have a memory address space beyond 4GB?   If not, what are the technical reasons for continuing with this 32-bit layout?  

I believe we should extend it to include "accessed" (i) and "dirty" (e) bits so as to map into import and export host calls, and get a convention on how corevm services use the 4104-4096=8 bytes to keep the page number and these additional metadata bits.  I understand JAM protocol may not wish to impose constraints on what these additional 8 bytes, but a nevertheless pregnant question regardless is: are there additional metadata bits candidates per segment we should consider when designing CoreVM + Coreplay services?  

id:
912
timestamp:
2025-03-25T17:10:53.762Z
sender:
@sourabhniyogi:matrix.org
content:
* With v0.5's PVM 64-bit change (and a irreversible commitment to not supporting 32-bit PVM), it is reasonable to adjust https://graypaper.fluffylabs.dev/#/68eaa1f/2c15002c1500?v=0.6.4 to have a memory address space beyond 4GB?   If not, what are the technical reasons for continuing with this 32-bit layout?  

I believe we should extend it to include "accessed" (i) and "dirty" (e) bits so as to map into import and export host calls, and get a convention on how corevm services use the 4104-4096=8 bytes to keep the page number and these additional metadata bits.  I understand JAM protocol may not wish to impose constraints on what these additional 8 bytes contain (though I believe it makes sense to have these i+e bits to support OSes to run on JAM), but a nevertheless pregnant question regardless is: are there additional metadata bits candidates per segment we should consider when designing CoreVM + Coreplay services?  

id:
2150
timestamp:
2025-03-25T18:04:17.114Z
sender:
@sourabhniyogi:matrix.org
content:
Can "JAM evangelists" get your assistance on having the DOOM (or Quake) demo at "foreign" places like this?

id:
2149
timestamp:
2025-03-25T18:51:04.783Z
sender:
@gav:polkadot.io
content:
I'm sure we can make that happen

id:
2148
timestamp:
2025-03-25T20:13:44.573Z
sender:
@leonidas_m:matrix.org
content:
Can someone confirm that the encoding of refine_load in the codec vectors is correct? Because I am also confused regarding this

id:
2147
timestamp:
2025-03-25T20:24:24.227Z
sender:
@gav:polkadot.io
content:
As per the GP, the new statistic-related items in Work Reports are compactly encode.d

id:
2146
timestamp:
2025-03-25T20:24:25.715Z
sender:
@gav:polkadot.io
content:
* As per the GP, the new statistic-related items in Work Reports are compactly encoded.

id:
2145
timestamp:
2025-03-25T20:24:43.156Z
sender:
@gav:polkadot.io
content:
fwiw, this is to save some very valuable space in work-reports.

id:
2144
timestamp:
2025-03-25T20:24:50.342Z
sender:
@gav:polkadot.io
content:
* As per the GP, the new statistic-related items in work-reports are compactly encoded.

id:
2143
timestamp:
2025-03-25T20:25:13.803Z
sender:
@gav:polkadot.io
content:
there's already an issue in the GP repo to look for other possible items which can reasonably be compactly encoded

id:
2142
timestamp:
2025-03-25T20:29:05.240Z
sender:
@leonidas_m:matrix.org
content:
by compact you mean general encoding C.6 right?

id:
911
timestamp:
2025-03-25T20:31:28.533Z
sender:
@sourabhniyogi:matrix.org
content:
* With v0.5's PVM 64-bit change (and a irreversible commitment to not supporting 32-bit PVM), it is reasonable to adjust https://graypaper.fluffylabs.dev/#/68eaa1f/2c15002c1500?v=0.6.4 to have a memory address space beyond 4GB?   If not, what are the technical reasons for continuing with this 32-bit layout?  

I believe we should extend the "A" [here](https://graypaper.fluffylabs.dev/#/68eaa1f/0a7e010a7e01?v=0.6.4) to include "accessed" (i) and "dirty" (e) bits so as to map into what pages must be imported and what pages must be exported, thus treating coreVM OSes specially.  Or get at least a convention on how corevm services use the 4104-4096=8 bytes to keep the page number and these additional metadata bits.  I understand JAM protocol may not wish to impose constraints on what these additional 8 bytes contain (though I believe it makes sense to have these i+e bits to support OSes to run on JAM), but a nevertheless pregnant question regardless is: are there additional metadata bits candidates per segment we should consider when designing CoreVM + Coreplay services?  

id:
910
timestamp:
2025-03-25T20:31:45.393Z
sender:
@gav:polkadot.io
content:
No idea what you're talking about. Please reframe it in specific terms of Omega_J.

id:
909
timestamp:
2025-03-25T20:33:08.944Z
sender:
@gav:polkadot.io
content:
We can't actually handle 4 GB of allocations. Realistically most programs will mostly execute with only 1MB actually accessible (maybe sometimes with 16MB, but only very rarely with more).

id:
908
timestamp:
2025-03-25T20:33:17.665Z
sender:
@gav:polkadot.io
content:
* We can't actually handle 4 GB of allocations in terms of gas. Realistically most programs will mostly execute with only 1MB actually accessible (maybe sometimes with 16MB, but only very rarely with more).

id:
907
timestamp:
2025-03-25T20:33:58.822Z
sender:
@gav:polkadot.io
content:
Gas will be scaled depending on how much memory you're accessing. It'll become impractical many orders of magnitude below 32-bit.

id:
906
timestamp:
2025-03-25T20:36:24.876Z
sender:
@jan:parity.io
content:
> If not, what are the technical reasons for continuing with this 32-bit layout?

Speed, as it makes sandboxing cheaper, and as Gav said you won't be able to use this much memory in practice anyway, so it's pointless to have a 64-bit address space.

id:
905
timestamp:
2025-03-25T20:36:50.428Z
sender:
@gav:polkadot.io
content:
And, for security (auditing) validators will need to be able to execute several refinements concurrently, probably around 10; we'll also need 2 guarantor refinements. If they all used, say, 4 GB of RAM, then validators would need to have 48GB of RAM free before we even start thinking about the state DB and various caches. 

id:
904
timestamp:
2025-03-25T20:37:10.355Z
sender:
@gav:polkadot.io
content:
That would probably push minimum requirements to beyond 64 GB per node, which is too much.

id:
903
timestamp:
2025-03-25T20:37:44.311Z
sender:
@gav:polkadot.io
content:
Any, in any case, there's no sensible on-chain use-case which would need 64-bit addressability.

id:
902
timestamp:
2025-03-25T21:03:51.334Z
sender:
@sourabhniyogi:matrix.org
content:
I read David Emett 's comment of "12MB is a limit on the number of pages read/written per WP. It is not the limit on the CoreVM memory." in the following way:
* A CoreVM service user actually really does have a 4GB virtual computer.  However, in any given work package, spanning say a few seconds of computation only	a small number of pages are accessed (imported) or written to (exported).
* Only W_M (3072 as of 0.6.4) pages = 12MB of them is realistic in JAM but it is only  _tiny fraction_ of the larger addressable subset of CoreVM 4GB memory.
* So one work package might access ABC (12MB) the next might access DEF (a different 12MB), the next EFG etc, none of which exceed W_M *individually* but in totality across multiple work packages exceed 12MB.  In this way, you could totally want much more than 4GB.
* When a builder submits a work package to a guarantor, JAM being basically an audit protocol of what happens in up to a W_M (12MB sized) sliver of memory of the larger 4GB computer did.  JAM is optimized for trustless OS services.
In previous decades there was a story of "640K ought to be enough for anybody" and maybe "I think there is a world market for about five computers" -- these days 4-8B people all having 4-8GB phones in their pocket and so perhaps the trustless supercomputing equivalent is "4GB ought to be enough for everyone" and "there is a world market for about 5 trustless supercomputers" =). 

id:
901
timestamp:
2025-03-25T21:04:19.386Z
sender:
@sourabhniyogi:matrix.org
content:
* I read David Emett 's comment of "12MB is a limit on the number of pages read/written per WP. It is not the limit on the CoreVM memory." in the following way:

- A CoreVM service user actually really does have a 4GB virtual computer.  However, in any given work package, spanning say a few seconds of computation only	a small number of pages are accessed (imported) or written to (exported).
- Only W\_M (3072 as of 0.6.4) pages = 12MB of them is realistic in JAM but it is only  _tiny fraction_ of the larger addressable subset of CoreVM 4GB memory.
- So one work package might access ABC (12MB) the next might access DEF (a different 12MB), the next EFG etc, none of which exceed W\_M _individually_ but in totality across multiple work packages exceed 12MB.  In this way, you could totally want much more than 4GB.
- When a builder submits a work package to a guarantor, JAM being basically an audit protocol of what happens in up to a W\_M (12MB sized) sliver of memory of the larger 4GB computer did.  JAM is optimized for trustless OS services.

In previous decades there was a story of "640K ought to be enough for anybody" and maybe "I think there is a world market for about five computers" -- these days 4-8B people all have 4-8GB phones in their pocket and so perhaps the trustless supercomputing equivalent is "12MB/4GB ought to be enough for everyone" and "there is a world market for about 5 trustless supercomputers" =).

id:
900
timestamp:
2025-03-25T21:06:31.261Z
sender:
@sourabhniyogi:matrix.org
content:
* I read David Emett 's comment of "12MB is a limit on the number of pages read/written per WP. It is not the limit on the CoreVM memory." in the following way:

- A CoreVM service user actually really does have a 4GB virtual computer.  However, in any given work package, spanning say a few seconds of computation only	a small number of pages are accessed (imported) or written to (exported).
- Only W\_M (3072 as of 0.6.4) pages = 12MB of them is realistic in JAM but it is only  _tiny fraction_ of the larger addressable subset of the CoreVM service users's 4GB virtual computer.
- So one work package might access ABC (12MB) the next might access DEF (a different 12MB), the next EFG etc, none of which exceed W\_M _individually_ but in totality across multiple work packages exceed 12MB.  In this way, you could totally want much more than 4GB.
- When a builder submits a work package to a guarantor, JAM being basically an audit protocol of what happens in up to a W\_M (12MB sized) sliver of memory of the larger 4GB computer did.  JAM is optimized for trustless OS services.

In previous decades there was a story of "640K ought to be enough for anybody" and maybe "I think there is a world market for about five computers" -- these days 4-8B people all have 4-8GB phones in their pocket and so perhaps the trustless supercomputing equivalent is "12MB/4GB ought to be enough for everyone" and "there is a world market for about 5 trustless supercomputers" =).  Could we imagine that all 4-8B people collectively get their Shared World Computer in a 64-bit way so they may all coreplay together, even though any individual work package only references a tiny sliver?

id:
899
timestamp:
2025-03-25T21:10:39.723Z
sender:
@sourabhniyogi:matrix.org
content:
* I read David Emett 's comment of "12MB is a limit on the number of pages read/written per WP. It is not the limit on the CoreVM memory." in the following way:

- A CoreVM service user actually really does have a 4GB virtual computer.  However, in any given work package, spanning say a few seconds of computation only	a small number of pages are accessed (imported) or written to (exported).
- Only W\_M (3072 as of 0.6.4) pages = 12MB of this 4GB virtual computer is realistic to set up with JAM's PVM but it is only  _tiny fraction_ of the larger addressable subset of the CoreVM service users's 4GB virtual computer.
- So one work package might access ABC (12MB) the next might access DEF (a different 12MB), the next EFG etc, none of which exceed W\_M _individually_ but in totality across multiple work packages exceed 12MB.  In this way, you could totally want much more than 4GB.
- When a builder submits a work package to a guarantor, JAM being basically an audit protocol of what happens in up to a W\_M (12MB sized) sliver of memory of the larger 4GB computer did.  JAM is optimized for trustless OS services.

In previous decades there was a story of "640K ought to be enough for anybody" and maybe "I think there is a world market for about five computers" -- these days 4-8B people all have 4-8GB phones in their pocket and so perhaps the trustless supercomputing equivalent is "12MB/4GB ought to be enough for everyone" and "there is a world market for about 5 trustless supercomputers" =).  Could we imagine that all 4-8B people collectively get their Shared World Computer in a 64-bit way so they may all coreplay together, even though any individual work package only references a tiny sliver?

id:
898
timestamp:
2025-03-25T21:14:07.954Z
sender:
@gav:polkadot.io
content:
I appreciate the ambition, but the limits are there for a reason.

id:
897
timestamp:
2025-03-25T21:14:33.804Z
sender:
@gav:polkadot.io
content:
We've explained the reasoning. Live with it or come up with a better protocol yourself.

id:
896
timestamp:
2025-03-25T21:16:48.728Z
sender:
@gav:polkadot.io
content:
There's the business of dreaming and the business of building. This channel is for the latter.

id:
895
timestamp:
2025-03-25T21:30:39.426Z
sender:
@sourabhniyogi:matrix.org
content:
* I read David Emett 's comment of "12MB is a limit on the number of pages read/written per WP. It is not the limit on the CoreVM memory." in the following way:

- A CoreVM service user actually really does have a 4GB virtual computer.  However, in any given work package, spanning say a few seconds of computation only	a small number of pages are accessed (imported) or written to (exported).
- Only W\_M (3072 as of 0.6.4) pages = 12MB of this 4GB virtual computer is realistic to set up with JAM's PVM but it is only  _tiny fraction_ of the larger addressable subset of the CoreVM service users's 4GB virtual computer.
- So one work package might access ABC (12MB) the next might access DEF (a different 12MB), the next EFG etc, none of which exceed W\_M _individually_ but in totality across multiple work packages exceed 12MB.  In this way, you could totally want much more than 4GB.
- When a builder submits a work package to a guarantor, JAM being basically an audit protocol of what happens in up to a W\_M (12MB sized) sliver of memory of what the larger 4GB computer did.  JAM is optimized for trustless OS services.

In previous decades there was a story of "640K ought to be enough for anybody" and maybe "I think there is a world market for about five computers" -- these days 4-8B people all have 4-8GB phones in their pocket and so perhaps the trustless supercomputing equivalent is "12MB/4GB ought to be enough for everyone" and "there is a world market for about 5 trustless supercomputers" =).  Could we imagine that all 4-8B people collectively get their Shared World Computer in a 64-bit way so they may all coreplay together, even though any individual work package only references a tiny sliver?

id:
894
timestamp:
2025-03-25T21:33:47.861Z
sender:
@celadari:matrix.org
content:
Thanks for pointing out my message wasn’t clear and asking for clarification — I appreciate it 🙂

Speaking in terms of **`Omega_J`**:
– we're removing an account `d` (https://graypaper.fluffylabs.dev/#/68eaa1f/32a50332a803?v=0.6.4)

Speaking in terms of the **trie root (Appendix D)**:
By removing this account `d`, the state root trie will eventually need to be updated.

My question is:
– Do we remove only the first component of the account in the trie? (https://graypaper.fluffylabs.dev/#/68eaa1f/382b03383b03?v=0.6.4)
– Or do we remove other components of account (storage, pre_image lookup, ...) as well?
(https://graypaper.fluffylabs.dev/#/68eaa1f/385403386103?v=0.6.4, https://graypaper.fluffylabs.dev/#/68eaa1f/384103384e03?v=0.6.4, https://graypaper.fluffylabs.dev/#/68eaa1f/387603387f03?v=0.6.4)

id:
893
timestamp:
2025-03-25T21:34:09.331Z
sender:
@celadari:matrix.org
content:
* Thanks for pointing out my message wasn’t clear and asking for clarification — I appreciate it 🙂

Speaking in terms of **`Omega_J`**:
– we're removing an account `d` (https://graypaper.fluffylabs.dev/#/68eaa1f/32a50332a803?v=0.6.4)

Speaking in terms of the **trie root (Appendix D)**:
By removing this account `d`, the state root trie will need to be updated at some point.

My question is:
– Do we remove only the first component of the account in the trie? (https://graypaper.fluffylabs.dev/#/68eaa1f/382b03383b03?v=0.6.4)
– Or do we remove other components of account (storage, pre\_image lookup, ...) as well?
(https://graypaper.fluffylabs.dev/#/68eaa1f/385403386103?v=0.6.4, https://graypaper.fluffylabs.dev/#/68eaa1f/384103384e03?v=0.6.4, https://graypaper.fluffylabs.dev/#/68eaa1f/387603387f03?v=0.6.4)

id:
892
timestamp:
2025-03-25T21:35:57.616Z
sender:
@celadari:matrix.org
content:
* Thanks for pointing out my message wasn’t clear and asking for clarification — I appreciate it 🙂

Speaking in terms of **`Omega_J`**:
– we're removing an account `d` (https://graypaper.fluffylabs.dev/#/68eaa1f/32a50332a803?v=0.6.4)

Speaking in terms of the **trie root (Appendix D)**:
By removing this account `d`, the state root trie will need to be updated at some point.

My question is:
– Do we remove only the first component of the account in the trie? (https://graypaper.fluffylabs.dev/#/68eaa1f/382b03383b03?v=0.6.4)
– Or do we remove other components of the account (storage, pre_image lookup, ...) as well?
(https://graypaper.fluffylabs.dev/#/68eaa1f/385403386103?v=0.6.4, https://graypaper.fluffylabs.dev/#/68eaa1f/384103384e03?v=0.6.4, https://graypaper.fluffylabs.dev/#/68eaa1f/387603387f03?v=0.6.4)

id:
891
timestamp:
2025-03-25T21:54:40.975Z
sender:
@gav:polkadot.io
content:
You can answer your own question if you simply phrase it in terms of Omega_J.

id:
890
timestamp:
2025-03-25T21:56:30.845Z
sender:
@gav:polkadot.io
content:
Omega_J has the effect of removing a particular item (d) from the accounts dictionary (delta).

id:
889
timestamp:
2025-03-25T21:57:38.733Z
sender:
@gav:polkadot.io
content:
And if you see the various data concerning accounts which makes up the state trie (from which the trie root may be derived), they're all defined through the contents of the accounts dictionary delta.

id:
888
timestamp:
2025-03-25T21:58:40.595Z
sender:
@gav:polkadot.io
content:
Therefore if the dictionary no longer contains a particular account, then state trie items such as the the stored data, preimages, &c which were concerning said account will no longer be in place.

id:
2141
timestamp:
2025-03-25T22:16:09.082Z
sender:
@mkchung:matrix.org
content:
https://graypaper.fluffylabs.dev/#/68eaa1f/1b7c011b7f01?v=0.6.4
https://graypaper.fluffylabs.dev/#/68eaa1f/139c00139c00?v=0.6.4

Is there a specific ordering for segment-root lookup L (or dictionary "D⟨K → V⟩" in general)?

id:
2140
timestamp:
2025-03-25T22:16:40.405Z
sender:
@danicuki:matrix.org
content:
Please share your team logos here: https://github.com/jamixir/jam-media

This is for the JAM Experience merch. 

id:
2139
timestamp:
2025-03-25T22:37:51.256Z
sender:
@gav:polkadot.io
content:
> <@mkchung:matrix.org> https://graypaper.fluffylabs.dev/#/68eaa1f/1b7c011b7f01?v=0.6.4
> https://graypaper.fluffylabs.dev/#/68eaa1f/139c00139c00?v=0.6.4
> 
> Is there a specific ordering for segment-root lookup L (or dictionary "D⟨K → V⟩" in general)?

Segment root lookup is never serialised soo ordering is moot. Dictionaries in general can be serialised and that is defined in the relevant appendix section. 

id:
2138
timestamp:
2025-03-25T22:38:00.525Z
sender:
@gav:polkadot.io
content:
> <@mkchung:matrix.org> https://graypaper.fluffylabs.dev/#/68eaa1f/1b7c011b7f01?v=0.6.4
> https://graypaper.fluffylabs.dev/#/68eaa1f/139c00139c00?v=0.6.4
> 
> Is there a specific ordering for segment-root lookup L (or dictionary "D⟨K → V⟩" in general)?

* Segment root lookup is never serialised so ordering it is moot. Dictionaries in general can be serialised and that is defined in the relevant appendix section. 

id:
2137
timestamp:
2025-03-25T23:33:25.748Z
sender:
@sourabhniyogi:matrix.org
content:
By C.24 https://graypaper.fluffylabs.dev/#/68eaa1f/379b02379b02?v=0.6.4 -- the segment root lookup l in the work report (included in the guarantee) is serialized and is important since 2 or 3 guarantors have to sign perfectly identical work reports.  Its silly to have segment roots map to segment roots, so the only entries in the work report would be work package keys mapping to segment roots.  So implementers faced with this segment root lookup ordering question must order by work package hash as described in https://graypaper.fluffylabs.dev/#/5f542d7/37bf0037bf00?v=0.6.4 -- check?

id:
2136
timestamp:
2025-03-25T23:33:53.445Z
sender:
@sourabhniyogi:matrix.org
content:
* By C.24 https://graypaper.fluffylabs.dev/#/68eaa1f/379b02379b02?v=0.6.4 -- the segment root lookup l in the work report (included in the guarantee) is serialized and is important since 2 or 3 guarantors have to sign perfectly identical work reports.  Its silly to have segment roots map to segment roots, so the only entries in the segment root lookup with the work report would be work package keys mapping to segment roots -- check?  So implementers faced with this segment root lookup ordering question must order by work package hash as described in https://graypaper.fluffylabs.dev/#/5f542d7/37bf0037bf00?v=0.6.4 -- check?

id:
2135
timestamp:
2025-03-25T23:51:37.496Z
sender:
@sourabhniyogi:matrix.org
content:
* By C.24 https://graypaper.fluffylabs.dev/#/68eaa1f/379b02379b02?v=0.6.4 -- the segment root lookup l in the work report (included in the guarantee) is serialized and is important since 2 or 3 guarantors have to sign perfectly identical work reports.  Since we don't need to map segment roots to identical segment roots, the only entries in the serialized segment root lookup within the serialized work report would be work package keys mapping to segment roots -- check?  So implementers faced with this segment root lookup ordering question must order by work package hash as described in https://graypaper.fluffylabs.dev/#/5f542d7/37bf0037bf00?v=0.6.4 -- check?

id:
2134
timestamp:
2025-03-25T23:52:28.039Z
sender:
@sourabhniyogi:matrix.org
content:
* By C.24 https://graypaper.fluffylabs.dev/#/68eaa1f/379b02379b02?v=0.6.4 -- the segment root lookup l in the work report (included in the guarantee) is serialized and is important since 2 or 3 guarantors have to sign perfectly identical work reports.  Since we don't need to map segment roots to identical segment roots, the only entries in the serialized segment root lookup within the serialized work report would be work package keys mapping to segment roots by this here https://graypaper.fluffylabs.dev/#/68eaa1f/1b78011ba101?v=0.6.4 -- check?  So implementers faced with this segment root lookup ordering question must order by work package hash as described in https://graypaper.fluffylabs.dev/#/5f542d7/37bf0037bf00?v=0.6.4 -- check?

id:
2133
timestamp:
2025-03-26T07:26:16.360Z
sender:
@gav:polkadot.io
content:
> <@mkchung:matrix.org> https://graypaper.fluffylabs.dev/#/68eaa1f/1b7c011b7f01?v=0.6.4
> https://graypaper.fluffylabs.dev/#/68eaa1f/139c00139c00?v=0.6.4
> 
> Is there a specific ordering for segment-root lookup L (or dictionary "D⟨K → V⟩" in general)?

* ~Segment root lookup is never serialised so ordering it is moot~. Dictionaries in general can be serialised and that is defined in the relevant appendix section. 

id:
2132
timestamp:
2025-03-26T07:27:23.584Z
sender:
@gav:polkadot.io
content:
> <@sourabhniyogi:matrix.org> By C.24 https://graypaper.fluffylabs.dev/#/68eaa1f/379b02379b02?v=0.6.4 -- the segment root lookup l in the work report (included in the guarantee) is serialized and is important since 2 or 3 guarantors have to sign perfectly identical work reports.  Since we don't need to map segment roots to identical segment roots, the only entries in the serialized segment root lookup within the serialized work report would be work package keys mapping to segment roots by this here https://graypaper.fluffylabs.dev/#/68eaa1f/1b78011ba101?v=0.6.4 -- check?  So implementers faced with this segment root lookup ordering question must order by work package hash as described in https://graypaper.fluffylabs.dev/#/5f542d7/37bf0037bf00?v=0.6.4 -- check?

Yes indeed, the previous answer is corrected. The second part still stands: dictionaries are encoded. 

id:
2131
timestamp:
2025-03-26T07:27:39.726Z
sender:
@gav:polkadot.io
content:
> <@sourabhniyogi:matrix.org> By C.24 https://graypaper.fluffylabs.dev/#/68eaa1f/379b02379b02?v=0.6.4 -- the segment root lookup l in the work report (included in the guarantee) is serialized and is important since 2 or 3 guarantors have to sign perfectly identical work reports.  Its silly to have segment roots map to segment roots, so the only entries in the work report would be work package keys mapping to segment roots.  So implementers faced with this segment root lookup ordering question must order by work package hash as described in https://graypaper.fluffylabs.dev/#/5f542d7/37bf0037bf00?v=0.6.4 -- check?

* Yes indeed, the previous answer is corrected. The second part still stands: dictionaries are encoded as per the appendix. 

id:
887
timestamp:
2025-03-26T07:36:57.539Z
sender:
@tvvkk7:matrix.org
content:
Hello gav , I'm implementing accumulation invocation. The [initializer function](https://graypaper.fluffylabs.dev/#/68eaa1f/2e36012e4001?v=0.6.4) I requires eta'_0. Does eta'_0 be input through [U](https://graypaper.fluffylabs.dev/#/68eaa1f/163702163702?v=0.6.4) ? Although, it is only needed in Psi_A.

id:
886
timestamp:
2025-03-26T09:21:03.765Z
sender:
@shawntabrizi:matrix.org
content:
As I recall, compact numbers in JAM are different than they are in SCALE and Polkadot today. Can someone write a small description of the new compact number format? ❤️

id:
885
timestamp:
2025-03-26T09:22:49.701Z
sender:
@shawntabrizi:matrix.org
content:
image.png

id:
884
timestamp:
2025-03-26T09:22:50.660Z
sender:
@shawntabrizi:matrix.org
content:
is that this?



id:
883
timestamp:
2025-03-26T09:22:58.755Z
sender:
@shawntabrizi:matrix.org
content:
need an ELI5 :)

id:
882
timestamp:
2025-03-26T09:24:12.071Z
sender:
@jan:parity.io
content:
One prefix byte plus payload. The prefix byte determines the length through the number of `1`s before the first `0`. The unused bits in the first byte are used for the payload. The first byte always contains the most significant bits. The rest of the bytes are always written in a little endian order. Can encode at most 64-bit numbers.

At most  7bit - 0xxxxxxx
At most 14bit - 10xxxxxx xxxxxxxx
At most 21bit - 110xxxxx xxxxxxxx xxxxxxxx
At most 28bit - 1110xxxx xxxxxxxx xxxxxxxx xxxxxxxx
At most 35bit - 11110xxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx
At most 42bit - 111110xx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx
At most 49bit - 1111110x xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx
At most 56bit - 11111110 xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx
At most 64bit - 11111111 xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx


id:
881
timestamp:
2025-03-26T09:24:57.319Z
sender:
@knight1205:matrix.org
content:
is there any specification for work package builders, yet? 

id:
880
timestamp:
2025-03-26T09:30:04.183Z
sender:
@clearloop:matrix.org
content:
hi there, as for the riscv test https://github.com/spacejamapp/jam-test-vectors/blob/main/pvm/programs/riscv_rv64ua_amoadd_d.json, it doesn't have jump table defined, (the program starts with [0, 0]), how do we perform jumps in the bytecode?

id:
879
timestamp:
2025-03-26T09:31:17.594Z
sender:
@clearloop:matrix.org
content:
* hi there, as for the riscv test https://github.com/spacejamapp/jam-test-vectors/blob/main/pvm/programs/riscv\_rv64ua\_amoadd\_d.json, it doesn't have jump table defined, (the program starts with \[0, 0\]), however seems jumps are required for executing the complete logic, but how do we perform jumps in the bytecode since we don't have the jump table?

id:
878
timestamp:
2025-03-26T09:31:25.012Z
sender:
@xlchen:matrix.org
content:
I give my code to chatgpt and this is the description from it
```
This encoding converts an unsigned integer into a sequence of bytes using just as many bytes as needed. Here’s how it works in simple terms:
	1.	Zero Handling:
If the integer is 0, it simply outputs one byte with the value 0.
	2.	Determining Byte Count:
For nonzero numbers, the encoder figures out how many extra bytes are needed by finding the smallest number l (from 0 to 7) such that the value fits within 7 \times (l+1) bits. If none of these work, it defaults to using 8 bytes in total.
	3.	Control Byte Creation:
The first byte (control byte) combines a prefix that indicates how many extra bytes follow and part of the number itself. The prefix is calculated as:
\text{prefix} = 256 - (1 \ll (8 - l))
The control byte is then formed by adding this prefix to the most significant bits of the number.
	4.	Appending Remaining Bytes:
After the control byte, the remaining bytes (if any) represent the lower parts of the number, each taking an 8-bit chunk.

In summary, the format starts with a control byte that tells you how many additional bytes there are and includes part of the data, followed by the extra bytes that complete the full representation of the integer.
```

id:
877
timestamp:
2025-03-26T09:32:09.192Z
sender:
@clearloop:matrix.org
content:
* hi there, as for the riscv test https://github.com/koute/jamtestvectors/blob/master_pvm_initial/pvm/programs/riscv_rv64ua_amoadd_d.json, it doesn't have jump table defined, (the program starts with \[0, 0\]), however seems jumps are required for executing the complete logic, but how do we perform jumps in the bytecode since we don't have the jump table?

id:
876
timestamp:
2025-03-26T09:33:56.810Z
sender:
@jan:parity.io
content:
Not sure I understand your question. That program contains only static jumps and it doesn't require a jump table.

id:
875
timestamp:
2025-03-26T09:35:23.244Z
sender:
@clearloop:matrix.org
content:
oh I got you, so there could be problems in my `djump` implementation, I'm referencing the jump table in all jump instructions

id:
874
timestamp:
2025-03-26T09:36:22.855Z
sender:
@jan:parity.io
content:
The only dynamic jump in that program is the jump that goes to the hardcoded special address which halts the program; no other jumps there use the jump table.

id:
873
timestamp:
2025-03-26T09:37:01.034Z
sender:
@jan:parity.io
content:
Non-dynamic jumps definitely shouldn't do anything with the jump table.

id:
872
timestamp:
2025-03-26T09:37:34.477Z
sender:
@clearloop:matrix.org
content:
I may need to fix more problems in my code, since I can pass all `test_instr_*` with my current implementation 😂

id:
871
timestamp:
2025-03-26T09:42:46.577Z
sender:
@gav:polkadot.io
content:
Specification, no, and there won't be.

id:
870
timestamp:
2025-03-26T09:43:18.183Z
sender:
@gav:polkadot.io
content:
There may be conventions, published APIs and/or SDKs to help create package builders.

id:
869
timestamp:
2025-03-26T09:44:23.112Z
sender:
@gav:polkadot.io
content:
But, the JAM protocol doesn't prescribe any means of building packages any more than it prescribes how you should create your service logic.

id:
868
timestamp:
2025-03-26T09:44:26.710Z
sender:
@gav:polkadot.io
content:
* But the JAM protocol doesn't prescribe any means of building packages any more than it prescribes how you should create your service logic.

id:
867
timestamp:
2025-03-26T09:45:53.540Z
sender:
@gav:polkadot.io
content:
It terms of data logistics, ideally builders will connect to the JAM network via inbuilt nodes (light or full, depending on the use-case and circumstances) and use internal node APIs to inject new packages. Nodes would then identify the right guarantors and send the package to them.

id:
866
timestamp:
2025-03-26T09:48:03.051Z
sender:
@gav:polkadot.io
content:
However, in the early days, we'll probably see RPCs being used by builder executables to deliver packages to nodes on testnets. It's definitely not something I'd want to see in production, but running & synching a full-node to insert a single work-package is plainly suboptimal and we don't have any light-clients yet.

id:
865
timestamp:
2025-03-26T09:48:57.743Z
sender:
@clearloop:matrix.org
content:
wait, I can reproduce my case in pvm debugger 

https://pvm.fluffylabs.dev/?program=0x0000808833000000018300ff9700103200330502330700000080330800f83a0a0000017ba782ab3e02000003c88b027ba23a02000003330400000080ab4b4c33050382ac3304ff97441f954400f8ab4c3a33050482ab3e02000003c88b027ba23a020000033304ff97441f954400f8ab4b1933050582ac33040000f88344ff97440cab4c0652050400287bff212941840a2904494924851492480a4932#/

at JUMP_IND, we triggers halt, if so, how do we run the rest of the logic to reach the expected test result? 

id:
864
timestamp:
2025-03-26T09:49:11.725Z
sender:
@clearloop:matrix.org
content:
* wait, I can reproduce my case in pvm debugger 

https://pvm.fluffylabs.dev/?program=0x0000808833000000018300ff9700103200330502330700000080330800f83a0a0000017ba782ab3e02000003c88b027ba23a02000003330400000080ab4b4c33050382ac3304ff97441f954400f8ab4c3a33050482ab3e02000003c88b027ba23a020000033304ff97441f954400f8ab4b1933050582ac33040000f88344ff97440cab4c0652050400287bff212941840a2904494924851492480a4932#/

at JUMP\_IND, we trigger halt, if so, how do we run the rest of the logic to reach the expected test result? 

id:
863
timestamp:
2025-03-26T09:49:32.068Z
sender:
@jan:parity.io
content:
You don't start execution at 0.

id:
862
timestamp:
2025-03-26T09:50:11.821Z
sender:
@clearloop:matrix.org
content:
oh I got it, missed the `initial_pc`, thanks!

id:
861
timestamp:
2025-03-26T09:50:14.881Z
sender:
@gav:polkadot.io
content:
That's not a question the GP (or I) can answer. How you get eta' into the appropriate place in your code for it to be able to calculate the initial machine state is entirely an implementation details.

id:
860
timestamp:
2025-03-26T09:50:15.933Z
sender:
@gav:polkadot.io
content:
* That's not a question the GP (or I) can answer. How you get eta' into the appropriate place in your code for it to be able to calculate the initial machine state is entirely an implementation detail.

id:
859
timestamp:
2025-03-26T09:50:18.897Z
sender:
@xlchen:matrix.org
content:
are validators expected to open connections with any builder nodes? obviously limits needs to be enforced on validators side but that can still be a DoS vector?

id:
858
timestamp:
2025-03-26T09:50:27.211Z
sender:
@gav:polkadot.io
content:
No.

id:
857
timestamp:
2025-03-26T09:50:46.464Z
sender:
@gav:polkadot.io
content:
The builder node set is unconstrained.

id:
856
timestamp:
2025-03-26T09:51:00.369Z
sender:
@gav:polkadot.io
content:
And even if it were somehow constrained, validators have no idea who they are.

id:
855
timestamp:
2025-03-26T09:51:26.791Z
sender:
@gav:polkadot.io
content:
Whereas validators IDs are well-known to anyone with an up to date JAM state.

id:
854
timestamp:
2025-03-26T09:52:22.661Z
sender:
@gav:polkadot.io
content:
Strategically, validators need to find work-packages they can guarantee in order to make rewards. But they must balance this with the possibility of being attacked.

id:
853
timestamp:
2025-03-26T09:52:28.355Z
sender:
@gav:polkadot.io
content:
* Strategically, validators need to find work-packages they can guarantee in order to make rewards. But they must balance this with the possibility of being DoSed/attacked.

id:
852
timestamp:
2025-03-26T09:53:36.297Z
sender:
@gav:polkadot.io
content:
So there will be some need for implementors to create guarantor strategies which balance these two opposing forces. There's not really a right answer here and it's the sort of thing which should be discussed at JAM0.

id:
851
timestamp:
2025-03-26T09:54:55.861Z
sender:
@gav:polkadot.io
content:
Realistically I'd expect validators to have several dozen nodes connected, other than fellow validators.

id:
850
timestamp:
2025-03-26T09:55:01.357Z
sender:
@xlchen:matrix.org
content:
I see. something to be figured out. worst case we just have tx pool and some package gossip protocol + peer reputation. ie something like what we have today 

id:
849
timestamp:
2025-03-26T09:55:37.863Z
sender:
@gav:polkadot.io
content:
And to actively churn through nodes, keeping ones who tend to give them good packages.

id:
848
timestamp:
2025-03-26T09:56:49.323Z
sender:
@gav:polkadot.io
content:
Connection could come with a promise to give packages adhering to a set of authorizers. Failure to provide a package on a core with one of those authorizers in the pool could result in booting.

id:
847
timestamp:
2025-03-26T09:57:12.714Z
sender:
@gav:polkadot.io
content:
Obviously if bad packages are received, then this would also result in booting.

id:
846
timestamp:
2025-03-26T10:12:37.414Z
sender:
@xlchen:matrix.org
content:
so for a builder to be able to consistently deliver work packages, it needs to work with all sorts of services. It certainly need a pool and some way to collect the packages. this is a big chunk of work

id:
845
timestamp:
2025-03-26T10:15:08.224Z
sender:
@gav:polkadot.io
content:
Highly unlikely.

id:
844
timestamp:
2025-03-26T10:15:33.963Z
sender:
@gav:polkadot.io
content:
Builders will almost certainly be private enterprises and specialised to a particular service or service-type.

id:
843
timestamp:
2025-03-26T10:16:05.446Z
sender:
@gav:polkadot.io
content:
E.g. for parachains, it could be that every parachain will have its own builder network (aka collator network).

id:
842
timestamp:
2025-03-26T10:16:33.676Z
sender:
@gav:polkadot.io
content:
* E.g. for parachains, it could be that every parachain will have its own builder network (aka collator network). Though with the Omninode, we'll probably see generic parachain builder networks.

id:
841
timestamp:
2025-03-26T10:16:54.877Z
sender:
@gav:polkadot.io
content:
But still, they'll only build for the one Parachains service.

id:
840
timestamp:
2025-03-26T10:17:45.120Z
sender:
@gav:polkadot.io
content:
It will be up to the builders to convince guarantors on cores which their packages are capable of running that they can furnish them with packages.

id:
839
timestamp:
2025-03-26T10:18:18.227Z
sender:
@gav:polkadot.io
content:
Thankfully this need not be done blindly; IsAuthorized is designed to run independently and cheaply.

id:
838
timestamp:
2025-03-26T10:19:04.710Z
sender:
@gav:polkadot.io
content:
And once IsAuthorized executes successfully, the guarantor knows that the builder can reasonably supply a package worth refining/guaranteeing.

id:
837
timestamp:
2025-03-26T10:35:17.954Z
sender:
@knight1205:matrix.org
content:
so the strategy to build connection and accept work packages will be fixed for each implementation, for consistency, or will there be different strategies? If fixed, will that be provided in JAM-NP?

id:
836
timestamp:
2025-03-26T10:36:17.398Z
sender:
@gav:polkadot.io
content:
JAM-SNP already contains network messages for provision/sharing of work-packages (and preimages)

id:
835
timestamp:
2025-03-26T10:37:51.257Z
sender:
@knight1205:matrix.org
content:
but that is just protocol for connection setup. will there be any strategy/requirements for acceptance or just we have to validate author hash or work package on our own and then perform computations?

id:
834
timestamp:
2025-03-26T10:38:01.680Z
sender:
@gav:polkadot.io
content:
As I just wrote:

id:
833
timestamp:
2025-03-26T10:38:02.902Z
sender:
@gav:polkadot.io
content:
> So there will be some need for implementors to create guarantor strategies which balance these two opposing forces. There's not really a right answer here and it's the sort of thing which should be discussed at JAM0

id:
832
timestamp:
2025-03-26T10:39:50.179Z
sender:
@gav:polkadot.io
content:
(For the purposes of M2 conformance testing we'll have idealised connections and implementations will not need to concern themselves with the possibility of DoS.)

id:
831
timestamp:
2025-03-26T10:41:41.857Z
sender:
@gav:polkadot.io
content:
As per the security audit (M5) implementations will need to demonstrate a resilience against DoS. But of course, over-conservative nodes which sacrifice too many rewards may find that fewer validators are willing to run them.

id:
830
timestamp:
2025-03-26T10:41:52.753Z
sender:
@gav:polkadot.io
content:
* As per the security audit (M5), implementations will need to demonstrate a resilience against DoS. But of course, over-conservative nodes which sacrifice too many rewards may find that fewer validators are willing to run them.

id:
829
timestamp:
2025-03-26T10:42:05.683Z
sender:
@gav:polkadot.io
content:
* As per the security audit (M5), implementations will need to demonstrate a resilience against DoS, including attacks by peers. But of course, over-conservative nodes which sacrifice too many rewards may find that fewer validators are willing to run them.

id:
828
timestamp:
2025-03-26T10:43:06.153Z
sender:
@gav:polkadot.io
content:
Again, no right answers, and I do expect (and hope for!) some differences between node strategies, but our implementor conferences are meant for brainstorming and sharing insights into such things.

id:
827
timestamp:
2025-03-26T10:43:30.788Z
sender:
@knight1205:matrix.org
content:
alright, got it. thank you very much

id:
826
timestamp:
2025-03-26T11:01:56.191Z
sender:
@dave:parity.io
content:
SNP currently allows builders to identify themselves at connection time by adding /builder to the protocol advertised during ALPN, see https://github.com/zdave-parity/jam-np/blob/main/simple.md#alpn. To some extent how validators treat these connections is a strategy thing and it isn't necessary for all implementations to behave the same. A reasonable strategy might be to grant a peer connecting with the /builder suffix a special builder connection slot (subject to availability), but require the peer to submit a valid work-package within a few seconds after connecting in order to keep the slot and not lose reputation.

id:
825
timestamp:
2025-03-26T11:51:23.705Z
sender:
@tvvkk7:matrix.org
content:
Many thanks ! I got it!

id:
824
timestamp:
2025-03-26T15:25:15.199Z
sender:
@celadari:matrix.org
content:
Thanks again for your time and answer.

Just to give a bit more context on why I was confused:

I hadn’t realized that the condition on 
`𝑑𝑖=2` was actually implying that the **storage**, **preimages**, etc. for that account had to be *already empty* — meaning the account must have gone through a `forget(Omega_F)` and `write(Omega_W)` before being eligible for deletion.

Initially, I thought we were supposed to manually remove these fields by directly deleting down from the partial trie key like
`𝐶(𝑠,𝐸4(2^32−1))`
which would have worked fine for **preImageLookupP** and **storage**, but not for **preImageLookupL** (because of `E(l)`) — and that one had me pulling my hair out 😅

id:
823
timestamp:
2025-03-26T15:25:33.765Z
sender:
@celadari:matrix.org
content:
* Thanks again for your time and answer.

Just to give a bit more context on why I was confused:

I hadn’t realized that the condition on
`𝑑𝑖=2` was actually implying that the **storage**, **preimages**, etc. for that account had to be _already empty_ — meaning the account must have gone through a `forget(Omega_F)` and `write(Omega_W)` before being eligible for deletion.

Initially, I thought we were supposed to manually remove these fields by directly deleting down from the partial trie key like
`𝐶(𝑠,𝐸4(2^32−1))`
which would have worked fine for **preImageLookupP** and **storage**, but not for **preImageLookupL** (because of `E(l)`) — and that one had me pulling my hair out 😅

All good now and thanks again :)

id:
2130
timestamp:
2025-03-26T22:59:20.491Z
sender:
@sourabhniyogi:matrix.org
content:
Thank you for clarifying -- We are currently using the segment root lookup within the work report to actually verify the justifications within the bundle and understand the segment root lookup piece of the work report to be absolutely critical to auditing.  If that's wrong, we'd appreciate being corrected as we think we have all the pieces fitting together finally.

In particular, we put some "guarantee test data" together in a new [0.6.4.0 guarantees dataset](https://github.com/jam-duna/jamtestnet/tree/0.6.4.0/guarantees), where we believe we have both generated and verified the self-justifying bundle exactly in the above audit situation.   

I wrote up a doc on what we think implementers should do with this [guarantees here](https://github.com/jam-duna/jamtestnet/issues/139) and hope we + others to slog through the heart of this central "puzzle".  

Earlier I was concerned about implementers coding to the test vectors and didn't want to share this kind of writeup (and give away the wrong puzzle answer, and superspread a JAM "mental disease", so to speak, in addition to taking away the delights of solving your puzzle in the first place), but *this* puzzle is so layered and from our month on the [0.6.2.x](https://github.com/jam-duna/jamtestnet/releases/tag/0.6.2.12), I've concluded implementers are so independent and puzzle-solving oriented that I'm not concerned anymore:  instead, I'm seeing lots of independent implementations against the same GP produce the anti-fragility you aimed for.  

However, if you are concerned, I'd appreciate what you think we should to do to modulate our collective activity a bit.  

id:
2129
timestamp:
2025-03-26T22:59:47.927Z
sender:
@sourabhniyogi:matrix.org
content:
* Thank you for clarifying -- We are currently using the segment root lookup within the work report to actually verify the justifications within the bundle and understand the segment root lookup piece of the work report to be absolutely critical to auditing.  If that's wrong, we'd appreciate being corrected as we think we have all the pieces fitting together finally.

In particular, we put some "guarantee test data" together in a new [0.6.4.0 guarantees dataset](https://github.com/jam-duna/jamtestnet/tree/0.6.4.0/guarantees), where we believe we have both generated and verified the self-justifying bundle exactly in the above audit situation.

I wrote up a doc on what we think implementers should do with this [guarantees here](https://github.com/jam-duna/jamtestnet/issues/139) and hope we + others to slog through the heart of this central "puzzle".

Earlier I was concerned about implementers coding to the test vectors and didn't want to share this kind of writeup (and give away the wrong puzzle answer, and superspread a JAM "mental disease", so to speak, in addition to taking away the delights of solving your puzzle in the first place), but _this_ puzzle is so layered and from our month on getting [0.6.2.x alignment](https://github.com/jam-duna/jamtestnet/releases/tag/0.6.2.12), I've concluded implementers are so independent and puzzle-solving oriented that I'm not concerned anymore:  instead, I'm seeing lots of independent implementations against the same GP produce the anti-fragility you aimed for.

However, if you are concerned, I'd appreciate what you think we should to do to modulate our collective activity a bit.

id:
2128
timestamp:
2025-03-26T23:00:53.579Z
sender:
@sourabhniyogi:matrix.org
content:
* Thank you for clarifying -- We are currently using the segment root lookup within the work report to actually verify the justifications within the bundle and understand the segment root lookup piece of the work report to be absolutely critical to auditing.  If that's wrong, we'd appreciate being corrected as we think we have all the pieces fitting together finally.

In particular, we put some "guarantee test data" together in a new [0.6.4.0 guarantees dataset](https://github.com/jam-duna/jamtestnet/tree/0.6.4.0/guarantees), where we believe we have both generated and verified the self-justifying bundle exactly in the above audit situation.

I wrote up a doc on what we think implementers should do with this [guarantees dataset here](https://github.com/jam-duna/jamtestnet/issues/139) and hope we + others can slog through the heart of this central "puzzle" together soon.

Earlier I was concerned about implementers coding to the test vectors and didn't want to share this kind of writeup (and give away the wrong puzzle answer, and superspread a JAM "mental disease", so to speak, in addition to taking away the delights of solving your puzzle in the first place), but _this_ puzzle is so layered and from our month on getting [0.6.2.x alignment](https://github.com/jam-duna/jamtestnet/releases/tag/0.6.2.12), I've concluded implementers are so independent and puzzle-solving oriented that I'm not concerned anymore:  instead, I'm seeing lots of independent implementations against the same GP produce the anti-fragility you aimed for.

However, if you are concerned, I'd appreciate what you think we should to do to modulate our collective activity a bit.

id:
2127
timestamp:
2025-03-27T11:50:34.301Z
sender:
@decentration:matrix.org
content:
i believe it is referring to C.6

id:
2126
timestamp:
2025-03-27T12:32:26.852Z
sender:
@danicuki:matrix.org
content:
Hi jammers, we are trying to set up a booth at  ETHLisbon to present JAM to event devs. We need a few volunteers to stay there during ETH Lisbon dates (9th-11th). LMK if you want to help

id:
822
timestamp:
2025-03-27T16:51:20.024Z
sender:
@jay_ztc:matrix.org
content:
should the pvm invocation definitions make explicit the use of the historical lookup function when representing an accounts code? Currently it looks like S_c is used as if it were intended to be the code preimage itself, rather than the code hash.

https://graypaper.fluffylabs.dev/#/68eaa1f/2fa9002fac00?v=0.6.4

id:
821
timestamp:
2025-03-27T23:13:46.114Z
sender:
@jay_ztc:matrix.org
content:
within the accumulate pvm invocation, is flushing S back to the partial state after read-only host functions intentional? (read, lookup, info). Seems like it should be moot.

https://graypaper.fluffylabs.dev/#/68eaa1f/2ebb022ebb02?v=0.6.4

id:
820
timestamp:
2025-03-28T08:38:57.814Z
sender:
@gav:polkadot.io
content:
s_{bold c} is the preimage (s_{regular c} is the hash)

id:
819
timestamp:
2025-03-28T08:39:47.234Z
sender:
@gav:polkadot.io
content:
* s\_{bold c} is the preimage (s\_{regular c} is the hash) - It is defined in (9.4)

id:
818
timestamp:
2025-03-28T08:40:13.247Z
sender:
@gav:polkadot.io
content:
* s\_{bold c} is the preimage (s\_{regular c} is the hash) - It is defined in (9.4), and doesn't rely on the preimage lookup function.

id:
817
timestamp:
2025-03-28T08:58:05.749Z
sender:
@gav:polkadot.io
content:
Yes it's a moot point; I'm happy to take a PR which simplifies it, though I'm not sure if that's necessarily easy.

id:
2125
timestamp:
2025-03-28T12:59:15.596Z
sender:
@stsoen:matrix.org
content:
​Actually, it makes sense. Moreover, if we look at the current Rollup Landscape, it appears as a patchwork quilt of different pieces. And the quilt is made not only of fabric but also of other materials that may be poorly compatible with each other. In most cases, there is no idea for existence other than "we made it, come here."​

JAM, on the other hand, offers a unified approach with reasonable modularity. Instead of immediately creating something complex with a large number of features unnecessary for most users, instances can be made more targeted for specific cases.​

And this applies not only to cases for companies and enterprises but also to more global and widespread things such as payment systems, data verification, creating common economic layers for a series of games or games from one studio. This also includes cases similar to Hyperliquid, and so on.​

At the same time, unlike AVS (Networks, and so on in different protocols) for restaking, which is actually properly developed only in EigenLayer, there is no need for an extra layer of "restaking." Moreover, the idea of restaking over the long term seems somewhat questionable because economically it does not work as expected.​

And JAM looks like something that is in superposition relative to all these options: it does not have the disadvantages of the above options but at the same time possesses their advantages.

id:
816
timestamp:
2025-03-28T13:34:41.008Z
sender:
@jay_ztc:matrix.org
content:
I see it now, thanks!

id:
815
timestamp:
2025-03-28T14:15:11.249Z
sender:
@jay_ztc:matrix.org
content:
https://github.com/gavofyork/graypaper/pull/313

id:
2124
timestamp:
2025-03-28T15:21:28.795Z
sender:
@gav:polkadot.io
content:
The JAM `top` is coming on - hope to release it soon along with a provisional node RPC spec so it can be used on all conformant JAM impls.

id:
2123
timestamp:
2025-03-28T15:21:38.720Z
sender:
@gav:polkadot.io
content:
image.png

id:
2122
timestamp:
2025-03-28T15:24:13.626Z
sender:
@boymaas:matrix.org
content:
Aaah this is why we are implementing 6.4 😀 😉

id:
2121
timestamp:
2025-03-28T15:24:21.257Z
sender:
@gav:polkadot.io
content:
Indeed:)

id:
2120
timestamp:
2025-03-28T17:59:02.857Z
sender:
@sourabhniyogi:matrix.org
content:
Schelling point request: should we consider host calls to log as having gas 10 or gas 0 --  https://hackmd.io/@polkadot/jip1 -- totally arbitrary choice right now 

id:
2119
timestamp:
2025-03-28T18:01:11.066Z
sender:
@sourabhniyogi:matrix.org
content:
* Schelling point request: should we consider host calls to log as having gas 10 or gas 0 --  https://hackmd.io/@polkadot/jip1 -- totally arbitrary choice right now, just want to pick one of the two.

id:
2118
timestamp:
2025-03-28T18:14:32.308Z
sender:
@gav:polkadot.io
content:
gas zero

id:
2117
timestamp:
2025-03-28T18:15:29.241Z
sender:
@gav:polkadot.io
content:
* gas zero - updated hackmd

id:
2116
timestamp:
2025-03-28T18:29:44.059Z
sender:
@emielsebastiaan:matrix.org
content:
> <@sourabhniyogi:matrix.org> Schelling point request: should we consider host calls to log as having gas 10 or gas 0 --  https://hackmd.io/@polkadot/jip1 -- totally arbitrary choice right now, just want to pick one of the two.

Non zero gas would potentially alter the flow of anything you’d wish to debug. So 0 gas please. 

id:
2115
timestamp:
2025-03-29T06:37:35.675Z
sender:
@clearloop:matrix.org
content:
may I ask what's the correct version of `ark-ec-vrfs` at 0.6.4? the [README](https://github.com/davxy/jam-test-vectors/tree/polkajam-vectors/safrole) specified 0.1.2 while it is yanked in crates.io

id:
2114
timestamp:
2025-03-29T06:37:54.806Z
sender:
@clearloop:matrix.org
content:
* may I ask what's the correct version of `ark-ec-vrfs` at 0.6.4? the [README](https://github.com/davxy/jam-test-vectors/tree/polkajam-vectors/safrole) specified 0.1.2 while it is yanked on crates.io

id:
2113
timestamp:
2025-03-29T07:00:38.864Z
sender:
@clearloop:matrix.org
content:
* may I ask what's the correct version of `ark-ec-vrfs` at 0.6.4? the [README](https://github.com/davxy/jam-test-vectors/tree/polkajam-vectors/safrole) specified 0.1.2 while it is yanked on crates.io

---

we are now using https://github.com/davxy/ark-vrf/tree/main?rev=bf2d1cf, everything works now

id:
2112
timestamp:
2025-03-29T07:33:56.213Z
sender:
@davxy:matrix.org
content:
> <@clearloop:matrix.org> may I ask what's the correct version of `ark-ec-vrfs` at 0.6.4? the [README](https://github.com/davxy/jam-test-vectors/tree/polkajam-vectors/safrole) specified 0.1.2 while it is yanked on crates.io

`ark-ec-vrfs` v0.1.2 works with 0.6.4.
However, future development will be done on [ark-vrf](https://github.com/davxy/ark-vrf) which is currently basically the same.



id:
814
timestamp:
2025-03-30T08:21:45.337Z
sender:
@celadari:matrix.org
content:
Hi everyone,

I have a question regarding **the program metadata introduced in GP 6.3.**

If an extrinsic `E_P` includes a `pre_image` that do not conform to the expected encoding
`Epsilon(double_arrow Epsilon(a_m), a_c)`
(as specified here: https://graypaper.fluffylabs.dev/#/68eaa1f/106c01107101?v=0.6.4):

Should we:

- Consider the **entire block invalid ?**
OR
- **Accept the block**, and allow the service lookup dictionaries to include these entries, with the understanding that invocations of Psi_A, Psi_R, Psi_I, Psi_T for the service of this pre_image would simply fail ?

id:
813
timestamp:
2025-03-30T08:23:53.478Z
sender:
@celadari:matrix.org
content:
* Hi everyone,
I have a question regarding the program metadata introduced in GP 6.3.
If an extrinsic E_P includes a pre_image that do not conform to the expected encoding
Epsilon(double_arrow Epsilon(a_m), a_c)
(as specified here: https://graypaper.fluffylabs.dev/#/68eaa1f/106c01107101?v=0.6.4):
Should we:
Consider the entire block invalid ?
OR
Accept the block, and allow the service lookup dictionaries to include these entries, with the understanding that invocations of Psi_A, Psi_R, Psi_I, Psi_T for the service of this pre_image would simply fail (by failing I mean that invocations panic thus don't change state) ?


id:
812
timestamp:
2025-03-30T08:25:07.278Z
sender:
@celadari:matrix.org
content:
* Hi everyone,

I have a question regarding the *program metadata introduced in GP* 6.3.

If an extrinsic `E_P` includes a `pre_image` that do not conform to the expected encoding
`Epsilon(double_arrow Epsilon(a_m), a_c)`
(as specified here: https://graypaper.fluffylabs.dev/#/68eaa1f/106c01107101?v=0.6.4):


Should we:
- *Consider the entire block invalid ?*
OR
- *Accept the block*, and allow the service lookup dictionaries to include these entries, with the understanding that invocations of `Psi_A, Psi_R, Psi_I, Psi_T` for the service of this pre_image would simply fail (by failing I mean that invocations panic thus don't change state) ?


id:
811
timestamp:
2025-03-30T11:55:23.521Z
sender:
@gav:polkadot.io
content:
does not conform to the expected encoding of what?!!

id:
810
timestamp:
2025-03-30T11:55:55.314Z
sender:
@gav:polkadot.io
content:
The preimage is determined solely by data encoded as per the GP specification. 

id:
809
timestamp:
2025-03-30T11:56:09.585Z
sender:
@gav:polkadot.io
content:
* does not conform to the expected encoding of what?

id:
808
timestamp:
2025-03-30T11:56:36.010Z
sender:
@gav:polkadot.io
content:
Either it is requested or it is not. If it is not, then the block is invalid. There’s no room for guesswork here. 

id:
807
timestamp:
2025-03-30T14:12:25.274Z
sender:
@celadari:matrix.org
content:
Let me use an example to explain my question more clearly:

Suppose we receive an incoming block with some extrinsics, among which are `E_P` extrinsics (preimages). Let’s assume one of these preimages is for service `s` and is encoded as a `vertical-double-array p`. From the first byte, we determine the length of `p`, extract the corresponding bytes, and treat that as the preimage.

This preimage `p` is expected to represent an `Epsilon(double_arrow Epsilon(a_m), a_c)` structure (https://graypaper.fluffylabs.dev/#/68eaa1f/106c01107101?v=0.6.4).

Now, say `p = [129, 2, 4, 5, 6]`. Interpreting this:

The metadata slice is supposed to be of length 129, starting right after the first byte.

But the total array doesn't even contain 129 elements—so this is clearly an incorrectly encoded preimage.

My question is:
- Should we **reject the entire block** due to this malformed preimage? 
OR
- Should we **accept the block**, include the "bad formed" preimage in the lookup for service `s`, and simply let the `Psi_A` panic at execution time (and thus not updating anything) for this service `s` ?

id:
806
timestamp:
2025-03-30T14:13:26.315Z
sender:
@celadari:matrix.org
content:
The reason I ask is because the current codec test vectors for preimages (https://github.com/davxy/jam-test-vectors/blob/polkajam-vectors/codec/data/preimages_extrinsic.json) don’t appear to cover a valid encoding that includes metadata.

id:
805
timestamp:
2025-03-30T14:50:35.032Z
sender:
@gav:polkadot.io
content:
Preimage extrinsic (E_P) is a sequence of pairs (service index with blob).

id:
804
timestamp:
2025-03-30T14:51:40.191Z
sender:
@gav:polkadot.io
content:
Implementations can determine it from the (encoded) block.

id:
803
timestamp:
2025-03-30T14:52:18.404Z
sender:
@gav:polkadot.io
content:
Each other in the preimage extrinsic must be a valid request as per the prior state.

id:
802
timestamp:
2025-03-30T14:52:53.719Z
sender:
@gav:polkadot.io
content:
See (12.30) - (12.33) for formal definitions of this.

id:
801
timestamp:
2025-03-30T14:53:09.641Z
sender:
@gav:polkadot.io
content:
If you are still confused, I suggest you rephrase your query in though terms.

id:
800
timestamp:
2025-03-30T14:54:29.324Z
sender:
@gav:polkadot.io
content:
I'm not sure what you're really asking, but if the question is "if I receive a block which doesn't correctly encode from from which I could make a best effort at some underlying meaning, should I import my best guess at it anyway?" then the answer OF COURSE NOT!

id:
799
timestamp:
2025-03-30T14:54:47.871Z
sender:
@gav:polkadot.io
content:
* I'm not sure what you're really asking, but if the question is "if I receive a block which doesn't correctly decode but from which I could make a best guess at some underlying meaning, should I import my best guess at it anyway?" then the answer OF COURSE NOT!

id:
798
timestamp:
2025-03-30T14:55:11.758Z
sender:
@gav:polkadot.io
content:
* I'm not sure what you're really asking, but if the question is "if I receive a block which doesn't correctly decode but from which I could make a best guess at imagining some underlying meaning, should I import it as though it was really an encoding of this best guess?" then the answer OF COURSE NOT!

id:
797
timestamp:
2025-03-30T14:55:16.013Z
sender:
@gav:polkadot.io
content:
* I'm not sure what you're really asking, but if the question is "if I receive a block which doesn't correctly decode but from which I could make a best guess at imagining some underlying meaning, should I import it as though it was really an encoding of this best guess?" then the answer is OF COURSE NOT!

id:
796
timestamp:
2025-03-30T14:55:50.092Z
sender:
@gav:polkadot.io
content:
Again, this is a consensus protocol. There is no room for error or guesswork.

id:
795
timestamp:
2025-03-30T15:43:38.029Z
sender:
@celadari:matrix.org
content:
To be "tougher" - and concise:

These `preimage` test vectors (https://github.com/davxy/jam-test-vectors/blob/polkajam-vectors/codec/data/preimages_extrinsic.json) don’t cover a valid encoding that includes metadata. They are 7 months old.

➡️ So if I understand correctly => **we shouldn’t try to test these `preimage` test-vectors with the current GP version and just wait for new test vectors for preimages to be published ?**

id:
2111
timestamp:
2025-03-30T16:21:39.293Z
sender:
@snowmead:matrix.org
content:
For the PVM definition A.1, is it intentionally left up to the implementor to avoid calling `deblob` for every subsequent recursive call for the same program blob `p`? 

id:
2110
timestamp:
2025-03-30T16:22:14.106Z
sender:
@snowmead:matrix.org
content:
or is there some reason behind this?

id:
2109
timestamp:
2025-03-30T17:15:29.956Z
sender:
@gav:polkadot.io
content:
> <@snowmead:matrix.org> For the PVM definition A.1, is it intentionally left up to the implementor to avoid calling `deblob` for every subsequent recursive call for the same program blob `p`? 

How would you imagine a recursive call into a program blob?

id:
2108
timestamp:
2025-03-30T17:23:01.243Z
sender:
@snowmead:matrix.org
content:
couldn't we just `deblob` once before and call a recursive function (which internally calls `psi_1` single step function)? 

id:
2107
timestamp:
2025-03-30T17:26:46.224Z
sender:
@ascriv:matrix.org
content:
> <@snowmead:matrix.org> couldn't we just `deblob` once before and call a recursive function (which internally calls `psi_1` single step function)? 

Should still fit the gp specifications I think, happy to be corrected

id:
2106
timestamp:
2025-03-30T17:30:15.483Z
sender:
@ascriv:matrix.org
content:
This is how I ended up interpreting it so I’m particularly eager to be corrected lol

id:
794
timestamp:
2025-03-30T17:31:27.157Z
sender:
@dave:parity.io
content:
If I understand correctly, what you're asking is: if a preimage is requested which will be used as the code blob for a service, must the preimage be a valid "code blob" for it to be includable in a block and integrated into the service storage? Pretty sure the answer to this is no: as long as a preimage has been requested then it can be included in a block. If a service requests a preimage that cannot be decoded or used as a code blob for whatever reason, then attempts to use it as such will fail at the point of use. I'm not sure it can really work any other way as there is no type/format/whatever associated with preimages in the state; they are opaque binary blobs.

id:
793
timestamp:
2025-03-30T17:41:38.141Z
sender:
@celadari:matrix.org
content:
Thanks 🫶! That answers it pretty much

PS: by "valid" I meant only encoded-wise (https://graypaper.fluffylabs.dev/#/68eaa1f/106c01107101?v=0.6.4)

Thanks for the help

id:
792
timestamp:
2025-03-30T18:25:07.281Z
sender:
@rustybot:matrix.org
content:
> <@celadari:matrix.org> To be "tougher" - and concise:
> 
> These `preimage` test vectors (https://github.com/davxy/jam-test-vectors/blob/polkajam-vectors/codec/data/preimages_extrinsic.json) don’t cover a valid encoding that includes metadata. They are 7 months old.
> 
> ➡️ So if I understand correctly => **we shouldn’t try to test these `preimage` test-vectors with the current GP version and just wait for new test vectors for preimages to be published ?**

https://github.com/davxy/jam-test-vectors/tree/polkajam-vectors/codec#semantic-correctness

id:
791
timestamp:
2025-03-30T18:26:35.009Z
sender:
@rustybot:matrix.org
content:
Codec vectors only exercise the codec. Data is just random data

id:
790
timestamp:
2025-03-30T18:27:05.921Z
sender:
@rustybot:matrix.org
content:
* Codec vectors only exercise the codec. Payload is mostly just random data

id:
789
timestamp:
2025-03-30T18:28:46.914Z
sender:
@celadari:matrix.org
content:
I agree but since we are talking about encoding I wasn't sure to which extend we were supposed to verify or not.

But thanks anyway ✌️

id:
788
timestamp:
2025-03-30T18:29:38.378Z
sender:
@gav:polkadot.io
content:
The Gray Paper is 100% clear on this.

id:
787
timestamp:
2025-03-30T18:30:29.137Z
sender:
@gav:polkadot.io
content:
If there was a need to verify, then it would state as much in the Gray Paper. It doesn't.

id:
2105
timestamp:
2025-03-30T18:32:45.365Z
sender:
@gav:polkadot.io
content:
The Gray Paper only states observable behaviour. The performance tests, when benchmark requirements are published, may imply certainly implementation optimisations.

id:
2104
timestamp:
2025-03-30T18:33:23.246Z
sender:
@gav:polkadot.io
content:
For M1/M2 such optimisations are not *required*, but might be sensible to do regardless.

id:
2103
timestamp:
2025-03-30T18:40:46.361Z
sender:
@gav:polkadot.io
content:
signal-2025-03-26-164136.mp4

id:
2102
timestamp:
2025-03-30T18:40:49.106Z
sender:
@gav:polkadot.io
content:
Almost finished!

id:
2101
timestamp:
2025-03-30T19:01:55.208Z
sender:
@emilkietzman:matrix.org
content:
That’s amazing, can’t wait for the results. The real question is the jacuzzi ready to handle the heat!

id:
2100
timestamp:
2025-03-31T08:06:25.074Z
sender:
@gav:polkadot.io
content:
> <@gav:polkadot.io> Almost finished!

I should say, almost finished phase 1 of 2. This is 80 machines of 87 for phase 1. Phase 2 will double that to 173. Each machine has 96 threadripper cores in it. 

id:
2099
timestamp:
2025-03-31T10:22:32.980Z
sender:
@sourabhniyogi:matrix.org
content:
There are many teams with tiny testnets producing blocks that are now likely ready to have pair wise interactions with other teams.  Should we do this outside of the toaster this spring / summer (because we haven’t even passed M1) or be trying to get teams to do this within in it? 

id:
2098
timestamp:
2025-03-31T10:26:48.359Z
sender:
@sourabhniyogi:matrix.org
content:
* There are many teams with tiny testnets (all independently) producing blocks that are now likely ready to have pair wise interactions with other teams. Should we do this outside of the toaster this spring / summer (because we haven’t even passed M1) or be trying to get teams to do this within it (so as to get teams get jamnp networking right well before M1/M2 have been formally passed)? 

id:
2097
timestamp:
2025-03-31T10:43:08.225Z
sender:
@gav:polkadot.io
content:
Evaluating and passing M1 should start before end of June; davxy is working hard on getting those conformance tests and 0.7.0 is [within sights](https://github.com/gavofyork/graypaper/milestone/3).

id:
2096
timestamp:
2025-03-31T10:44:11.744Z
sender:
@gav:polkadot.io
content:
I don't think running on the Toaster makes much sense until both M1 and M2 functionality exists, and also certain RPC and structured logging functionality is in place.

id:
2095
timestamp:
2025-03-31T10:46:36.747Z
sender:
@gav:polkadot.io
content:
We can probably publish a draft spec for tooling-facilitating RPCs quite soon (maybe this week, if it's a high-enough priority). Structured logging format is not yet started AFAIK but should be on the horizon as the Toaster comes online and erin and Arkadiy begin writing the relevant tooling to analyse and visualise it.

id:
2094
timestamp:
2025-03-31T11:18:03.941Z
sender:
@sourabhniyogi:matrix.org
content:
Looking forward to RPC + structured logging functionality details!  Can we aim to make this coordination a central topic in the May Lisbon meetup?

id:
2093
timestamp:
2025-03-31T13:20:02.514Z
sender:
@gav:polkadot.io
content:
https://hackmd.io/@polkadot/jip2

id:
2092
timestamp:
2025-03-31T13:20:16.686Z
sender:
@gav:polkadot.io
content:
It really needs putting in a more standard form, but hopefully it's helpful as-is.

id:
2091
timestamp:
2025-03-31T13:20:47.324Z
sender:
@gav:polkadot.io
content:
If these RPCs are properly implemented then the JAM tooling I've been writing should work with the node.

id:
2090
timestamp:
2025-03-31T13:21:55.793Z
sender:
@clearloop:matrix.org
content:
seems this week just passed in minutes ...

id:
2089
timestamp:
2025-03-31T14:54:36.526Z
sender:
@gav:polkadot.io
content:
I updated my (unofficial) Prize notes: https://hackmd.io/@polkadot/jamprize

id:
2088
timestamp:
2025-03-31T14:55:04.289Z
sender:
@gav:polkadot.io
content:
They now include the two additional paths for prizes (non-PVM and light-client)

id:
2087
timestamp:
2025-03-31T14:55:30.062Z
sender:
@gav:polkadot.io
content:
This will still need to be discussed and ratified at W3F level. But should give the right flavour.

id:
2086
timestamp:
2025-03-31T14:58:21.482Z
sender:
@gav:polkadot.io
content:
I gave the non-PVM path a conversion option to get almost the same prize as the regular path; it's not quite the same (450,000 vs 500,000) since there's a slight desire to get at least one or two teams building a recompiler soonish.

id:
2085
timestamp:
2025-03-31T14:58:51.980Z
sender:
@gav:polkadot.io
content:
It would be helpful to have a show of hands how many teams plan to go for which prize path.

id:
2084
timestamp:
2025-03-31T15:00:38.187Z
sender:
@gav:polkadot.io
content:
* It would be helpful to have a show of hands how many teams plan to go for which prize path. Maybe 🪶 for light-client path, 🎸 for authoring including PVM and 🤠 for non-PVM authoring.

id:
2083
timestamp:
2025-03-31T15:02:50.485Z
sender:
@gav:polkadot.io
content:
Note that that for teams who use one language for most business logic but another for specific subcomponents in order to achieve sufficient performance, your language set will be that of the business logic as long as the second language is very clearly limited to the subcomponents.

id:
2082
timestamp:
2025-03-31T15:03:19.432Z
sender:
@gav:polkadot.io
content:
* Note that that for teams who use one language for most business logic but another for specific subcomponents in order to achieve sufficient performance, your language set will be that of the business logic as long as the second language is very clearly limited to the subcomponents and the subcomponents are properly scope-limited.

id:
2081
timestamp:
2025-03-31T15:04:26.709Z
sender:
@gav:polkadot.io
content:
* Note that that for teams who use one language for business logic but another for specific subcomponents in order to achieve sufficient performance, your language set will be that of the business logic as long as the second language is very clearly limited to the subcomponents and the subcomponents are properly scope-limited.

id:
2080
timestamp:
2025-03-31T15:06:30.354Z
sender:
@emielsebastiaan:matrix.org
content:
Most likely 🤠 too many performance related research unknowns at this point to aim for 🎸. 

id:
2079
timestamp:
2025-03-31T15:10:14.667Z
sender:
@ascriv:matrix.org
content:
^similar boat aiming for guitar but may go cowboy at m3

id:
2078
timestamp:
2025-03-31T15:13:03.414Z
sender:
@oliver.tale-yazdi:parity.io
content:
JamBrains is working on authoring with PVM-interpreter, recompiler is currently not planned

id:
2077
timestamp:
2025-03-31T15:16:04.681Z
sender:
@jay_ztc:matrix.org
content:
I'm very excited to open source my pvm recompiler implementation soon(ish). Following the naturally emergent api boundaries from the GP & reusability across different projects & languages have been design priorities since day 1.

id:
786
timestamp:
2025-03-31T15:39:48.070Z
sender:
@ascriv:matrix.org
content:
Has anyone yet taken a serious look at if size-synchrony antagonism has been formalized mathematically? Would be nice to have further validation e.g. that what we’re doing is somewhat optimal 

id:
785
timestamp:
2025-03-31T15:40:24.450Z
sender:
@gav:polkadot.io
content:
There are some quite similar concepts in systems theory.

id:
784
timestamp:
2025-03-31T15:55:37.437Z
sender:
@gav:polkadot.io
content:
AI answer:
> One relevant concept is "complexity theory," which posits that as systems grow in size and complexity, the potential for disorder and misalignment among components increases. This can lead to difficulties in achieving coherence. Larger systems may have more diverse elements, which can result in varying goals, behaviors, and interactions that can disrupt overall coherence.
> Another related idea is "Ashby's Law of Requisite Variety," which states that for a system to effectively manage its environment, it must be as diverse as the environment it operates in. In larger systems, the variety of components and interactions can lead to challenges in maintaining coherence unless there are effective mechanisms for integration and coordination.


id:
783
timestamp:
2025-03-31T15:56:06.212Z
sender:
@gav:polkadot.io
content:
However the strict trilemma of Scale, Speed and Coherence doesn't seem to be a thing.

id:
782
timestamp:
2025-03-31T15:56:20.282Z
sender:
@gav:polkadot.io
content:
* However the strict trilemma of Scale, Speed and Coherence doesn't seem to be established.

id:
781
timestamp:
2025-03-31T15:57:29.097Z
sender:
@gav:polkadot.io
content:
It seems to me, at least, quite demonstrable given real systems have causality bound by speed and component-distances.

id:
780
timestamp:
2025-03-31T15:58:19.125Z
sender:
@jay_ztc:matrix.org
content:
CAP comes to mind, sort of a cousin principle if you will

id:
779
timestamp:
2025-03-31T16:00:22.632Z
sender:
@gav:polkadot.io
content:
Coherence -> degree of causality across all pairwise pieces of system state
Speed -> bound as the time that it would take light to effect a causal resolution across the two most distant causally entangled parts of state
Size -> Given some maximal density of system state, the total size of the system

id:
778
timestamp:
2025-03-31T16:00:47.446Z
sender:
@gav:polkadot.io
content:
It seems pretty trivial to show that If you increase any of these you must reduce one or both of the others.

id:
777
timestamp:
2025-03-31T16:01:30.605Z
sender:
@gav:polkadot.io
content:
* Coherence -> degree of causality across all pairwise pieces of system state
Speed -> bound as the time that it would take light to effect a causal resolution across the two most distant causally entangled parts of state
Size -> Given some maximal density of system state, the maximum distance between causally entangled state-components of the system

id:
776
timestamp:
2025-03-31T16:02:26.339Z
sender:
@gav:polkadot.io
content:
So if you make a system bigger (add more state components and therefore make things farther apart) you either need to accept causal resolution will be slower or you have to limit what parts are causally entangled and thus reduce coherence.

id:
775
timestamp:
2025-03-31T16:02:48.357Z
sender:
@gav:polkadot.io
content:
* So if you make a system bigger (add more state components and therefore make things farther apart) you either need to accept causal resolution will be slower because at leats some portions of state are farther apart or you have to limit what parts are causally entangled and thus reduce coherence.

id:
774
timestamp:
2025-03-31T16:03:41.681Z
sender:
@gav:polkadot.io
content:
CAP is somehow related, but it's binary (select any two).

id:
773
timestamp:
2025-03-31T16:04:11.786Z
sender:
@gav:polkadot.io
content:
* CAP is somehow related, but it's binary (select any two). It also doesn't deal with size of speed but only properties.

id:
772
timestamp:
2025-03-31T16:04:19.833Z
sender:
@gav:polkadot.io
content:
* CAP is somehow related, but it's binary (select any two). It also doesn't deal with size of speed but only "correctness" properties.

id:
771
timestamp:
2025-03-31T16:04:57.158Z
sender:
@gav:polkadot.io
content:
But yes, is a related trilemma/antagonism applicable to (distributed) systems.

id:
770
timestamp:
2025-03-31T16:05:53.382Z
sender:
@gav:polkadot.io
content:
* So if you make a system bigger (add more state components and therefore make things farther apart) you either need to accept causal resolution will be slower because at least some portions of state are farther apart (and light only travels at a certain speed) or you have to limit what parts are causally entangled, limiting distances travelled for resolution and thus reduce coherence.

id:
2076
timestamp:
2025-03-31T16:19:00.755Z
sender:
@jaymansfield:matrix.org
content:
Is there still a talk planned about how to achieve a high performance recompiler? I think I recall seeing something mentioned here in the past.

id:
769
timestamp:
2025-03-31T16:54:53.310Z
sender:
@ascriv:matrix.org
content:
> <@gav:polkadot.io> So if you make a system bigger (add more state components and therefore make things farther apart) you either need to accept causal resolution will be slower or you have to limit what parts are causally entangled and thus reduce coherence.

distance = rate * time in some sense?

id:
768
timestamp:
2025-03-31T16:55:42.897Z
sender:
@ascriv:matrix.org
content:
Maybe that’s generalized too much

id:
767
timestamp:
2025-03-31T17:00:29.131Z
sender:
@ascriv:matrix.org
content:
Distance ~ size
Rate ~ speed
Time ~ coherence

id:
766
timestamp:
2025-03-31T17:06:34.259Z
sender:
@ascriv:matrix.org
content:
* Distance ~ size
Rate ~ speed
Time ~ coherence

So roughly size = speed*coherence

id:
765
timestamp:
2025-03-31T17:21:13.302Z
sender:
@emielsebastiaan:matrix.org
content:
If a single global coherent state is the design goal (which it is) you can can play/design around different types of decoherence. Eg spatial decoherence (shards), temporal decoherence (ordered accumulation). You can allow for certain types of decoherence and still have a fully coherent global state sufficiently oftentimes to allow for the emergent abstraction of the Cloud layer.

id:
764
timestamp:
2025-03-31T17:24:38.503Z
sender:
@emielsebastiaan:matrix.org
content:
* If a single global coherent state is the design goal (which it is) you can can play/design around different types of decoherence. Eg spatial decoherence (shards), temporal decoherence (ordered accumulation / asynchrony). You can allow for certain types of decoherence and still have a fully coherent global state sufficiently oftentimes to allow for the emergent abstraction of the Cloud layer.

id:
2075
timestamp:
2025-03-31T17:45:31.880Z
sender:
@finsig:matrix.org
content:
 Time permitting Martlet will try for 🤠, otherwise 🪶.

id:
2074
timestamp:
2025-03-31T18:41:52.140Z
sender:
@sourabhniyogi:matrix.org
content:
Can we get a set of work packages (or bundles), exporting the frame buffers as segments, relative to some version (0.6.4 or ___) so we can set an April goal of refining them in time for May meet up?

id:
2073
timestamp:
2025-03-31T18:43:07.191Z
sender:
@sourabhniyogi:matrix.org
content:
Ideally this would come with the exact way to show the demo of rendering the exported DOOM/... segments that even non-implementers could run following some top-level README.

id:
2072
timestamp:
2025-03-31T19:24:18.398Z
sender:
@tomusdrw:matrix.org
content:
Would be cool to get confirmation if I'm getting the incentives & options for non-fast-set languages right. I'd love to target 🎸 however it might be just impossible to reach the performance requirements, and by investing time&effort into this I'm risking being "outpaced" by 🤠 & 🪶, right? So with 50k DOT penalty it seems just safer to go 🤠, isn't it?

id:
2071
timestamp:
2025-03-31T19:42:17.393Z
sender:
@danicuki:matrix.org
content:
This is an optional (but I highly recommend) after activity for those who come to JAM Experience in Lisbon:

On May 8th, there will be a very talented and high quality Brazilian music gig at the Lisbon Colosseum. Buy your tickets if you wanna go:

https://www.bol.pt/Comprar/Bilhetes/155009-diogo_nogueira-coliseu_de_lisboa/Sessoes

id:
2070
timestamp:
2025-03-31T20:18:43.363Z
sender:
@tomusdrw:matrix.org
content:
* Would be cool to get confirmation if I'm getting the incentives & options for non-fast-set languages right. I'd love to target 🎸 however it might be just impossible to reach the performance requirements, and by investing time&effort into this I'm risking being outpaced by 🤠 & 🪶, right? So with 50k DOT penalty it seems just safer to go 🤠, isn't it?

id:
2069
timestamp:
2025-03-31T20:19:12.050Z
sender:
@tomusdrw:matrix.org
content:
* Would be cool to get confirmation if I'm getting the incentives & options for non-fast-set languages right. I'd love to target 🎸 however it might be just impossible to reach the performance requirements, and by investing time&effort into this I'm risking being outpaced by 🤠 & 🪶, right? So with 50k DOT penalty it seems just safer to go 🤠, isn't it? (450k DOT vs 0)

id:
2068
timestamp:
2025-03-31T20:54:34.802Z
sender:
@xlchen:matrix.org
content:
is this JSONRPC or HTTP GET? how are the parameter and responses encoded? JSON or JAM codec?

id:
2067
timestamp:
2025-03-31T20:55:28.576Z
sender:
@xlchen:matrix.org
content:
given the subscriptions methods, looks like JSONRPC over WebSocket?

id:
2066
timestamp:
2025-03-31T20:58:50.755Z
sender:
@xlchen:matrix.org
content:
the types reads like thing should be JAM codec encoded? because otherwise Hash/Blob should really be encoded as hex string instead of array of numbers

id:
2065
timestamp:
2025-03-31T21:12:59.790Z
sender:
@xlchen:matrix.org
content:
can I confirm that generative AI is ok (or not) for:
write documents
write tests
code review
help reading GP
doing research (e.g. how to write a recompiler)
write code that's not part of GP (e.g. RPC or CLI handling). In theory, those code should not be judged?

id:
2064
timestamp:
2025-03-31T21:14:25.400Z
sender:
@xlchen:matrix.org
content:
* can I confirm that generative AI is ok (or not) for:
write documents
write tests
code review
help reading GP
doing research (e.g. how to write a recompiler)
write code that's not covered by GP (e.g. RPC or CLI handling). In theory, those code should not be judged?

id:
2063
timestamp:
2025-03-31T22:14:19.467Z
sender:
@sourabhniyogi:matrix.org
content:
Looks comprehensive but the big "obvious" objects appear to be missing:
1. block - Returns block with the given header hash, or null if this is not known.
2. state - Returns C1-C15 with the given header hash, or null if this is not known.
3. workPackage - Returns work report (including availability spec) with the given work package hash
4. segment - Returns back segment given (requestedHash, index) (either work package hash or exported segments root) from Segments DA
Are these meaningful to add to jip-2?

id:
2062
timestamp:
2025-03-31T22:14:52.532Z
sender:
@sourabhniyogi:matrix.org
content:
* Looks comprehensive but the big "obvious" objects appear to be missing:

1. _block_ - Returns block with the given header hash, or null if this is not known.
2. _state_ - Returns C1-C15 with the given header hash, or null if this is not known.
3. _workPackage_ - Returns work report (including availability spec) with the given work package hash
4. _segment_ - Returns back segment given (requestedHash, index) (either work package hash or exported segments root) from Segments DA

Are these meaningful to add to jip-2?


id:
2061
timestamp:
2025-03-31T22:16:57.696Z
sender:
@sourabhniyogi:matrix.org
content:
Surely, everyone will naturally want content-type choice (JSON vs JAM Codec) in the responses and expect JSON to match up with w3f / davxy choices.

id:
2060
timestamp:
2025-04-01T05:29:18.158Z
sender:
@clearloop:matrix.org
content:
cowboy in avatar & emojis

id:
2059
timestamp:
2025-04-01T06:02:05.510Z
sender:
@clearloop:matrix.org
content:
our team have good background in compiler and super interested in implementing a re-compiler for fun, however we'd like to target 🤠 first since it's safer, we may implement a re-compiler only after passing M4

id:
2058
timestamp:
2025-04-01T06:02:33.531Z
sender:
@clearloop:matrix.org
content:
* our team have good background in compiler and super interested in implementing a re-compiler for fun, however we'd like to target 🤠 first since it's safer, we may implement a re-compiler only after passing M4 (else if we can finish it in one week)

id:
2057
timestamp:
2025-04-01T06:04:32.383Z
sender:
@jan:parity.io
content:
You most likely won't be able to pass M3 without a recompiler, nevermind M4. (Unless you mean the non-validating MN4 milestone.)

id:
2056
timestamp:
2025-04-01T06:05:23.103Z
sender:
@clearloop:matrix.org
content:
can't we use polka VM for M3 & M4? the non-PVM path?

id:
2055
timestamp:
2025-04-01T06:05:31.209Z
sender:
@clearloop:matrix.org
content:
* can't we use polkaVM for M3 & M4? the non-PVM path?

id:
2054
timestamp:
2025-04-01T06:08:38.940Z
sender:
@jan:parity.io
content:
So you meant the non-validating milestones. Those are called `MN3` and `MN4` in Gav's document. Please use proper unambiguous naming, otherwise it's very confusing.

id:
2053
timestamp:
2025-04-01T06:10:48.526Z
sender:
@dakkk:matrix.org
content:
> <@jan:parity.io> So you meant the non-validating milestones. Those are called `MN3` and `MN4` in Gav's document. Please use proper unambiguous naming, otherwise it's very confusing.

Which document?

id:
2052
timestamp:
2025-04-01T06:11:27.813Z
sender:
@jan:parity.io
content:
The one that Gav posted a few message ago here: https://hackmd.io/@polkadot/jamprize

id:
2051
timestamp:
2025-04-01T06:11:48.264Z
sender:
@dakkk:matrix.org
content:
> <@jan:parity.io> The one that Gav posted a few message ago here: https://hackmd.io/@polkadot/jamprize

Thx

id:
2050
timestamp:
2025-04-01T06:14:05.322Z
sender:
@clearloop:matrix.org
content:
* our team have good background in compiler and super interested in implementing a re-compiler for fun, however we'd like to target 🤠 first since it's safer, we may implement a re-compiler only after passing ~~M4~~ NN4 (else if we can finish it in one week)

id:
2049
timestamp:
2025-04-01T06:14:16.256Z
sender:
@clearloop:matrix.org
content:
* our team have good background in compiler and super interested in implementing a re-compiler for fun, however we'd like to target 🤠 first since it's safer, we may implement a re-compiler only after passing ~~M4~~ MN4 (else if we can finish it in one week)

id:
763
timestamp:
2025-04-01T06:14:50.988Z
sender:
@faiz_871:matrix.org
content:
* Could somebody please explain the meaning of variable δ here in Refine invocation function PsiR https://graypaper.fluffylabs.dev/#/5f542d7/2d65002d0e01?v=0.6.2

id:
2048
timestamp:
2025-04-01T06:58:43.881Z
sender:
@vinsystems:matrix.org
content:
What are the incentives for people to run a light node? What do they bring to the JAM network?

id:
2047
timestamp:
2025-04-01T07:00:28.779Z
sender:
@bkchr:parity.io
content:
Light nodes are being run by the people wanting to interact with Jam/services on top of it 

id:
2046
timestamp:
2025-04-01T07:00:35.740Z
sender:
@bkchr:parity.io
content:
Not by any other operator 

id:
2045
timestamp:
2025-04-01T07:01:19.976Z
sender:
@bkchr:parity.io
content:
And you get trustless access to the chain, as you are verifying the signatures of the blocks etc. 

id:
2044
timestamp:
2025-04-01T07:01:31.388Z
sender:
@xlchen:matrix.org
content:
there won't be RPC nodes for JAM chain. you have to run a light node to access jam chain data like block hash etc

id:
2043
timestamp:
2025-04-01T07:02:40.014Z
sender:
@xlchen:matrix.org
content:
* there won't be RPC nodes for JAM chain. you have to run a light node to access jam chain data like best block hash etc

id:
2042
timestamp:
2025-04-01T07:03:35.493Z
sender:
@chungquantin:matrix.org
content:
I'm curious where this info is mentioned? Would love to read more about it.

id:
2041
timestamp:
2025-04-01T07:04:11.591Z
sender:
@xlchen:matrix.org
content:
somewhere in this room 💁‍♂️

id:
2040
timestamp:
2025-04-01T07:04:51.441Z
sender:
@xlchen:matrix.org
content:
we don't have much docs about the non GP part as nothing is finalized. we have some ideas of how things should look like, but until some PoC, we can't say for sure it will work

id:
2039
timestamp:
2025-04-01T07:07:11.694Z
sender:
@xlchen:matrix.org
content:
https://hackmd.io/0gSmXyElT2iKawymS5jJKw?view this maybe useful

id:
2038
timestamp:
2025-04-01T07:07:55.789Z
sender:
@chungquantin:matrix.org
content:
I see, thanks for the resource! 

id:
2037
timestamp:
2025-04-01T07:15:50.046Z
sender:
@bkchr:parity.io
content:
With polkadot we tried to be light client first, but if you don't force people to use it, no one is gonna do it. Or ultra slow 

id:
762
timestamp:
2025-04-01T11:05:24.834Z
sender:
@gav:polkadot.io
content:
size = time_taken / coherence

id:
761
timestamp:
2025-04-01T11:06:41.761Z
sender:
@gav:polkadot.io
content:
or speed * size * coherence = 1

id:
760
timestamp:
2025-04-01T11:07:35.130Z
sender:
@gav:polkadot.io
content:
you can make a system go fast, go big, or stay fully coherent but not all of them.

id:
759
timestamp:
2025-04-01T11:09:13.083Z
sender:
@gav:polkadot.io
content:
decentralisation implies size, but not the other way around.

id:
758
timestamp:
2025-04-01T11:10:21.494Z
sender:
@gav:polkadot.io
content:
so, if you keep a system small (in order to keep it fast and coherent, you'll not be able to decentralise nor will you be able to scale out.

id:
757
timestamp:
2025-04-01T11:10:36.716Z
sender:
@gav:polkadot.io
content:
* so, if you keep a system small (in order to keep it fast and coherent we might presume), you'll not be able to decentralise nor will you be able to scale out.

id:
756
timestamp:
2025-04-01T11:11:58.008Z
sender:
@gav:polkadot.io
content:
i'd argue that by introducing such decoherence you do not have a fully coherent state.

id:
755
timestamp:
2025-04-01T11:13:59.140Z
sender:
@gav:polkadot.io
content:
however there may be ways to make the system *apparently* coherent, or dynamically rebalance the speed and/or coherence in order to optimise all three at any given time. 

id:
2036
timestamp:
2025-04-01T11:15:56.318Z
sender:
@gav:polkadot.io
content:
yes!

id:
2035
timestamp:
2025-04-01T11:16:22.909Z
sender:
@gav:polkadot.io
content:
* Note that for teams who use one language for business logic but another for specific subcomponents in order to achieve sufficient performance, your language set will be that of the business logic as long as the second language is very clearly limited to the subcomponents and the subcomponents are properly scope-limited.

id:
2034
timestamp:
2025-04-01T11:17:34.255Z
sender:
@sourabhniyogi:matrix.org
content:
JAM Game of Life demonstration (from www )
 https://www.youtube.com/watch?v=lvkF7i6pmR8


id:
2033
timestamp:
2025-04-01T11:22:31.647Z
sender:
@gav:polkadot.io
content:
Yeah it'll depend on how much confidence you have in your language(s) of choice. There's only a limited number of spaces for 🪶 in each language set and they only fetch half the amount, so being outpaced by them at least seems unlikely.

id:
754
timestamp:
2025-04-01T11:30:12.521Z
sender:
@ascriv:matrix.org
content:
* Distance ~ size
Rate ~ speed
Time ~ 1/coherence

So roughly size = speed/coherence

id:
2032
timestamp:
2025-04-01T11:32:50.517Z
sender:
@ascriv:matrix.org
content:
> <@sourabhniyogi:matrix.org> JAM Game of Life demonstration (from www )
>  https://www.youtube.com/watch?v=lvkF7i6pmR8
> 

Neato 

id:
753
timestamp:
2025-04-01T11:35:23.663Z
sender:
@ascriv:matrix.org
content:
> <@gav:polkadot.io> or speed * size * coherence = 1

Or size*coherence ~ speed, with more speed of info travel you can get bigger or more coherent, no?

id:
752
timestamp:
2025-04-01T11:36:41.124Z
sender:
@ascriv:matrix.org
content:
* Distance ~ size
Rate ~ speed
Time ~ 1/coherence

So roughly size ~ speed/coherence

id:
751
timestamp:
2025-04-01T11:36:50.771Z
sender:
@gav:polkadot.io
content:
sure.

id:
750
timestamp:
2025-04-01T11:37:02.394Z
sender:
@ascriv:matrix.org
content:
* Distance ~ size
Rate ~ speed
Time ~ coherence

So roughly size = speed*coherence

id:
749
timestamp:
2025-04-01T11:37:11.776Z
sender:
@gav:polkadot.io
content:
but there's two different speeds here

id:
748
timestamp:
2025-04-01T11:37:58.043Z
sender:
@gav:polkadot.io
content:
there's the physical limit of speed (speed of light), and the overall speed of the system (one over time to causal resolution)

id:
747
timestamp:
2025-04-01T11:38:38.211Z
sender:
@gav:polkadot.io
content:
it probably isnt sensible to call both things "speed".

id:
746
timestamp:
2025-04-01T11:38:43.051Z
sender:
@gav:polkadot.io
content:
* it probably isn't sensible to call both things "speed".

id:
745
timestamp:
2025-04-01T11:41:15.933Z
sender:
@gav:polkadot.io
content:
the speed of light determines the upper limit of causal resolution - no system could ever process causal interactions faster than this. but it doesn't account for keeping a complex system in coherence. as coherent systems become bigger and more complex, the speed of causality diverges from this universal physical limit.

id:
744
timestamp:
2025-04-01T11:41:33.814Z
sender:
@gav:polkadot.io
content:
* the speed of light determines the upper limit of causal resolution - no system could ever process causal interactions faster than this. but it doesn't account for keeping a complex and arbitrary system in coherence. as coherent systems become bigger and more complex, the speed of causality diverges from this universal physical limit.

id:
743
timestamp:
2025-04-01T11:41:58.986Z
sender:
@gav:polkadot.io
content:
* the speed of light determines the upper limit of causal resolution - no system could ever process causal interactions faster than this. but it doesn't account for keeping a complex and arbitrary system in coherence. as coherent systems become bigger and more complex, the speed of their overall causality diverges from this universal physical limit.

id:
742
timestamp:
2025-04-01T11:45:10.537Z
sender:
@gav:polkadot.io
content:
at a basic level, as a coherent system grows, even if all of its internal causality happened at the speed of light, it would still take longer to step through its state transitions becuase it would take light longer to get from the corners of the system to interact and resolve.

id:
741
timestamp:
2025-04-01T11:46:05.547Z
sender:
@gav:polkadot.io
content:
so the system - in terms of state transitions per second - would be slower.

id:
740
timestamp:
2025-04-01T11:47:02.073Z
sender:
@gav:polkadot.io
content:
this is compounded by complexity, meaning that internal causal entanglements probably resolve slower as the system grows more complex and arbitrary.

id:
739
timestamp:
2025-04-01T11:47:35.192Z
sender:
@gav:polkadot.io
content:
light is not just going in a straight line.

id:
738
timestamp:
2025-04-01T11:48:49.292Z
sender:
@gav:polkadot.io
content:
of course our systems have a long way to go before the speed of light becomes too important. but still, the principle can serve us well now.

id:
737
timestamp:
2025-04-01T11:58:49.176Z
sender:
@gav:polkadot.io
content:
basically `T=ZX/C` where:
- `T` is time to causal resolution (s),
- `Z` is size of the system (m),
- `X` is complexity factor of the system (no units)
- `C` is speed of light

id:
736
timestamp:
2025-04-01T12:01:40.698Z
sender:
@gav:polkadot.io
content:
* basically `T=ZX/C` where:

- `T` is time to causal resolution (s),
- `Z` is size of the system (m - the diameter of its bounding sphere basically),
- `X` is complexity factor of the system (no units, but a factor at least 1 which describes the average distance light must travel in order to guarantee a causal resolution of state-transition)
- `C` is speed of light

id:
735
timestamp:
2025-04-01T12:01:47.618Z
sender:
@gav:polkadot.io
content:
* basically `T=ZX/C` where:

- `T` is time to causal resolution (s),
- `Z` is size of the system (m - the diameter of its bounding sphere basically),
- `X` is complexity factor of the system (no units, but a factor at least 1 which describes the distance light must travel in order to guarantee a causal resolution of state-transition)
- `C` is speed of light

id:
734
timestamp:
2025-04-01T12:02:26.010Z
sender:
@gav:polkadot.io
content:
* basically `T=ZX/C` where:

- `T` is time to causal resolution (s),
- `Z` is size of the system (m - the diameter of its bounding sphere basically),
- `X` is complexity factor of the system (no units, but a factor at least 1 which describes the number of times light must travel back across the diameter of the bounding sphere in order to guarantee a causal resolution of state-transition)
- `C` is speed of light

id:
733
timestamp:
2025-04-01T12:02:37.934Z
sender:
@gav:polkadot.io
content:
* basically `T=ZX/C` where:

- `T` is time to causal resolution (s),
- `Z` is size of the system (m - the diameter of its bounding sphere basically),
- `X` is complexity factor of the system (no units, but a factor of at least 1 which describes the number of times light must travel back across the diameter of the bounding sphere in order to guarantee a causal resolution of state-transition)
- `C` is speed of light

id:
732
timestamp:
2025-04-01T12:03:02.983Z
sender:
@gav:polkadot.io
content:
* basically `T=ZX/C` where:

- `T` is time to causal resolution (s - this is the inverse of the system's operating speed),
- `Z` is size of the system (m - the diameter of its bounding sphere basically),
- `X` is complexity factor of the system (no units, but a factor of at least 1 which describes the number of times light must travel back across the diameter of the bounding sphere in order to guarantee a causal resolution of state-transition)
- `C` is speed of light

id:
731
timestamp:
2025-04-01T12:03:14.852Z
sender:
@gav:polkadot.io
content:
* basically `T=ZX/C` where:

- `T` is time to causal resolution (s - this is the inverse of the system's operating speed),
- `Z` is size of the system (m - the diameter of the system's bounding sphere),
- `X` is complexity factor of the system (no units, but a factor of at least 1 which describes the number of times light must travel back across the diameter of the bounding sphere in order to guarantee a causal resolution of state-transition)
- `C` is speed of light

id:
730
timestamp:
2025-04-01T12:06:34.294Z
sender:
@gav:polkadot.io
content:
a totally trivial system would be a single laser switch in a vacuum with one light emitter transmitting a light signal to some light receiver. in this case Z would be the distance between the emitter and receiver, X would be close to one and T would therefore amount to the time it took light to travel between them.

id:
729
timestamp:
2025-04-01T12:06:56.367Z
sender:
@gav:polkadot.io
content:
this wouldn't do much any processing though.

id:
728
timestamp:
2025-04-01T12:07:52.103Z
sender:
@gav:polkadot.io
content:
as we introduce the capability of data processing, `X` increases, and as we introduce state (whether intra-transition or inter-transition) `Z` increases.

id:
727
timestamp:
2025-04-01T12:08:35.664Z
sender:
@gav:polkadot.io
content:
* as we introduce the capability of data processing `X` increases since the round trip of light is much higher as it passes through more gates and it routed around; and as we introduce state (whether intra-transition or inter-transition) `Z` increases as we need to cover a greater space.

id:
726
timestamp:
2025-04-01T12:08:58.573Z
sender:
@gav:polkadot.io
content:
* as we introduce the capability of data processing `X` increases since the round trip of light is much higher as it passes through more gates and it routed around; and as we introduce state (whether intra-transition or inter-transition) `Z` increases as we need to cover a greater space to hold more information (also a fundamental physical principle).

id:
725
timestamp:
2025-04-01T12:09:08.974Z
sender:
@gav:polkadot.io
content:
* as we introduce the capability of data processing `X` increases since the round trip of light is much higher as it passes through more gates and it routed around; and as we introduce state (whether intra-transition or inter-transition) `Z` increases as we need to cover a greater space to hold more information (also a fundamental physical principle as well as intuitively correct).

id:
724
timestamp:
2025-04-01T12:27:51.312Z
sender:
@ascriv:matrix.org
content:
That seems like a good model. as a cool aside maximum info scales with the surface area of the bounding region, given by the bekenstein bound which black holes are believed to saturate 

id:
723
timestamp:
2025-04-01T12:29:51.188Z
sender:
@ascriv:matrix.org
content:
* That seems like a good model. as a cool aside maximum info scales with the surface area (not volume) of the bounding region, given by the bekenstein bound which black holes are believed to saturate 

id:
722
timestamp:
2025-04-01T12:37:26.712Z
sender:
@ascriv:matrix.org
content:
* That seems like a good model. as a cool aside maximum info scales with the surface area (not volume) of the bounding sphere, given by the bekenstein bound which black holes are believed to saturate 

id:
721
timestamp:
2025-04-01T12:59:13.332Z
sender:
@emielsebastiaan:matrix.org
content:
My team and I have put some thought into this. I’ll try to digest it into something presentable for our little Lisbon meetup. 

id:
2031
timestamp:
2025-04-01T13:35:55.018Z
sender:
@qinwenwang:matrix.org
content:
Hi everyone, I am thrilled to release the Chinese Version of JAM Gray Paper DRAFT 0.6.4. on https://www.lollipop.builders/JAM-Graypaper-Chinese.pdf ；I look forward to feedback, discussions, and further support Mandarin speaking dev community for JAM 

id:
720
timestamp:
2025-04-01T14:16:44.890Z
sender:
@gav:polkadot.io
content:
* this wouldn't do any processing though.

id:
719
timestamp:
2025-04-01T14:18:36.413Z
sender:
@gav:polkadot.io
content:
Ahh yeah the holographic principle iirc

id:
718
timestamp:
2025-04-01T18:07:30.759Z
sender:
@boymaas:matrix.org
content:
Have we considered, as a thought experiment, https://en.wikipedia.org/wiki/Quantum_entanglement as a means to get instant coherence in distributed systems? Going beyond the speed of light ... 😃

id:
717
timestamp:
2025-04-01T18:09:30.060Z
sender:
@dakkk:matrix.org
content:
> <@boymaas:matrix.org> Have we considered, as a thought experiment, https://en.wikipedia.org/wiki/Quantum_entanglement as a means to get instant coherence in distributed systems? Going beyond the speed of light ... 😃

You can't communicate any information faster than the speed of light; quantum entanglement doesn't do that 

id:
716
timestamp:
2025-04-01T18:13:12.511Z
sender:
@boymaas:matrix.org
content:
Too bad, reading it now indeed, would have been an interesting case.

id:
715
timestamp:
2025-04-01T18:42:59.147Z
sender:
@jay_ztc:matrix.org
content:
is sbrk here to stay? I noticed its being used in the accumulate testvectors.

id:
2030
timestamp:
2025-04-01T22:08:03.826Z
sender:
@jay_ztc:matrix.org
content:
is the repo for the jam-types crate public? noticed the transfer memo length doesn't match the GP spec and was going to open an issue.

id:
2029
timestamp:
2025-04-01T22:08:17.897Z
sender:
@jay_ztc:matrix.org
content:
* is the repo for the jam-types crate public? noticed the transfer memo length doesn't match the GP spec and was going to open an issue. The link on crates.io is broken.

id:
2028
timestamp:
2025-04-01T22:08:33.762Z
sender:
@jay_ztc:matrix.org
content:
* is the repo for the jam-types crate public? noticed the transfer memo length doesn't match the GP spec and was going to open an issue. The link on crates.io is 404.

id:
2027
timestamp:
2025-04-01T22:09:31.206Z
sender:
@jay_ztc:matrix.org
content:
* is the repo for the jam-types crate public? noticed the transfer memo length doesn't match the GP spec and was going to open an issue. The gh link on crates.io is 404.

id:
714
timestamp:
2025-04-02T08:24:35.028Z
sender:
@greywolve:matrix.org
content:
Is the [28 days that erasure coded chunks need to be held for](https://graypaper.fluffylabs.dev/#/68eaa1f/1d2d001d3100?v=0.6.4) a minimum or a maximum? 

id:
713
timestamp:
2025-04-02T08:26:07.436Z
sender:
@xlchen:matrix.org
content:
minimum. you are not wrong if retained for one more day, but not the case otherwise

id:
712
timestamp:
2025-04-02T08:27:30.561Z
sender:
@greywolve:matrix.org
content:
and I assume storing more data just costs you more?

id:
711
timestamp:
2025-04-02T08:27:56.411Z
sender:
@xlchen:matrix.org
content:
yeah use more disk storage

id:
2026
timestamp:
2025-04-03T15:28:03.268Z
sender:
@clearloop:matrix.org
content:
curious about how do you like your PVM interfaces so for? after following the invocation interfaces strictly, my PVM interpreter turns into functional style finally, it's cool but the function signatures just have too many arguments, I'm thinking of that we may need more composed types defined in GP for the PVM part

id:
2025
timestamp:
2025-04-03T15:36:43.391Z
sender:
@jan:parity.io
content:
Not currently. It's released from our private polkajam repo.

id:
2024
timestamp:
2025-04-03T15:48:40.222Z
sender:
@jan:parity.io
content:
Do you want to know how my interfaces look like, or are you looking for suggestions? :P If so I'd probably suggest a traditional fetch + eval interpreter loop at a bare minimum, plus a stateful interface to control it. The GP isn't really an implementer's guide and its equations have no bearing on how something should *actually* be implemented; sure, you can translate its equations directly into code, but you'll be killing your performance by doing that, so for any non-toy implementations I'd probably suggest not to do that (especially for PVM, where you can make your situation potentially orders of magnitude worse even compared to a straighforward traditional interpreter design). :P

id:
2023
timestamp:
2025-04-03T15:49:19.518Z
sender:
@jan:parity.io
content:
* Do you want to know how my interfaces look like, or are you looking for suggestions? :P If so I'd probably suggest a traditional fetch + eval interpreter loop at a bare minimum, plus a stateful interface to control it. The GP isn't really an implementer's guide and its equations have no bearing on how something should _actually_ be implemented; sure, you can translate its equations directly into code, but you'll be killing your performance by doing that, so for any non-toy implementations I'd probably suggest not to do that (especially for PVM, where you can make your situation potentially orders of magnitude worse even compared to a basic straighforward traditional interpreter design). :P

id:
2022
timestamp:
2025-04-03T16:02:46.969Z
sender:
@clearloop:matrix.org
content:
while considering supporting both interpreter and re-compiler in the same interface, we have an abstraction layer for PVM interfaces ( also it is caused by the design of our runtime, things like PVM, validator, storage, and even the network implementation are modular ), so I'd like to confirm if the PVM interfaces in the GP has already been the standard, if so, I'll keep following it atm, otherwise my hands are not tied

id:
2021
timestamp:
2025-04-03T16:09:23.268Z
sender:
@jan:parity.io
content:
The GP doesn't define any PVM interfaces in the sense of an interface in a programming language; again, it describes the visible *behavior* of a JAM implementation, but doesn't specifically require *how* that behavior should be achieved. There's no standard PVM interface, and even if there were one it'd most likely look very very different than what the equations in the GP suggest, assuming its intended use would be as an abstraction layer over multiple PVM implementations for a production JAM node.

id:
2020
timestamp:
2025-04-03T16:17:45.135Z
sender:
@jan:parity.io
content:
If you want some inspiration on how to structure your interfaces I'd suggest to look at some of the production VMs out there and see how they do it.

For example, some of the WASM VMs:

wasmtime - https://docs.rs/wasmtime/latest/wasmtime/
wasmi - https://docs.rs/wasmi/latest/wasmi/
wazero - https://pkg.go.dev/github.com/tetratelabs/wazero

Or even just look at my interfaces:

https://docs.rs/polkavm/latest/polkavm/struct.Module.html
https://docs.rs/polkavm/latest/polkavm/struct.RawInstance.html

(Feel free to copy them if you want; it's just an interface, and I wouldn't consider it collusion or plagiarism - there isn't really anything super JAM specific in there, and all of it is fairly straightforward if you have any experience with working with VMs.)

id:
2019
timestamp:
2025-04-03T16:19:47.786Z
sender:
@clearloop:matrix.org
content:
I'm mainly following the interfaces of wasmtime as well since I used to be a WASM developer 🙈

id:
2018
timestamp:
2025-04-03T16:26:49.230Z
sender:
@sourabhniyogi:matrix.org
content:
How shall teams get started with PVM recompilation?  We are ready to plunge into this to get the 50x gains.

id:
2017
timestamp:
2025-04-03T16:30:00.621Z
sender:
@jay_ztc:matrix.org
content:
clearloop | SpaceJam: I would consider the idea of "emergent api boundaries"-> a product of the GP api & what is practical given todays hardware. I'm actively working on this, more to come soon.

id:
2016
timestamp:
2025-04-03T16:30:31.270Z
sender:
@jay_ztc:matrix.org
content:
* clearloop | SpaceJam: I would consider the idea of "naturally emergent api boundaries"-> a product of the GP api & what is practical given todays hardware. I'm actively working on this, more to come soon.

id:
2015
timestamp:
2025-04-03T16:31:54.078Z
sender:
@jay_ztc:matrix.org
content:
* clearloop | SpaceJam: I would consider the idea of a "naturally emergent api"-> a product of the GP api & what is practical given todays hardware. I'm actively working on this, more to come soon.

id:
2014
timestamp:
2025-04-03T16:36:42.629Z
sender:
@sourabhniyogi:matrix.org
content:
For [CE128 Block request](https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-128-block-request) I _strongly_ agree that a 4-byte (or compact) length prefix preceding each block would be useful.  The additional bandwidth savings from _not_ having this length-prefix is %-wise not meaningful -- if bandwidth savings mattered _that_ much, you could get those savings much more easily (and a bit less work!) by changing the "little-endian 32-bit unsigned integer" [here](https://github.com/zdave-parity/jam-np/blob/main/simple.md#messages) to compact form.   What do you think?

id:
2013
timestamp:
2025-04-03T16:38:09.073Z
sender:
@sourabhniyogi:matrix.org
content:
image.png

id:
2012
timestamp:
2025-04-03T16:38:16.736Z
sender:
@sourabhniyogi:matrix.org
content:
^ context from last year

id:
2011
timestamp:
2025-04-03T16:38:55.536Z
sender:
@clearloop:matrix.org
content:
sounds dope! I'm about to stop talking about PVM here today otherwise I'd like to implement a re-compiler as well now xd, can't afford to race with you since we haven't even finished the runtime part yet 🫠

id:
2010
timestamp:
2025-04-03T16:42:30.108Z
sender:
@dave:parity.io
content:
Would rather not change SNP without a really good reason. It's just a temporary protocol; it's not intended to be the final protocol used by JAM. If you open an issue in the repo we can address things like this in the full protocol

id:
2009
timestamp:
2025-04-03T16:46:05.660Z
sender:
@sourabhniyogi:matrix.org
content:
The really good reason is that its not "simple" and creates a lot of pointless busy work.

id:
2008
timestamp:
2025-04-03T16:46:22.976Z
sender:
@dave:parity.io
content:
How is it not simple?

id:
2007
timestamp:
2025-04-03T16:46:41.181Z
sender:
@dave:parity.io
content:
Just read the message into a vector and then decode blocks from it

id:
2006
timestamp:
2025-04-03T16:46:55.146Z
sender:
@dave:parity.io
content:
This is what polkajam does and it's not particularly complicated

id:
2005
timestamp:
2025-04-03T16:47:44.210Z
sender:
@sourabhniyogi:matrix.org
content:
It should not be a requirement to force implementations to decode with streaming when you can just add a length prefix.  Why is this so important to NOT have the length prefix?

id:
2004
timestamp:
2025-04-03T16:48:01.394Z
sender:
@dave:parity.io
content:
There is a length prefix on every message

id:
2003
timestamp:
2025-04-03T16:49:26.885Z
sender:
@dave:parity.io
content:
There isn't one for each block in this case. This is something I'm happy to change in the full protocol but it doesn't seem like a big issue honestly

id:
2002
timestamp:
2025-04-03T16:51:49.905Z
sender:
@sourabhniyogi:matrix.org
content:
I am asking for a length prefix for each block in the array of blocks in the CE128 response.  
```
<-- [Block]
```
so that the boundaries between Block X and Block X + 1 are trivial to identify, and don't require decoding the individual pieces of each block just to identify that boundary.  

We are happy to do the busywork, as will a dozen other teams, but just to save a few bytes seems quite unnecessary.

id:
2001
timestamp:
2025-04-03T16:52:57.759Z
sender:
@dave:parity.io
content:
It's not to save a few bytes, I'm not sure why anyone thinks that is the case. It's simply to avoid the protocol being a moving target. Of course if there are actual issues with the protocol that needs to be fixed, but things like this don't seem particularly important to me

id:
2000
timestamp:
2025-04-03T16:53:23.851Z
sender:
@dave:parity.io
content:
* It's not to save a few bytes, I'm not sure why anyone thinks that is the case. It's simply to avoid the protocol being a moving target. Of course if there are actual issues with the protocol that need to be fixed we can do that, but things like this don't seem particularly important to me

id:
1999
timestamp:
2025-04-03T18:37:19.643Z
sender:
@sourabhniyogi:matrix.org
content:
Ok its great to know at least one thing is not a moving target!

id:
710
timestamp:
2025-04-04T13:02:54.386Z
sender:
@clearloop:matrix.org
content:
hi there, please correct me if I'm wrong, a should be encoded right after c in (A.37)

https://github.com/gavofyork/graypaper/pull/323/files#diff-16981432fb50e7e5c3d19d2f40b81e3a14b1c9986de5a179b392b47cd8018383R773

id:
709
timestamp:
2025-04-04T13:03:47.015Z
sender:
@clearloop:matrix.org
content:
* hi there, please correct me if I'm wrong, a should be encoded right after c in (A.37) (Standard Program Initialization)

https://github.com/gavofyork/graypaper/pull/323/files#diff-16981432fb50e7e5c3d19d2f40b81e3a14b1c9986de5a179b392b47cd8018383R773

id:
1998
timestamp:
2025-04-04T13:28:13.811Z
sender:
@tomusdrw:matrix.org
content:
That's a bit weird of an argument imho. if the protocol stated it's a var-len sequence of blocks there wouldn't be any boundaries either.

The only issue I see with this particular encoding is that it's neither var-len sequence nor fixed-len sequence as specified in GP, but rather a concatenation of block encodings.

id:
1997
timestamp:
2025-04-04T13:28:59.744Z
sender:
@tomusdrw:matrix.org
content:
* That's a bit weird of an argument imho. if the protocol stated it's a var-len sequence of blocks there wouldn't be any boundaries either.

The only issue I see with this particular encoding is that it's neither var-len sequence nor fixed-len sequence (we don't know the length upfront) as specified in GP, but rather a concatenation of block encodings.

id:
1996
timestamp:
2025-04-04T13:29:36.620Z
sender:
@sourabhniyogi:matrix.org
content:
It doesn't matter, we'll take the "its not a moving target" to be a relief =)

id:
708
timestamp:
2025-04-04T16:16:40.634Z
sender:
@yuchun:matrix.org
content:
Hey there,

I have a question regarding the available work-reports 

The **W** available work-reports (defined in equation [(11.16)](https://graypaper.fluffylabs.dev/#/68eaa1f/144601144601?v=0.6.4)) are extracted from `rhoDagger` using the core index. As I understand it, each core should correspond to only one work-report, is that correct?

However, I’m a bit confused about equation [(13.10)](https://graypaper.fluffylabs.dev/#/68eaa1f/195601195601?v=0.6.4). It sums the work-reports for a specific core from the set of available work-reports. Does this imply that the same core might appear multiple times in the available work-reports?

Please feel free to let me know if I’ve misunderstood anything.

Thanks


id:
1995
timestamp:
2025-04-04T18:43:45.000Z
sender:
@gav:polkadot.io
content:
> <@tomusdrw:matrix.org> That's a bit weird of an argument imho. if the protocol stated it's a var-len sequence of blocks there wouldn't be any boundaries either.
> 
> The only issue I see with this particular encoding is that it's neither var-len sequence nor fixed-len sequence (we don't know the length upfront) as specified in GP, but rather a concatenation of block encodings.

That’s a regular sequence encoding as specified in the GP. 

id:
1994
timestamp:
2025-04-04T18:46:02.938Z
sender:
@gav:polkadot.io
content:
GP requires the double-end-vertical-arrow prefix notation in order to prepend a length. GP doesn’t state that it *must* be used when the sequence length may not be static, and indeed there are instances (like hashing) where it may reasonably be omitted even if the sequence is of variable length. 

id:
1993
timestamp:
2025-04-04T18:47:00.885Z
sender:
@gav:polkadot.io
content:
Streaming, or decoding from a known size container, would be other such instances. 

id:
707
timestamp:
2025-04-04T18:49:17.665Z
sender:
@gav:polkadot.io
content:
Yes and no. 

id:
706
timestamp:
2025-04-04T18:49:58.813Z
sender:
@gav:polkadot.io
content:
(In the case of 13.10, sum was simply to ensure that we get zero if the core is empty) 

id:
1992
timestamp:
2025-04-04T19:40:42.762Z
sender:
@tomusdrw:matrix.org
content:
Sure, it is, but the receiver does not know how many elements is there.
From that perspective a variable-length sequence encoding (with a length discriminator) is not needed at all.
Obviously there is a much better-defined boundary of the items in sequence (end of the byte stream) vs not-matching encoding, but still I'd argue that this kind of usage of fixed-length encoding is rather a peculiar case.

id:
1991
timestamp:
2025-04-04T19:40:59.229Z
sender:
@tomusdrw:matrix.org
content:
* Sure, it is, but the receiver does not know how many elements is there.
From that perspective a variable-length sequence encoding (with a length discriminator) is not needed at all.
Obviously there is a much better-defined boundary of the items in sequence (end of the byte stream vs not-matching encoding), but still I'd argue that this kind of usage of fixed-length encoding is rather a peculiar case.

id:
1990
timestamp:
2025-04-04T20:26:00.678Z
sender:
@gav:polkadot.io
content:
Again it’s not actually a fixed length encoding. 

id:
1989
timestamp:
2025-04-04T20:26:15.683Z
sender:
@gav:polkadot.io
content:
It’s  just not prefixed with the number of elements. 

id:
1988
timestamp:
2025-04-04T20:26:39.286Z
sender:
@gav:polkadot.io
content:
* Again would not be correct to call it a “fixed length encoding”. 

id:
1987
timestamp:
2025-04-04T20:27:21.596Z
sender:
@gav:polkadot.io
content:
So most of the time the length prefix will be used when there isn’t a fixed number of elements but correlation is not equivalence. 

id:
1986
timestamp:
2025-04-04T20:28:22.178Z
sender:
@gav:polkadot.io
content:
As for the wisdom of prefixing with the sequence length or not in this case, I agree it is arguable. 

id:
1985
timestamp:
2025-04-04T20:29:55.977Z
sender:
@gav:polkadot.io
content:
But it’d be wrong to characterise it as somehow strictly wrong on the mistaken basis that the length prefix strictly implies and I’d implied by whether the underlying data is defined as have a fixed length. 

id:
1984
timestamp:
2025-04-04T20:30:32.791Z
sender:
@gav:polkadot.io
content:
* But it’d be wrong to characterise it as somehow strictly wrong on the mistaken basis that the length prefix strictly *implies and is implied by* the condition of the underlying data being defined as have a fixed length. 

id:
1983
timestamp:
2025-04-04T20:30:52.460Z
sender:
@gav:polkadot.io
content:
* But it’d be wrong to characterise it as legally incorrect on the mistaken basis that the length prefix strictly *implies and is implied by* the condition of the underlying data being defined as have a fixed length. 

id:
1982
timestamp:
2025-04-04T20:31:36.498Z
sender:
@gav:polkadot.io
content:
* But it’d be wrong to characterise it as literally incorrect on the mistaken basis that the length prefix strictly *implies and is implied by* the condition of the underlying data being defined as have a fixed length. 

id:
1981
timestamp:
2025-04-04T20:33:08.789Z
sender:
@gav:polkadot.io
content:
There are already multiple instances in the GP where sequence term with non-fixed length is encoded without the length prefix.

id:
1980
timestamp:
2025-04-04T20:35:42.012Z
sender:
@gav:polkadot.io
content:
And on a more general note, the GP doesn’t use any kind of formal typing; 42 is 42 whether it is in N_{256} or N_{2^32}. With this set theoretic syntax it is not actually possible to reason about whether the data is fixed or variable length from the perspective of the value itself. 

id:
1979
timestamp:
2025-04-04T20:36:50.627Z
sender:
@gav:polkadot.io
content:
So it’s best to use terminology that reflects the formalism. 

id:
705
timestamp:
2025-04-04T22:07:56.330Z
sender:
@0xjunha:matrix.org
content:
In memory accessibility notation, is there any specific reason to use both \subseteq and \subset ? Or is that a typo?
\subset makes more sense to me - some host functions (and sbrk inst) are using \subseteq while others use \subset.

https://graypaper.fluffylabs.dev/#/68eaa1f/34a90234a902?v=0.6.4
https://graypaper.fluffylabs.dev/#/68eaa1f/336b00336b00?v=0.6.4

id:
704
timestamp:
2025-04-05T04:41:17.529Z
sender:
@gav:polkadot.io
content:
> <@0xjunha:matrix.org> In memory accessibility notation, is there any specific reason to use both \subseteq and \subset ? Or is that a typo?
> \subset makes more sense to me - some host functions (and sbrk inst) are using \subseteq while others use \subset.
> 
> https://graypaper.fluffylabs.dev/#/68eaa1f/34a90234a902?v=0.6.4
> https://graypaper.fluffylabs.dev/#/68eaa1f/336b00336b00?v=0.6.4

Typo. It should all be subseteq

id:
703
timestamp:
2025-04-07T04:55:13.305Z
sender:
@shwchg:matrix.org
content:
https://graypaper.fluffylabs.dev/#/68eaa1f/09cf0109d301?v=0.6.4
https://graypaper.fluffylabs.dev/#/68eaa1f/09e70109e901?v=0.6.4
is the jam common era timestamp annotation wrong? 
1,735,689,600 (0000 UTC on January 1, 2025)
or
1,735,732,800 (1200 UTC on January 1, 2025)

id:
1978
timestamp:
2025-04-07T10:59:50.959Z
sender:
@emielsebastiaan:matrix.org
content:
Hello all,

I'd like to kick off a little discussion we had internally in our team meeting recently regarding our collective approach in working towards a public testnet.
Last October in Bangkok those attending JAM0 have initiated an initiative to converge towards a community testnet, called the JAMduna Testnet.
Me and my team believe this is an admirable objective, however we have some worries about potential conflicts with the JAM-prize rules, that may warrent discussion and ideally clarification from W3F and/or Gavin.

The official rules state the following: 
- Rule 6: "Clean-room implementation using the Graypaper and public implementers' channel as the only resources."
- Rule 8: "Relevant private (not in the public implementers' channel) conversations with other implementers must be declared and summarised in reports to the Fellowship."
- Rule 18: "Implementations must pass all relevant public and private conformance/performance tests. These will be shared in the near future."

Gav's Unofficial notes add to rule 6:
- Additional materials may be added here at a later stage.

What we like about the Testnet:
- We believe some coordination is absolutely necessary in order to get to the minimum necessary agreement on practical testnet setups.
- Coordination towards a testnet in our opinion is no different than coordination around the W3F Test Vectors. Like the community findings of W3F Test Vectors, the community findings in the testnet repo lead to higher quality implementations, findings of errors and ambiguities in Graypaper and findings of errors in the W3F Testvectors.

What we are concerned about with the Testnet
- Discussions and thus coordination activities are increasingly happening on other channels like Telegram and Discord (read: NOT the public implementers' channel on Matrix). These channels are arguably public since they were once referenced on the Matrix channels, but these are separate channels that could potentially unlevel the playing field or create an entry barrier.
- Rule 6 and 18 do not explicitly state the Testnet resources to be accepted as a source of coordination (as for the W3F Testvectors for the record).
- Obviously any resource outside Graypaper allows for potential blindspots to emerge (collective misinterpretations). There is however a trade-off to be made here.

Questions we have:
- Is there a risk of JAM-price penalty when a team collaborates on the initiative towards a public testnet?
- Can we classify the JAMDUNA testnet repository in the same category as the W3F Testvectors? And perhaps add this to the (unofficial) rules ("Additional materials may be added here at a later stage"). Or agree on what needs to happen to make this so.
- Can we agree on some do's and dont's in working towards the testnet?

Let's use this thread to have a discussion about this.

id:
702
timestamp:
2025-04-07T11:38:18.636Z
sender:
@0xjunha:matrix.org
content:
Probably will be included in the next release?

https://github.com/gavofyork/graypaper/commit/b16207dc291d343991de2fdfb9aa3feb25927b50

id:
1977
timestamp:
2025-04-07T12:22:15.503Z
sender:
@gav:polkadot.io
content:
> <@emielsebastiaan:matrix.org> Hello all,
> 
> I'd like to kick off a little discussion we had internally in our team meeting recently regarding our collective approach in working towards a public testnet.
> Last October in Bangkok those attending JAM0 have initiated an initiative to converge towards a community testnet, called the JAMduna Testnet.
> Me and my team believe this is an admirable objective, however we have some worries about potential conflicts with the JAM-prize rules, that may warrent discussion and ideally clarification from W3F and/or Gavin.
> 
> The official rules state the following: 
> - Rule 6: "Clean-room implementation using the Graypaper and public implementers' channel as the only resources."
> - Rule 8: "Relevant private (not in the public implementers' channel) conversations with other implementers must be declared and summarised in reports to the Fellowship."
> - Rule 18: "Implementations must pass all relevant public and private conformance/performance tests. These will be shared in the near future."
> 
> Gav's Unofficial notes add to rule 6:
> - Additional materials may be added here at a later stage.
> 
> What we like about the Testnet:
> - We believe some coordination is absolutely necessary in order to get to the minimum necessary agreement on practical testnet setups.
> - Coordination towards a testnet in our opinion is no different than coordination around the W3F Test Vectors. Like the community findings of W3F Test Vectors, the community findings in the testnet repo lead to higher quality implementations, findings of errors and ambiguities in Graypaper and findings of errors in the W3F Testvectors.
> 
> What we are concerned about with the Testnet
> - Discussions and thus coordination activities are increasingly happening on other channels like Telegram and Discord (read: NOT the public implementers' channel on Matrix). These channels are arguably public since they were once referenced on the Matrix channels, but these are separate channels that could potentially unlevel the playing field or create an entry barrier.
> - Rule 6 and 18 do not explicitly state the Testnet resources to be accepted as a source of coordination (as for the W3F Testvectors for the record).
> - Obviously any resource outside Graypaper allows for potential blindspots to emerge (collective misinterpretations). There is however a trade-off to be made here.
> 
> Questions we have:
> - Is there a risk of JAM-price penalty when a team collaborates on the initiative towards a public testnet?
> - Can we classify the JAMDUNA testnet repository in the same category as the W3F Testvectors? And perhaps add this to the (unofficial) rules ("Additional materials may be added here at a later stage"). Or agree on what needs to happen to make this so.
> - Can we agree on some do's and dont's in working towards the testnet?
> 
> Let's use this thread to have a discussion about this.

The rules are there for a reason. Disregarding them, in particular with setting up additional semi-private channels for teams to exchange notes and ideas would endanger the prize.

id:
1976
timestamp:
2025-04-07T12:22:46.581Z
sender:
@gav:polkadot.io
content:
> <@emielsebastiaan:matrix.org> Hello all,
> 
> I'd like to kick off a little discussion we had internally in our team meeting recently regarding our collective approach in working towards a public testnet.
> Last October in Bangkok those attending JAM0 have initiated an initiative to converge towards a community testnet, called the JAMduna Testnet.
> Me and my team believe this is an admirable objective, however we have some worries about potential conflicts with the JAM-prize rules, that may warrent discussion and ideally clarification from W3F and/or Gavin.
> 
> The official rules state the following: 
> - Rule 6: "Clean-room implementation using the Graypaper and public implementers' channel as the only resources."
> - Rule 8: "Relevant private (not in the public implementers' channel) conversations with other implementers must be declared and summarised in reports to the Fellowship."
> - Rule 18: "Implementations must pass all relevant public and private conformance/performance tests. These will be shared in the near future."
> 
> Gav's Unofficial notes add to rule 6:
> - Additional materials may be added here at a later stage.
> 
> What we like about the Testnet:
> - We believe some coordination is absolutely necessary in order to get to the minimum necessary agreement on practical testnet setups.
> - Coordination towards a testnet in our opinion is no different than coordination around the W3F Test Vectors. Like the community findings of W3F Test Vectors, the community findings in the testnet repo lead to higher quality implementations, findings of errors and ambiguities in Graypaper and findings of errors in the W3F Testvectors.
> 
> What we are concerned about with the Testnet
> - Discussions and thus coordination activities are increasingly happening on other channels like Telegram and Discord (read: NOT the public implementers' channel on Matrix). These channels are arguably public since they were once referenced on the Matrix channels, but these are separate channels that could potentially unlevel the playing field or create an entry barrier.
> - Rule 6 and 18 do not explicitly state the Testnet resources to be accepted as a source of coordination (as for the W3F Testvectors for the record).
> - Obviously any resource outside Graypaper allows for potential blindspots to emerge (collective misinterpretations). There is however a trade-off to be made here.
> 
> Questions we have:
> - Is there a risk of JAM-price penalty when a team collaborates on the initiative towards a public testnet?
> - Can we classify the JAMDUNA testnet repository in the same category as the W3F Testvectors? And perhaps add this to the (unofficial) rules ("Additional materials may be added here at a later stage"). Or agree on what needs to happen to make this so.
> - Can we agree on some do's and dont's in working towards the testnet?
> 
> Let's use this thread to have a discussion about this.

* The rules are there for a reason. Disregarding them, in particular with setting up additional semi-private, invitation only or even merely difficult to discover channels for teams to exchange notes and ideas would endanger the prize.

id:
1975
timestamp:
2025-04-07T12:23:02.196Z
sender:
@gav:polkadot.io
content:
> <@emielsebastiaan:matrix.org> Hello all,
> 
> I'd like to kick off a little discussion we had internally in our team meeting recently regarding our collective approach in working towards a public testnet.
> Last October in Bangkok those attending JAM0 have initiated an initiative to converge towards a community testnet, called the JAMduna Testnet.
> Me and my team believe this is an admirable objective, however we have some worries about potential conflicts with the JAM-prize rules, that may warrent discussion and ideally clarification from W3F and/or Gavin.
> 
> The official rules state the following: 
> - Rule 6: "Clean-room implementation using the Graypaper and public implementers' channel as the only resources."
> - Rule 8: "Relevant private (not in the public implementers' channel) conversations with other implementers must be declared and summarised in reports to the Fellowship."
> - Rule 18: "Implementations must pass all relevant public and private conformance/performance tests. These will be shared in the near future."
> 
> Gav's Unofficial notes add to rule 6:
> - Additional materials may be added here at a later stage.
> 
> What we like about the Testnet:
> - We believe some coordination is absolutely necessary in order to get to the minimum necessary agreement on practical testnet setups.
> - Coordination towards a testnet in our opinion is no different than coordination around the W3F Test Vectors. Like the community findings of W3F Test Vectors, the community findings in the testnet repo lead to higher quality implementations, findings of errors and ambiguities in Graypaper and findings of errors in the W3F Testvectors.
> 
> What we are concerned about with the Testnet
> - Discussions and thus coordination activities are increasingly happening on other channels like Telegram and Discord (read: NOT the public implementers' channel on Matrix). These channels are arguably public since they were once referenced on the Matrix channels, but these are separate channels that could potentially unlevel the playing field or create an entry barrier.
> - Rule 6 and 18 do not explicitly state the Testnet resources to be accepted as a source of coordination (as for the W3F Testvectors for the record).
> - Obviously any resource outside Graypaper allows for potential blindspots to emerge (collective misinterpretations). There is however a trade-off to be made here.
> 
> Questions we have:
> - Is there a risk of JAM-price penalty when a team collaborates on the initiative towards a public testnet?
> - Can we classify the JAMDUNA testnet repository in the same category as the W3F Testvectors? And perhaps add this to the (unofficial) rules ("Additional materials may be added here at a later stage"). Or agree on what needs to happen to make this so.
> - Can we agree on some do's and dont's in working towards the testnet?
> 
> Let's use this thread to have a discussion about this.

* The rules are there for a reason. Disregarding them, in particular with setting up additional semi-private, invitation only or even merely difficult to discover channels for teams to exchange notes and ideas would endanger those teams ability to claim the prize.

id:
701
timestamp:
2025-04-07T12:23:34.839Z
sender:
@shwchg:matrix.org
content:
Thanks!

id:
1974
timestamp:
2025-04-07T13:01:15.351Z
sender:
@emielsebastiaan:matrix.org
content:
Thank you. Nothing new here, but this strengthens our concern regarding the additional channels.
We are of the opinion that participating in Telegram and Discord channels might not be smart (free advice) when discussing findings, GP-ambiguities, and similar things.

Follow-up question I have:
1. Are current W3F Test Vectors considered public conformance tests? These have proven to be a source of finding Graypaper mistakes and ambiguities.
2. Can JAMDUNA testnet traces (https://github.com/jam-duna/jamtestnet) be considered public conformance tests equally? These have proven to be a source of finding Graypaper mistakes and ambiguities.
3. Can JAM-Docs (https://github.com/jambrains/jam-docs) be considered a public resource for JAM Implementor teams?

When considering (2) and (3) these in contrast to (1) are not listed in the Resources section on Graypaper.com.
There is a collective payoff in these initiatives (better graypaper) and individual payoff (additional source of finding mistakes in your implementation).

4. IMO it would be smart to keep a good accounting record of the list of things your team got from these unofficial resources and the list of things your team contributed to the community due to these unofficial resources.

5. Is working towards to public testnet (JAMDUNA initiative) in its core an activity that might conflict with JAM Prize rules?

id:
1973
timestamp:
2025-04-07T13:16:16.277Z
sender:
@gav:polkadot.io
content:
With regards to a testnet, it’s a little complicated. In principle I think it is admirable and wonderful to see teams so keen on pushing things forward. It’s a good “problem”to have. But indeed it could be a problem. My main concern is over the accessibility of such a network - there are perverse incentives because of the (regrettably necessary) first come first serve nature of the prize. A secondary concern is that those partaking in it may inadvertently get extra “knowledge” over conventions and customs which others do not. This is exasperated by communications happening outside of the two official channels. 

id:
1972
timestamp:
2025-04-07T13:16:46.728Z
sender:
@gav:polkadot.io
content:
1. Yes, this is the aim. 

id:
1971
timestamp:
2025-04-07T13:18:20.882Z
sender:
@gav:polkadot.io
content:
2. Feel free to report GP ambiguities or clarification questions that arise from this. Feel very free to publish them.  They cannot be considered official requirements though as there is no W3F review process in place. 

id:
1970
timestamp:
2025-04-07T13:19:37.068Z
sender:
@gav:polkadot.io
content:
3. No it cannot. It may very be a helpful resource but only the GP is official for the JAM prize. 

id:
1969
timestamp:
2025-04-07T13:20:10.217Z
sender:
@gav:polkadot.io
content:
Even JIPs/RFCs are unofficial conventions until/unless they move or are referenced by the GP. 

id:
1968
timestamp:
2025-04-07T13:21:19.824Z
sender:
@gav:polkadot.io
content:
5. Potentially yes. 

id:
1967
timestamp:
2025-04-07T13:23:18.510Z
sender:
@emielsebastiaan:matrix.org
content:
I get your concern about a public testnet, which given your statement may not be smart to pursue.
That said, so far no such public testnet exists only coordination on how such testnet could be set up. 
Is this where a line should be drawn? 

One interesting snippet from my team meeting:
Team member: Gavin's team was looking at the JAMDUNA testnet repository. So it is okay.
My paranoid response: Gavin's team might not be pursuing JAM Prize payout.


id:
1966
timestamp:
2025-04-07T13:23:22.834Z
sender:
@gav:polkadot.io
content:
In so much as conformance to an unofficial testnet has no direct relevance to the GP and thus could be considered either a distraction or outright dangerous for consensus (by forming sub-dialects of the protocol). 

id:
1965
timestamp:
2025-04-07T13:23:56.978Z
sender:
@gav:polkadot.io
content:
Well, naturally I’m interested to see!

id:
1964
timestamp:
2025-04-07T13:25:50.734Z
sender:
@gav:polkadot.io
content:
But the purpose of the prize is to maximise team independence and expertise. To the extent that an unofficial testnet works in favour of this, I’m in favour. But it’s a judgement call, and ultimately i believe it possible for teams to cross the line in pursuit of *their* consensus. 

id:
1963
timestamp:
2025-04-07T13:26:27.807Z
sender:
@gav:polkadot.io
content:
* But the purpose of the prize is to maximise team independence and expertise. To the extent that an unofficial testnet works in favour of this, I’m in favour. But it’s a judgement call, and ultimately i believe it possible for teams to cross the line in pursuit of *their* consensus. Resulting in a case of structurally reduced consensus among JAM impls as a whole. 

id:
1962
timestamp:
2025-04-07T13:27:13.673Z
sender:
@gav:polkadot.io
content:
I think provided the testnet is 100% open, zero-barrier and responsible to W3F/Fellowship it’ll be fine.

id:
1961
timestamp:
2025-04-07T13:29:29.216Z
sender:
@gav:polkadot.io
content:
Which is precisely why I’d prefer to limit testnets to the toaster and meet-ups where all teams get an invitation, open access is clear, and communications are recorded for those not present to benefit from

id:
1960
timestamp:
2025-04-07T13:32:33.263Z
sender:
@emielsebastiaan:matrix.org
content:
Yes I agree fully. I was concerned enough to ask for clarifications. Dropped this here so we can have an open discussion about these things. Thank you Gavin!

id:
1959
timestamp:
2025-04-07T13:44:19.471Z
sender:
@gav:polkadot.io
content:
Without making any promises, I’d say that outside of the get-togethers, 0.7.0 would be the time to start a persistent, open access, public testnet. It will likely need to be reset on subsequent GP releases (with an appropriate delay to give teams a chance to catch up to any changes).

id:
1958
timestamp:
2025-04-07T14:19:41.040Z
sender:
@oliver.tale-yazdi:parity.io
content:
The jam-docs website explicitly mentions to only contain information that is *not* part of the GP. Which is how I hoped to avoid any controversy around it.  

id:
1957
timestamp:
2025-04-07T14:20:19.615Z
sender:
@oliver.tale-yazdi:parity.io
content:
* The jam-docs website explicitly mentions to only contain information that is _not_ part of the GP. Which is how I hoped to avoid any controversy around it.  
But I guess the intro sentence still contains the prize link, which can be misinterpreted

id:
1956
timestamp:
2025-04-07T14:22:20.709Z
sender:
@sourabhniyogi:matrix.org
content:
This repo has been 100% open for a while -- and we (and many other teams) have learned a ton, not via private communications but via several dozen issues referencing GP equations and derivations (and in a few cases, asking for clarifications)

https://github.com/jam-duna/jamtestnet/releases/tag/0.6.4.4
https://github.com/jam-duna/jamtestnet

So far, there is nothing much in the way of teams actually connecting to each other
 https://github.com/jam-duna/jamtestnet/issues/69
Last week was the only connection we made, with the primary discovery that we had to lowercase our SAN, and that we needed to address CE128 multiblock responses.  These conversations can definitely be 100x noisy and require a "did you get header hash 0x4e37..." type level -- WAY too noisy for a room with several hundred people and I think, a few dozen teams in a "jamtestnet" room.

So far the "jamtestnet" is a total misnomer -- more accurately it can be called "importblocksalignment" but, with a half dozen having achieved that alignment.

From [this](https://github.com/jam-duna/jamtestnet/issues/69) there are actually only a few teams actually both wanting and ready to participate in a testnet.  Given the "the testnet is 100% open, zero-barrier" requirement, we can quell any "could you make this a little less noise" type complaints by doing all this testnet work in a public room.  Does it matter if its Discord, Telegram or Matrix?  Or should we just shut down the repo and wait for the Toaster?

id:
1955
timestamp:
2025-04-07T14:23:40.468Z
sender:
@sourabhniyogi:matrix.org
content:
* This repo has been 100% open for a while -- and we (and many other teams) have learned a ton, not via private communications but via several dozen issues referencing GP equations and derivations (and in a few cases, asking for clarifications)

https://github.com/jam-duna/jamtestnet/releases/tag/0.6.4.4
https://github.com/jam-duna/jamtestnet

So far, there is nothing much in the way of teams actually connecting to each other
https://github.com/jam-duna/jamtestnet/issues/69
Last week was the only connection we made, with the primary discovery that we had to lowercase our SAN, and that we needed to address CE128 multiblock responses.  These conversations can definitely be 100x noisy and require a "did you get header hash 0x4e37..." type level -- WAY too noisy for a room with several hundred people and I think, a few dozen teams in a "jamtestnet" room.

So far the "jamtestnet" is a total misnomer -- more accurately it can be called "importblocksalignment" but, with a half dozen having achieved that alignment, its possible to achieve real "accumulate-side" testnet alignment, with actual import blocks happening from genesis onwards!

From [this](https://github.com/jam-duna/jamtestnet/issues/69) there are actually only a few teams actually both wanting and ready to participate in a testnet.  Given the "the testnet is 100% open, zero-barrier" requirement, we can quell any "could you make this a little less noise" type complaints by doing all this testnet work in a public room.  Does it matter if its Discord, Telegram or Matrix?  Or should we just shut down the repo and wait for the Toaster?

id:
1954
timestamp:
2025-04-07T14:24:14.894Z
sender:
@sourabhniyogi:matrix.org
content:
* This repo has been 100% open for a while -- and we (and many other teams) have learned a ton, not via private communications but via several dozen issues referencing GP equations and derivations (and in a few cases, asking for clarifications)

https://github.com/jam-duna/jamtestnet/releases/tag/0.6.4.4
https://github.com/jam-duna/jamtestnet

So far, there is nothing much in the way of teams actually connecting to each other
https://github.com/jam-duna/jamtestnet/issues/69
Last week was the only connection we made, with the primary discovery that we had to lowercase our SAN, and that we needed to address CE128 multiblock responses.  These conversations can definitely be 100x noisy and require a "did you get header hash 0x4e37..." type level -- WAY too noisy for a room with several hundred people and I think, a few dozen teams in a "jamtestnet" room.

So far the "jamtestnet" is a total misnomer -- more accurately it can be called "importblocksalignment" but, with a half dozen having achieved that alignment, its possible to achieve real "accumulate-side" testnet alignment, with actual import blocks happening from genesis onwards!

From [this](https://github.com/jam-duna/jamtestnet/issues/69) there are actually only a few teams actually both wanting and ready to participate in a testnet.  Given the "the testnet is 100% open, zero-barrier" requirement, we can quell any "could you make this a little less noisy" type complaints by doing all this testnet work in a public room.  Does it matter if its Discord, Telegram or Matrix?  Or should we just shut down the jamduna/jamtestnet repo and wait for the Toaster?

id:
1953
timestamp:
2025-04-07T14:28:45.441Z
sender:
@sourabhniyogi:matrix.org
content:
If Matrix is best, we currently have a "JAM Implementers" room with 59 people -- we can make that public and meet the "the testnet is 100% open, zero-barrier" goal -- is that the best answer?

id:
1952
timestamp:
2025-04-07T14:29:24.297Z
sender:
@emielsebastiaan:matrix.org
content:
My concerns are a reflection of my team’s internal discussions. It is not a critique of any kind. Simply a note of caution and an attempt to open the discussion of avoiding consensus blind spots. Some great takeaways for everyone in this thread. 

id:
1951
timestamp:
2025-04-07T14:35:02.526Z
sender:
@rick:carback.us
content:
can confirm, i'm (goberryjam) a ways from being able to participate in a testnet.. I expect to not be the first in my category but hopeful to make useful contribution in the form of some high quality golang components folks can work into their implementations once we go more open. 

id:
1950
timestamp:
2025-04-07T15:15:39.306Z
sender:
@p1sar:matrix.org
content:
With all due respect, the concern that participants might "inadvertently gain extra 'knowledge'" not shared by others could just as easily apply to someone attending the PBA or other University, for instance, compared to someone who did not.

id:
1949
timestamp:
2025-04-07T15:20:32.381Z
sender:
@sourabhniyogi:matrix.org
content:
* This repo has been 100% open for a while -- and we (and many other teams) have learned a ton, not via private communications but via several dozen issues referencing GP equations and derivations (and in a few cases, asking for clarifications)

https://github.com/jam-duna/jamtestnet/releases/tag/0.6.4.4
https://github.com/jam-duna/jamtestnet

So far, there is nothing much in the way of teams actually connecting to each other
https://github.com/jam-duna/jamtestnet/issues/69
Last week was the only connection we made, with the primary discovery that we had to lowercase our SAN, and that we needed to address CE128 multiblock responses.  These conversations can definitely be 100x noisy and require a "did you get header hash 0x4e37..." type level real time communication pattern -- WAY too noisy for a room with several hundred people and I think, a few dozen teams in a "jamtestnet" room, whether for the toaster or just for 2 teams to figure out wire format bugs and QUIC connectivity.   

So far the "jamtestnet" is a total misnomer -- more accurately it can be called "importblocksalignment" but, with a half dozen having achieved that alignment, its possible to achieve real "accumulate-side" testnet alignment, with actual import blocks happening from genesis onwards!

From [this](https://github.com/jam-duna/jamtestnet/issues/69) there are actually only a few teams actually both wanting and ready to participate in a testnet.  Given the "the testnet is 100% open, zero-barrier" requirement, we can quell any "could you make this a little less noisy" type complaints by doing all this testnet work in a public room.  Does it matter if its Discord, Telegram or Matrix?  Or should we just shut down the jamduna/jamtestnet repo and wait for the Toaster?

id:
1948
timestamp:
2025-04-07T15:48:12.889Z
sender:
@emielsebastiaan:matrix.org
content:
The PBA example does not have the effect of a slippery slope of compromising implementation/team independence.  

id:
1947
timestamp:
2025-04-07T16:01:14.651Z
sender:
@sourabhniyogi:matrix.org
content:
I suggest we make this a topic on next weeks call =)

id:
1946
timestamp:
2025-04-07T16:08:40.224Z
sender:
@sourabhniyogi:matrix.org
content:
For JAM Implementers who may not be aware, this room is now public: 
 https://matrix.to/#/!KKOmuUpvYKPcniwOzw:matrix.org?via=matrix.org&via=parity.io
PARTICIPATE AT YOUR OWN RISK =)



id:
1945
timestamp:
2025-04-07T16:13:31.660Z
sender:
@sourabhniyogi:matrix.org
content:
* If Matrix is best, we currently have a "JAM Implementers" room with 59 people -- I have made that public and this could be used to have super noisy testnet work all done in public to meet the "the testnet is 100% open, zero-barrier" goal -- is that the best answer?

id:
700
timestamp:
2025-04-08T20:08:06.150Z
sender:
@prematurata:matrix.org
content:
tecnically speaking is there something preventing the same service to be executed multiple times in the same block?

id:
699
timestamp:
2025-04-08T20:08:14.165Z
sender:
@prematurata:matrix.org
content:
* I have a question: tecnically speaking is there something preventing the same service to be executed multiple times in the same block?

id:
698
timestamp:
2025-04-08T20:16:28.675Z
sender:
@gav:polkadot.io
content:
Not at all. 

id:
697
timestamp:
2025-04-08T20:17:26.120Z
sender:
@gav:polkadot.io
content:
And it can happen due to queuing and earlier work tranches but using all their allotted gas. 

id:
696
timestamp:
2025-04-08T20:20:21.867Z
sender:
@prematurata:matrix.org
content:
tkz

id:
1944
timestamp:
2025-04-09T03:23:30.813Z
sender:
@xlchen:matrix.org
content:
I am confused about CE128 block request https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-128-block-request

> Note that blocks directly contain only the hashes of included work-reports and preimages. If unknown, the actual work-reports and preimages should be requested using protocols 136 and 143 respectively.

and

> Block = As in GP

but blocks defined in GP already contains the work report and preimages, not just the hashes of them?

id:
1943
timestamp:
2025-04-09T08:52:10.297Z
sender:
@dave:parity.io
content:
Yeah I think this and/or the GP needs updating. Blocks requested via CE128 are not intended to include reports/preimages. The idea is that the vast majority of the size of a block will be reports, and in practice these are distributed to all validators in advance, so it's a waste of time and bandwidth to distribute them again with new blocks

id:
1942
timestamp:
2025-04-09T08:54:01.529Z
sender:
@xlchen:matrix.org
content:
I see. So is not `As in GP` at current version. It will be a LightBlock or something that the preimages and work reports will be replaced by a hash?

id:
1941
timestamp:
2025-04-09T08:54:36.586Z
sender:
@xlchen:matrix.org
content:
I will do full block for now until the light block is defined

id:
1940
timestamp:
2025-04-09T08:56:04.015Z
sender:
@xlchen:matrix.org
content:
but this is the simple protocol... so maybe just keep it simple? or we are expecting the simple protocol should be as close as possible to the final protocol?

id:
1939
timestamp:
2025-04-09T09:30:42.716Z
sender:
@dave:parity.io
content:
Full block for now is reasonable. It's effectively what is specified and is what eg Polkajam is currently doing. I'll remove this wording that implies light blocks for now. Light blocks are still the plan for the full protocol, and _may_ be necessary to add to SNP for performance reasons

id:
1938
timestamp:
2025-04-09T15:15:58.219Z
sender:
@snowmead:matrix.org
content:
quick question: is it ok to rely on the ASN type definitions in the test vectors to quickly create the basic objects and types in our implementation? Obviously I would be validating these types as I implement the protocol. 

id:
1937
timestamp:
2025-04-09T15:16:49.659Z
sender:
@snowmead:matrix.org
content:
* quick question: is it ok to rely on the ASN type definitions in the test vectors to quickly create the basic objects and types in our implementation? Obviously I would be validating these types as I implement the protocol and correct any mismatch

id:
1936
timestamp:
2025-04-09T16:40:40.818Z
sender:
@codingsh:matrix.org
content:
Hi guys! Is it possible for coretime to become a governance model on JAM? 

id:
1935
timestamp:
2025-04-09T16:46:20.773Z
sender:
@erin:parity.io
content:
> <@codingsh:matrix.org> Hi guys! Is it possible for coretime to become a governance model on JAM? 

coretime is not a governance model, it is a market for allocation of blockspace. governance is much more nuanced and can control many more things than blockspace allocation 

id:
1934
timestamp:
2025-04-09T16:56:29.238Z
sender:
@codingsh:matrix.org
content:
> <@erin:parity.io> coretime is not a governance model, it is a market for allocation of blockspace. governance is much more nuanced and can control many more things than blockspace allocation 

Thank you, I’ve been thinking about PVM, and the need for coretime to be executed today is very cheap, but when we have global adhesion. who has coretime could determine what will be executed?

id:
1933
timestamp:
2025-04-09T16:59:07.516Z
sender:
@erin:parity.io
content:
It is a permissionless system. With both coretime and JAM whoever pays for the computation time is able to run what they would like to.

id:
1932
timestamp:
2025-04-09T17:02:35.844Z
sender:
@codingsh:matrix.org
content:
Awesome, Perfect! 

id:
1931
timestamp:
2025-04-09T19:25:00.713Z
sender:
@davxy:matrix.org
content:
> <@snowmead:matrix.org> quick question: is it ok to rely on the ASN type definitions in the test vectors to quickly create the basic objects and types in our implementation? Obviously I would be validating these types as I implement the protocol and correct any mismatch

The ASN.1 module included with the test vectors **should** map one-to-one with the types defined in the GP. If you notice any discrepancies:

-The GP is the authoritative source of truth.
- Please open an issue to report it :-)

id:
1930
timestamp:
2025-04-09T19:25:38.856Z
sender:
@snowmead:matrix.org
content:
thanks for letting me know

id:
1929
timestamp:
2025-04-09T19:26:15.341Z
sender:
@davxy:matrix.org
content:
> <@snowmead:matrix.org> quick question: is it ok to rely on the ASN type definitions in the test vectors to quickly create the basic objects and types in our implementation? Obviously I would be validating these types as I implement the protocol. 

* The ASN.1 module included with the test vectors **should** map one-to-one with the types defined in the GP. If you notice any discrepancies:
-The GP is the authoritative source of truth.
- Please open an issue to report it :-)

id:
1928
timestamp:
2025-04-09T19:26:39.637Z
sender:
@davxy:matrix.org
content:
> <@snowmead:matrix.org> quick question: is it ok to rely on the ASN type definitions in the test vectors to quickly create the basic objects and types in our implementation? Obviously I would be validating these types as I implement the protocol. 

* The ASN.1 module included with the test vectors **should** map one-to-one with the types defined in the GP. If you notice any discrepancies:

- The GP is the authoritative source of truth.
- Please open an issue to report it :-)

id:
1927
timestamp:
2025-04-09T22:05:31.277Z
sender:
@sourabhniyogi:matrix.org
content:
image.png

id:
1926
timestamp:
2025-04-09T22:05:37.072Z
sender:
@sourabhniyogi:matrix.org
content:
Can we get whoever did these Polkadot Icons here
 https://icons.polkadot.network/ 
to do a refresh for JAM?  I did some placeholders


id:
1925
timestamp:
2025-04-10T06:04:59.416Z
sender:
@clearloop:matrix.org
content:
interesting, however, seems the repo is outdated now https://github.com/w3f/polkadot-react-icons

id:
1924
timestamp:
2025-04-11T09:54:51.519Z
sender:
@decentration:matrix.org
content:
just to confirm for M1 in accumulate do we process separate services in parallel or sequentially? i think i read somewhere that parallelizing services (in accumulate) will be a later version.

id:
1923
timestamp:
2025-04-11T09:58:04.507Z
sender:
@decentration:matrix.org
content:
* just to confirm for M1 in accumulate do we execute separate services in parallel or sequentially? i think i read somewhere that parallelizing services (in accumulate) will be a later version.

id:
1922
timestamp:
2025-04-11T09:58:28.295Z
sender:
@clearloop:matrix.org
content:
if I'm not mistaken, the current test vectors of accumulate for 0.6.4 doesn't have tests for PVM integration, e.g. you can complete the stf without the execution part

id:
1921
timestamp:
2025-04-11T09:58:58.987Z
sender:
@clearloop:matrix.org
content:
* if I'm not mistaken, the current test vectors of accumulate (0.6.4) doesn't have tests for PVM integration, e.g. you can complete the stf without the execution part

id:
1920
timestamp:
2025-04-11T10:00:47.001Z
sender:
@clearloop:matrix.org
content:
* if I'm not mistaken, the current test vectors of accumulate (0.6.4) doesn't have tests for PVM integration, e.g. you can complete the stf without the execution part

or ur talking about https://graypaper.fluffylabs.dev/#/68eaa1f/17b90017bb00?v=0.6.4 ? it's just a wrap

id:
1919
timestamp:
2025-04-11T10:01:15.920Z
sender:
@clearloop:matrix.org
content:
* if I'm not mistaken, the current test vectors of accumulate (0.6.4) doesn't have tests for PVM integration, e.g. you can complete the stf without the execution part

or ur talking about https://graypaper.fluffylabs.dev/#/68eaa1f/17b90017bb00?v=0.6.4 ?

id:
1918
timestamp:
2025-04-11T10:03:01.959Z
sender:
@clearloop:matrix.org
content:
* if I'm not mistaken ,we haven't had the test vectors for PVM integration in accumulate yet, mb ur talking about https://graypaper.fluffylabs.dev/#/68eaa1f/17b90017bb00?v=0.6.4 ?

id:
1917
timestamp:
2025-04-11T10:11:47.452Z
sender:
@decentration:matrix.org
content:
hmm, but on the jam website:

```
5. PVM instancing, execution and host-functions
```

im interpreting this as do the pvm invocation using current pvm library such as `jam-pvm-common` (in my case using an FFI)...

id:
1916
timestamp:
2025-04-11T10:13:21.312Z
sender:
@decentration:matrix.org
content:
yes re: sequential or parallel im talking about that part of the GP. where it says execute parallel, but i think i heard in a recent lecture that its currently sequential only... could be mistaken. 

id:
1915
timestamp:
2025-04-11T10:14:57.511Z
sender:
@decentration:matrix.org
content:
* hmm, but on the jam website:

```
5. PVM instancing, execution and host-functions
```

im interpreting this as do the pvm invocation using current pvm library such as `jam-pvm-common` (in my case using an FFI)... we dont have the vectors yet, but i imagine we should be expecting them to come for M1. 

id:
1914
timestamp:
2025-04-11T10:15:16.997Z
sender:
@xlchen:matrix.org
content:
m1 don’t have performance requirement so it is not possible to require parallel execution. it is not possible to tell if you generated the output in a single thread or 50

id:
1913
timestamp:
2025-04-11T10:17:01.985Z
sender:
@decentration:matrix.org
content:
ok makes sense

id:
1912
timestamp:
2025-04-11T11:45:49.592Z
sender:
@jan:parity.io
content:
If I don't get screwed over by the airlines I should be able to join the JAM Experience, so I could probably host a talk about writing recompilers during it if there's interest.

id:
1911
timestamp:
2025-04-11T14:02:37.479Z
sender:
@jimboj21:matrix.org
content:
This would be super rad I would love to have this happen

id:
1910
timestamp:
2025-04-11T14:22:07.104Z
sender:
@charliewinston14:matrix.org
content:
I hope that you can record this and share online (or publish live) as not all teams have the funding to fly to portugal. Some of us are on the other side of the world and would benefit from this as well.

id:
1909
timestamp:
2025-04-13T12:44:54.378Z
sender:
@sourabhniyogi:matrix.org
content:
Can a JAM equivalent of this be put together now?
https://spec.polkadot.network/sect-lightclient

id:
1908
timestamp:
2025-04-13T12:53:55.485Z
sender:
@sourabhniyogi:matrix.org
content:
* Can a JAM equivalent of this be put together now?
https://spec.polkadot.network/sect-lightclient

I believe this is valuable to set this in motion with jip-2 paving the way for storage value proofs and "warp" and not-warp syncing being a key thing to execute on with paths that jump over M3+M4.

id:
1907
timestamp:
2025-04-13T13:06:42.249Z
sender:
@sourabhniyogi:matrix.org
content:
* Can a JAM equivalent of this be put together now?
https://spec.polkadot.network/sect-lightclient

I believe this is valuable to set this in motion this Spring with jip-2 paving the way for storage value proofs and "warp" and not-warp syncing being a key thing to execute on with paths that jump over M3+M4.   It appears natural to chart light client implementation plans first without GRANDPA and then with.

id:
1906
timestamp:
2025-04-13T13:15:26.544Z
sender:
@sourabhniyogi:matrix.org
content:
* Can a JAM equivalent of this be put together now?
https://spec.polkadot.network/sect-lightclient

I believe this is valuable to set this in motion this Spring with jip-2 paving the way for storage value proofs and "warp" and not-warp syncing (ie CE128) being a key thing to execute on with paths that jump over M3+M4.   It appears natural to chart light client implementation plans first without GRANDPA and then with.

id:
1905
timestamp:
2025-04-13T13:37:16.599Z
sender:
@sourabhniyogi:matrix.org
content:
Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive "subscribeBlock(withJustifications=true)"  design for light client verification that returns all the  CDT storage value justifications needed to actually reexecute a JAM state transition (in a self-similar way to CoreChains (?)).  

Full node implementations responsive to light clients "subscribeBlock(withJustifications=true)" would track whichever storage values has been read from and written to (typically, most of C1-C15 and whatever service storage has been accumulated but also xyz others).  

The expectation is then that light clients verify all the CDT storage value justifications, then reexecute ordered accumulation (including PVM) just like full node do, and if they derive the same state root, the light client considers the individual JAM state transition valid.   What else is needed?

id:
1904
timestamp:
2025-04-13T13:37:44.675Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive "subscribeBlock(withJustifications=true)"  design for light client verification that returns all the  CDT storage value justifications needed to actually reexecute a JAM state transition (in a self-similar way to CoreChains (?)).  

Full node implementations responsive to light clients "subscribeBlock(withJustifications=true)" would track whichever storage values has been read from and written to (typically, most of C1-C15 and whatever service storage has been accumulated but also xyz other services).  

The expectation is then that light clients verify all the CDT storage value justifications, then reexecute ordered accumulation (including PVM) just like full node do, and if they derive the same state root, the light client considers the individual JAM state transition valid.   What else is needed?

id:
1903
timestamp:
2025-04-13T13:44:46.250Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive "subscribeBlock(withJustifications=true)"  design for light client verification that returns all the  CDT storage value justifications needed to actually reexecute a JAM state transition (in a self-similar way to CoreChains (?)).  

Full node implementations responsive to light clients "subscribeBlock(withJustifications=true)" would track whichever storage values has been read from and written to (typically, most of C1-C15 and whatever service storage has been accumulated but also xyz other services).  

The expectation is then that light clients verify all the CDT storage value justifications (including code hashing to codehashes), then reexecute ordered accumulation (including PVM using code/codehashes) just like full nodes do, and if they derive the same state root, the light client considers the individual JAM state transition valid.   What else is needed?

id:
1902
timestamp:
2025-04-13T13:48:47.025Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive jip-2 method "subscribeBlockWithJustifications"  explicitly designed for light client verification that returns all the  CDT storage value justifications needed to actually reexecute a JAM state transition (in a self-similar way to CoreChains (?)).  

Full node implementations responsive to light clients "subscribeBlock(withJustifications=true)" MUST track whichever storage values have been read from state: (a) C1-C15 (b) `read` (c) code in preimages.  

The expectation is then that light clients verify all the CDT storage value justifications (including code hashing to codehashes), then reexecute ordered accumulation (including PVM using code/codehashes) just like full nodes do, and if they derive the same state root, the light client considers the individual JAM state transition valid.   What else is needed?

id:
1901
timestamp:
2025-04-13T13:51:18.080Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive jip-2 method "subscribeBlockWithJustifications"  explicitly designed for light client verification that returns all the  CDT storage value justifications needed to actually reexecute a JAM state transition (in a self-similar way to CoreChains (?)).

Full node implementations responsive to light clients "subscribeBlock(withJustifications=true)" MUST track whichever storage values have been read from state: (a) pre-state C1-C15 (b) pre-state storage keys/values accessed via `read` host function calls and (c) code in preimages referenced in all ordered accumulation.

The expectation is then that light clients verify all the CDT storage value justifications (a)-(c), then reexecute ordered accumulation (including PVM using verified code of (c)) just like full nodes do, and if they derive the same state root, the light client considers the individual JAM state transition valid.   What else is needed?

id:
1900
timestamp:
2025-04-13T14:02:25.660Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive jip-2 method "subscribeBlockWithJustifications"  explicitly designed for light client verification that returns all the  CDT storage value justifications needed to actually reexecute a JAM state transition (in a self-similar way to CoreChains (?)).

Full node implementations responsive to light clients "subscribeBlock(withJustifications=true)" MUST track whichever storage values have been read from state and provide CDT proofs of: 
(a) pre-state C1-C15 that is actually read
(b) pre-state storage keys/values accessed via `read` host function calls and 
(c) code in preimages referenced in all ordered accumulation 

The expectation is then that light clients verify all the CDT storage value justifications (a)-(c), then reexecute ordered accumulation (including PVM using verified code of (c)) just like full nodes do, and if they derive the same state root, the light client considers the individual JAM state transition valid.   What else is needed?

id:
1899
timestamp:
2025-04-13T14:03:00.858Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive jip-2 method "subscribeBlockWithJustifications"  explicitly designed for light client verification that returns all the  CDT storage value justifications needed to actually reexecute a JAM state transition (in a self-similar way to CoreChains (?)).

Full node implementations responsive to light clients "subscribeBlock(withJustifications=true)" MUST track whichever storage values have been read from state and provide CDT proofs of:
(a) pre-state C1-C15 that is actually read
(b) pre-state storage keys/values accessed via `read` host function calls and
(c) code in preimages referenced in all ordered accumulation

The expectation is then that light clients verify all the CDT storage value justifications (a)-(c), then reexecute ordered accumulation (including PVM using verified code of (c)) just like full nodes do, and if they derive the same state root, the light client considers the individual JAM state transition valid.   

What else is needed?

id:
1898
timestamp:
2025-04-13T15:26:27.484Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive jip-2 method "subscribeBlock(withJustifications=true)"  explicitly designed for light client verification that returns all the  CDT storage value justifications needed to actually reexecute a JAM state transition (in a self-similar way to CoreChains (?)).

Full node implementations responsive to light clients "subscribeBlock(withJustifications=true)" MUST track whichever storage values have been read from state and provide CDT proofs of:
(a) pre-state C1-C15 that is actually read
(b) pre-state storage keys/values accessed via `read` host function calls and
(c) code in preimages referenced in all ordered accumulation

The expectation is then that light clients verify all the CDT storage value justifications (a)-(c), then reexecute ordered accumulation (including PVM using verified code of (c)) just like full nodes do, and if they derive the same state root, the light client considers the individual JAM state transition valid.

What else is needed?

id:
1897
timestamp:
2025-04-13T15:28:13.794Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive jip-2 method "subscribeBlock(withJustifications=true)"  explicitly designed for light client verification that returns all the  CDT storage value justifications needed to actually reexecute a JAM state transition (in a self-similar way to CoreChains (?)).

Full node implementations responsive to light clients "subscribeBlock(withJustifications=true)" MUST track whichever storage values have been read from state and provide CDT proofs of:
(a) pre-state C1-C15 that is actually read
(b) pre-state storage keys/values accessed via `read` host function calls and
(c) code in preimages referenced in all ordered accumulation

The expectation is then that light clients verify all the CDT storage value justifications (a)-(c), then reexecute ordered accumulation (including PVM using verified code of (c)) just like full nodes do, and if they derive the same state root, the light client considers the individual JAM state transition valid.

What else is needed?

All of this can be done without GRANDPA and warp syncing in one half of implementation, and then with.  Does that make sense?

id:
1896
timestamp:
2025-04-13T17:08:02.201Z
sender:
@sourabhniyogi:matrix.org
content:
* Can a JAM equivalent of this be put together now? https://spec.polkadot.network/sect-lightclient I believe this is valuable to set in motion this Spring with jip-2 paving the way for storage value proofs and "warp" and not-warp syncing (ie CE128) being a key thing to execute on with paths that jump over M3+M4. It appears natural to chart light client implementation plans first without GRANDPA and then with.

id:
1895
timestamp:
2025-04-13T17:46:10.374Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive jip-2 method "subscribeBlock(withJustifications=true)"  explicitly designed for light client verification that returns all the   storage value justifications needed to actually reexecute a JAM state transition (in a self-similar way to CoreChains (?)).

Full node implementations responsive to light clients "subscribeBlock(withJustifications=true)" MUST track whichever storage values have been read from state and provide  proofs of:
(a) pre-state C1-C15 that is actually read
(b) pre-state storage keys/values accessed via `read` host function calls and
(c) code in preimages referenced in all ordered accumulation

The expectation is then that light clients verify all the  storage value justifications (a)-(c), then reexecute ordered accumulation (including PVM using verified code of (c)) just like full nodes do, and if they derive the same state root, the light client considers the individual JAM state transition valid.

What else is needed?

All of this can be done without GRANDPA and warp syncing in one half of implementation, and then with.  Does that make sense?

id:
1894
timestamp:
2025-04-13T19:40:53.989Z
sender:
@danicuki:matrix.org
content:
> <@charliewinston14:matrix.org> I hope that you can record this and share online (or publish live) as not all teams have the funding to fly to portugal. Some of us are on the other side of the world and would benefit from this as well.

All content will be recorded and shared 

id:
1893
timestamp:
2025-04-13T20:29:50.646Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive jip-2 method "subscribeBlock(withJustifications=true)"  explicitly designed for light client verification that returns all the   storage value justifications needed to actually reexecute a JAM state transition (in a self-similar way to CoreChains (?)).  

Full node implementations responsive to light clients "subscribeBlock(withJustifications=true)" MUST track whichever storage values have been read from state and provide  proofs of:
(a) pre-state C1-C15 that is actually read
(b) pre-state storage keys/values accessed via `read` host function calls and
(c) code in preimages referenced in all ordered accumulation

The expectation is then that light clients verify all the  storage value justifications (a)-(c), then reexecute ordered accumulation (including PVM using verified code of (c)) just like full nodes do, and if they derive the same state root, the light client considers the individual JAM state transition valid.

Or, if we preserve "7.4.4 Remote Read Messages" JAM equivalent so that the light client just requests justifications for _some_ of the pre-state storage keys/values, then we would still want some parameter input of `subscribeBlock` enable returning _all_ the storage keys read in the state transition.

What else is needed?

All of this can be done without GRANDPA and warp syncing in one half of implementation, and then with.  Does that make sense?

id:
1892
timestamp:
2025-04-13T20:58:05.396Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive jip-2 method `subscribeBlock(withJustifications=true)` explicitly designed for light client verification that returns _all_ the storage value justifications needed to actually reexecute a JAM state transition.

Full node implementations responsive to light clients `subscribeBlock(withJustifications=true)` must track all storage values that are read from state trie and provide justificants of:
(a) pre-state C1-C15 that are actually read
(b) pre-state storage keys/values accessed via `read` host function calls and
(c) code in preimages referenced in all ordered accumulation

Light clients can verify all storage value justifications (a)-(c) (or some subset of them), then reexecute ordered accumulation (including PVM using verified code of (c)) just like full nodes do, and if they derive the same state root, the light client considers the individual JAM state transition valid.

To preserve "7.4.4 Remote Read Messages" JAM equivalent so that the light client just requests justifications for _some_ of the pre-state storage keys/values, presumably to not have a lot of bandwidth used up for justifications, then we would still want some parameter input of `subscribeBlock` enable returning _all_ the storage keys+values read in the state transition.

What else is needed?

All of this can be done without GRANDPA and warp syncing in one half of implementation, and then with.  Does that make sense?

id:
1891
timestamp:
2025-04-13T22:30:53.792Z
sender:
@dave:parity.io
content:
Maybe I'm not understanding but light clients shouldn't need to execute blocks or even need to fetch enough state to be able to do this. I understand you want to get something working without GRANDPA but that can be done by simply having the light client trust finality without verifying.

id:
1890
timestamp:
2025-04-13T22:35:57.672Z
sender:
@dave:parity.io
content:
Light clients will use the JAM network protocol (SNP or its successor). The RPC interface is really only intended to be used to connect to locally running nodes. This is for example to make it easy to write tools; instead of incorporating a node tools can just use the RPC interface of a standalone node. Long term it's possible that these tools will just incorporate a light client. In any case I think light client and RPC are probably mutually exclusive things.

id:
1889
timestamp:
2025-04-13T22:41:20.650Z
sender:
@dave:parity.io
content:
Of course SNP as it stands is not suitable for light clients. If you want something now you can make up your own extension to SNP, but bear in mind that this might not become the standard...

id:
1888
timestamp:
2025-04-13T22:46:00.964Z
sender:
@xlchen:matrix.org
content:
speaking of extensions, it will be great if SNP support some basic implementation name / version discovery so that we can have some feature discovery / telemetry ability

id:
1887
timestamp:
2025-04-13T22:53:38.821Z
sender:
@dave:parity.io
content:
You can use QUIC/TLS ALPN for this. Obviously that's not going to work very well if there are many extensions, don't think we really want to get to that point though

id:
1886
timestamp:
2025-04-13T22:56:06.851Z
sender:
@xlchen:matrix.org
content:
my main goal is know which implementation/version is the other side running. this can be helpful to for example workaround some bug in some legacy version or know where to report bugs if found out the other side is sending bad data

id:
1885
timestamp:
2025-04-14T00:13:41.500Z
sender:
@sourabhniyogi:matrix.org
content:
Hmm, for "shouldn't need to execute blocks or even need to fetch enough state to be able to do this" I think we should get on the same page about what a JAM light client should be.   I thought light clients re-execute the minimal state transition logic but **without having to store anything or process every state transition* but they still would need storage proofs, verify these storage proofs, and verify recent headers ( including the state root ), in order to be a *trustless* light client.  

Light clients:
- download **and verify** block headers to stay in sync with the chain’s head. This includes the state root, which requires doing ordered accumulation, including PVM.    
- verify that blocks have been finalized (which are GRANDPA finality proofs in both Polkadot+JAM).  I assume the last 28 days will be warp synced
- request storage proofs needed to validate a transition, which is/was the purpose of 7.4.4.  

If you have low-powered light clients they can choose to pick some *subset* of storage proofs (e.g. in a self-centered way, biased to just the services the light client cares about) for reduced security, but as you go from verifying all to verifying nothing, you go from "trustless" light client to "trust me bro, I'm infura" light client.   

CE129 could be adjusted to fill the role of 7.4.4 but as of this moment it doesn't return Justifications and is optimized for range queries.  Another CE could be created for light clients to get storage proofs, but there still needs to be a source of what storage keys have been read.

id:
1884
timestamp:
2025-04-14T00:16:17.406Z
sender:
@sourabhniyogi:matrix.org
content:
* Hmm, for "shouldn't need to execute blocks or even need to fetch enough state to be able to do this" I think we should get on the same page about what a JAM light client should be.   I thought light clients re-execute the minimal state transition logic but _without having to store anything or process every state transition_ -- this means they need to get storage proofs, verify these storage proofs, and verify recent headers ( including the state root which requires doing PVM accumulation, and ordered accumulation at that ), in order to be a _trustless_ light client.   There is surely some 'I'm actually only interested in *my* services' smarter approach to verifying the storage proofs, but to verify the state root it appears essential to do the PVM accumulation.

Light clients:

- download **and verify** block headers to stay in sync with the chain’s head. This includes the state root, which requires doing ordered accumulation, including PVM.
- verify that blocks have been finalized (which are GRANDPA finality proofs in both Polkadot+JAM).  I assume the last 28 days will be warp synced
- request storage proofs needed to validate a transition, which is/was the purpose of 7.4.4.

If you have low-powered light clients they can choose to pick some _subset_ of storage proofs (e.g. in a self-centered way, biased to just the services the light client cares about) for reduced security, but as you go from verifying all to verifying nothing, you go from "trustless" light client to "trust me bro, I'm infura" light client.

CE129 could be adjusted to fill the role of 7.4.4 but as of this moment it doesn't return Justifications and is optimized for range queries.  Another CE could be created for light clients to get storage proofs, but there still needs to be a source of what storage keys have been read.

id:
1883
timestamp:
2025-04-14T00:55:03.673Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive jip-2 method `subscribeBlock(withJustifications=true)` explicitly designed for light client verification that returns _all_ the storage value justifications needed to actually reexecute a JAM state transition.

Full node implementations responsive to light clients `subscribeBlock(withJustifications=true)` must track all storage values that are read from state trie and provide justificants of:
(a) pre-state C1-C15 that are actually read
(b) pre-state storage keys/values (including preimages + lookup) accessed via  host function calls: `read`, `lookup`, `solicit`, `forget`, `info`, `eject`, `query`, `transfer`, ...
(c) code in preimages referenced in all ordered accumulation

Light clients can verify all storage value justifications (a)-(c) (or some subset of them), then reexecute ordered accumulation (including PVM using verified code of (c)) just like full nodes do, and if they derive the same state root, the light client considers the individual JAM state transition valid.

To preserve "7.4.4 Remote Read Messages" JAM equivalent so that the light client just requests justifications for _some_ of the pre-state storage keys/values, presumably to not have a lot of bandwidth used up for justifications, then we would still want some parameter input of `subscribeBlock` enable returning _all_ the storage keys+values read in the state transition.

What else is needed?

All of this can be done without GRANDPA and warp syncing in one half of implementation, and then with.  Does that make sense?

id:
1882
timestamp:
2025-04-14T00:58:02.366Z
sender:
@sourabhniyogi:matrix.org
content:
* Hmm, for "shouldn't need to execute blocks or even need to fetch enough state to be able to do this" I think we should get on the same page about what a JAM light client should be.   I thought light clients re-execute the minimal state transition logic but _without having to store anything or process every state transition_ -- this means they need to get storage proofs, verify these storage proofs, and verify recent headers ( including the state root which requires doing PVM accumulation, and ordered accumulation at that ), in order to be a _trustless_ light client.   There is surely some 'I'm actually only interested in _my_ services' smarter approach to verifying the storage proofs, but to verify the state root it appears essential to do the PVM accumulation.

Light clients:

- download **and verify** block headers to stay in sync with the chain’s head. This includes the state root, which requires doing ordered accumulation, including PVM.
- verify that blocks have been finalized (which are GRANDPA finality proofs in both Polkadot+JAM).  I assume the last 28 days will be warp synced
- request storage proofs needed to validate a transition, which is/was the purpose of 7.4.4.

If you have low-powered light clients they can choose to pick some _subset_ of storage proofs (e.g. in a self-centered way, biased to just the services the light client cares about) for reduced security, but as you go from verifying all to verifying nothing, you go from "trustless" light client to "trust me bro, I'm infura" light client.

CE129 could be adjusted to fill the role of 7.4.4 but as of this moment it doesn't return Justifications and is optimized for range queries.  Another CE could be created for light clients to get storage proofs, but there still needs to be a source of which state trie keys have been read.

id:
1881
timestamp:
2025-04-14T01:14:06.987Z
sender:
@sourabhniyogi:matrix.org
content:
* Instead of adapting "7.4.4 Remote Read Messages", I suggest a more proactive jip-2 method `subscribeBlock(withJustifications=true)` explicitly designed for light client verification that returns _all_ the storage value justifications needed to actually reexecute a JAM state transition.

Full node implementations responsive to light clients `subscribeBlock(withJustifications=true)` must track all storage values that are read from state trie and provide justifications of:
(a) pre-state C1-C15 that are actually read
(b) pre-state storage keys/values (including preimages + lookup) accessed via  host function calls: `read`, `lookup`, `solicit`, `forget`, `info`, `eject`, `query`, `transfer`, ...
(c) code in preimages referenced in all ordered accumulation

Light clients can verify all storage value justifications (a)-(c) (or some subset of them), then reexecute ordered accumulation (including PVM using verified code of (c)) just like full nodes do, and if they derive the same state root, the light client considers the individual JAM state transition valid.

To preserve "7.4.4 Remote Read Messages" JAM equivalent so that the light client just requests justifications for _some_ of the pre-state storage keys/values, presumably to not have a lot of bandwidth used up for justifications, then we would still want some parameter input of `subscribeBlock` enable returning _all_ the storage keys+values read in the state transition.

What else is needed?

All of this can be done without GRANDPA and warp syncing in one half of implementation, and then with.  Does that make sense?

id:
1880
timestamp:
2025-04-14T07:47:48.552Z
sender:
@gav:polkadot.io
content:
Then ALPN should be fine, no?

id:
1879
timestamp:
2025-04-14T07:48:27.419Z
sender:
@gav:polkadot.io
content:
No.

id:
1878
timestamp:
2025-04-14T07:48:47.535Z
sender:
@gav:polkadot.io
content:
Light clients sync to the most recent finalised block through a GRANDPA proof.

id:
1877
timestamp:
2025-04-14T07:49:33.740Z
sender:
@gav:polkadot.io
content:
They do no re-execution of on-chain logic. Theoretically they could but it's super-cumbersome and it is not generally what is meant by "light-client".

id:
1876
timestamp:
2025-04-14T07:50:00.534Z
sender:
@xlchen:matrix.org
content:
unless I missed something, ALPN is for protocol version, how can we use it to discovery the implementation version of peers?

id:
1875
timestamp:
2025-04-14T07:50:09.702Z
sender:
@gav:polkadot.io
content:
Instead, they accept proofs of on-chain state through the (prior) state-root found in the header.

id:
1873
timestamp:
2025-04-14T07:50:18.696Z
sender:
@gav:polkadot.io
content:
They trust this because of the GRANDPA finality proof.

id:
1874
timestamp:
2025-04-14T07:50:20.511Z
sender:
@xlchen:matrix.org
content:
* unless I missed something, ALPN is for protocol version negotiation, how can we use it to discovery the implementation version of peers?

id:
1872
timestamp:
2025-04-14T07:50:30.697Z
sender:
@gav:polkadot.io
content:
* They trust this header/state-root because of the GRANDPA finality proof.

id:
1871
timestamp:
2025-04-14T07:51:03.412Z
sender:
@gav:polkadot.io
content:
> as you go from verifying all to verifying nothing, you go from "trustless" light client to "trust me bro, I'm infura" light client

No.

id:
1870
timestamp:
2025-04-14T07:51:25.218Z
sender:
@gav:polkadot.io
content:
You've totally missed the whole point of finality.

id:
1869
timestamp:
2025-04-14T07:51:39.599Z
sender:
@gav:polkadot.io
content:
* You've totally missed the whole point of JAM/Polkadot'a finality.

id:
1868
timestamp:
2025-04-14T07:51:41.801Z
sender:
@gav:polkadot.io
content:
* You've totally missed the whole point of JAM/Polkadot's finality.

id:
1867
timestamp:
2025-04-14T07:53:37.872Z
sender:
@gav:polkadot.io
content:
The point of finality is not merely to avoid reversion. It is also to place economic weight behind *correctness*. With our assumption of no more than 1/3 malicious nodes, GRANDPA will only finalize valid transitions, *including* in-core logic.

id:
1866
timestamp:
2025-04-14T07:53:43.095Z
sender:
@gav:polkadot.io
content:
* The point of finality is not merely to avoid reversion. It is also to place economic weight behind _correctness_. With our assumption of no more than 1/3 malicious nodes, GRANDPA will only finalize valid transitions, _including in-core logic_.

id:
1865
timestamp:
2025-04-14T07:54:49.079Z
sender:
@gav:polkadot.io
content:
This means that trusting in GRANDPA gets a similar degree of security to trusting "JAM" more generally.

id:
1864
timestamp:
2025-04-14T07:56:33.945Z
sender:
@gav:polkadot.io
content:
Now, of course, this is not (quite) as high a confidence level as actually executing everything yourself from first-principles. But then this isn't practically viable anyway on a system as scalable as JAM/Polkadot as the in-core computation has extrinsic data which is not made indefinitely available (e.g. parachain transactions in Polkadot's case).

id:
1863
timestamp:
2025-04-14T07:57:22.799Z
sender:
@gav:polkadot.io
content:
But this idea of (re-)executing everything in order to gain confidence in correctness is a Bitcoin mind virus.

id:
1862
timestamp:
2025-04-14T07:58:45.662Z
sender:
@gav:polkadot.io
content:
It's not true. The Elves paper is an example of how it's possible to argue for correctness from first principles whilst not presuming (indefinite) ability to re-execute. zk-SNARKS offer another means for doing this.

id:
1861
timestamp:
2025-04-14T07:59:00.389Z
sender:
@gav:polkadot.io
content:
* Now, of course, this is not (quite) as high a confidence level as actually executing everything yourself. But then this isn't practically viable anyway on a system as scalable as JAM/Polkadot as the in-core computation has extrinsic data which is not made indefinitely available (e.g. parachain transactions in Polkadot's case).

id:
1860
timestamp:
2025-04-14T08:00:41.250Z
sender:
@gav:polkadot.io
content:
In short, when GRANDPA finalises some state and you have a state-proof for some storage value against it, you can assume one of three things:
1. Elves/JAM/Polkadot has a design flaw nobody knows about.
2. More than 1/3 of validators are misbehaving.
3. It's correct.

id:
1859
timestamp:
2025-04-14T08:01:29.188Z
sender:
@gav:polkadot.io
content:
It's certainly categorically better than your characterisation of "trust me bro, I'm infura"

id:
1858
timestamp:
2025-04-14T08:01:30.577Z
sender:
@gav:polkadot.io
content:
* It's certainly categorically better than your characterisation of "trust me bro, I'm infura".

id:
1857
timestamp:
2025-04-14T08:02:30.358Z
sender:
@gav:polkadot.io
content:
I think you can have a fairly arbitrary ID sequence, can't you?

id:
1856
timestamp:
2025-04-14T08:02:43.599Z
sender:
@gav:polkadot.io
content:
https://github.com/quicwg/base-drafts/wiki/ALPN-IDs-used-with-QUIC

id:
1855
timestamp:
2025-04-14T08:02:59.129Z
sender:
@gav:polkadot.io
content:
E.g. `http/2+quic/NN`.

id:
1854
timestamp:
2025-04-14T08:03:15.535Z
sender:
@gav:polkadot.io
content:
* E.g. `http/2+quic/NN` allows specification of two protocols and their versions.

id:
1853
timestamp:
2025-04-14T08:04:47.251Z
sender:
@gav:polkadot.io
content:
We currently use `jamnp-s/V/H`

id:
1852
timestamp:
2025-04-14T08:04:59.638Z
sender:
@gav:polkadot.io
content:
* We currently use `jamnp-s/V/H`, but we could extend this to include the impl ID and version?

id:
1851
timestamp:
2025-04-14T08:05:05.798Z
sender:
@xlchen:matrix.org
content:
well, I am using `msquic`, and it requires a known list of ALPN https://github.com/microsoft/msquic/blob/e2a994c9dc91240147c9d079436d4e6daba75e79/docs/api/ListenerStart.md#parameters

id:
1850
timestamp:
2025-04-14T08:05:20.380Z
sender:
@gav:polkadot.io
content:
* We currently use `jamnp-s/V/H`, (`V` is protocol version and `H` is genesis hash) but we could extend this to include the impl ID and version?

id:
1849
timestamp:
2025-04-14T08:05:53.127Z
sender:
@gav:polkadot.io
content:
Sounds like that's not in line with JAM-SNP

id:
1848
timestamp:
2025-04-14T08:05:54.561Z
sender:
@gav:polkadot.io
content:
https://github.com/zdave-parity/jam-np/blob/main/simple.md

id:
1847
timestamp:
2025-04-14T08:05:58.883Z
sender:
@xlchen:matrix.org
content:
I can fork it and do the necessary changes, but that's something I want to avoid if possible

id:
1846
timestamp:
2025-04-14T08:06:18.598Z
sender:
@dave:parity.io
content:
If you want an impl ID/name ALPN is not suitable

id:
1845
timestamp:
2025-04-14T08:06:59.640Z
sender:
@dave:parity.io
content:
We can add some protocol to request this if necessary

id:
1844
timestamp:
2025-04-14T08:07:08.003Z
sender:
@gav:polkadot.io
content:
Ahh I see it's a pattern match thing.

id:
1843
timestamp:
2025-04-14T08:08:51.137Z
sender:
@gav:polkadot.io
content:
* Now, of course, this is not (quite) as high a confidence level as actually executing everything yourself. But then this isn't practically viable anyway on a system as scalable as JAM/Polkadot firstly as the in-core computation has extrinsic data which is not made indefinitely available (e.g. parachain transactions in Polkadot's case); and secondly you'd need near Toaster-level hardware to re-execute all of the work-packages going through every core.

id:
1842
timestamp:
2025-04-14T08:09:38.706Z
sender:
@xlchen:matrix.org
content:
yeah the purpose of ALPN is for protocol version negotiation. I don't think add impl name/version is the right choice

id:
1841
timestamp:
2025-04-14T08:10:46.427Z
sender:
@gav:polkadot.io
content:
Yeah then we can try to introduce this to JAM SNP.

id:
1840
timestamp:
2025-04-14T08:11:24.892Z
sender:
@gav:polkadot.io
content:
[JIP-2](https://hackmd.io/@polkadot/jip2) is updated with a new `parameters` RPC.

id:
1839
timestamp:
2025-04-14T08:11:51.332Z
sender:
@gav:polkadot.io
content:
This is intended to allow RPC clients to understand the parameters of the node's chain.

id:
1838
timestamp:
2025-04-14T08:12:05.831Z
sender:
@gav:polkadot.io
content:
* This is intended to allow RPC clients to understand the parameters of the node's chain including number of cores and validators.

id:
1837
timestamp:
2025-04-14T08:31:04.014Z
sender:
@xlchen:matrix.org
content:
> <@gav:polkadot.io> I updated my (unofficial) Prize notes: https://hackmd.io/@polkadot/jamprize

can I confirm that generative AI is ok (or not) for:
write documents
write tests
code review
help reading GP
doing research (e.g. how to write a recompiler)
write code that's not covered by GP (e.g. RPC or CLI handling). In theory, those code should not be judged?

id:
1836
timestamp:
2025-04-14T08:32:50.843Z
sender:
@gav:polkadot.io
content:
Most should be fine, yes. The only slight concern would be for GP interpretation.

id:
1835
timestamp:
2025-04-14T08:33:55.028Z
sender:
@gav:polkadot.io
content:
Again, it's generally best to treat genAI as an indirect (and lossy) comms system.

id:
1834
timestamp:
2025-04-14T08:35:10.507Z
sender:
@gav:polkadot.io
content:
And comms concerning GP interpretation is generally best done here to maximise the chances of clean-room impl and minimise the chances of sub-dialects (inadvertently) forming.

id:
1833
timestamp:
2025-04-14T08:35:29.344Z
sender:
@gav:polkadot.io
content:
* And comms concerning GP interpretation is generally best done in rooms visible to all to maximise the chances of clean-room impl and minimise the chances of sub-dialects (inadvertently) forming.

id:
1832
timestamp:
2025-04-14T08:35:38.293Z
sender:
@gav:polkadot.io
content:
* And comms concerning GP interpretation is generally best done in easily-discoverable forums visible to all to maximise the chances of clean-room impl and minimise the chances of sub-dialects (inadvertently) forming.

id:
1831
timestamp:
2025-04-14T08:37:18.292Z
sender:
@gav:polkadot.io
content:
It's written in both English and formal logic in order to minimise the chance of linguistic issues for non-native speakers. In any case I fear genAI has less of a chance of knowing what I intent the GP to mean than you.

id:
1830
timestamp:
2025-04-14T08:37:23.789Z
sender:
@gav:polkadot.io
content:
* It's written in both English and formal logic in order to minimise the chance of linguistic issues for non-native speakers. In any case I fear genAI has less of a chance of knowing what I intend the GP to mean than you would.

id:
1829
timestamp:
2025-04-14T08:37:53.409Z
sender:
@gav:polkadot.io
content:
* It's written in both English and formal logic in order to minimise the chance of linguistic issues for non-native speakers; I would not expect genAI to help much here and it may easily introduce translation artefacts. In any case, I fear genAI has less of a chance of knowing what I intend the GP to mean than you would.

id:
1828
timestamp:
2025-04-14T08:57:30.414Z
sender:
@xlchen:matrix.org
content:
is there any disassembler works with output of `jam-pvm-build`? I tried `polkatool disassemble` and it doesn't work

id:
1827
timestamp:
2025-04-14T08:58:56.485Z
sender:
@xlchen:matrix.org
content:
ok tried this and looking good so far https://pvm.fluffylabs.dev/?#/

id:
1826
timestamp:
2025-04-14T09:01:09.380Z
sender:
@jan:parity.io
content:
Yes, that currently doesn't work. It's just a matter of making a JAM blob -> `.polkavm` blob converter, which is on my TODO list (but PRs are welcome; it should be very simple to do, just do the reverse of what the `jam-pvm-build` does, since it converts a `.polkavm` blob into a JAM blob)

id:
1825
timestamp:
2025-04-14T09:15:40.905Z
sender:
@sourabhniyogi:matrix.org
content:
After JAM light clients sync to the most recent finalized block through a GRANDPA proof, and receive a set of unfinalized blocks, what verification does the light client do when there is no GRANDPA proof, if anything at all?  It appears that JAM clients only will be verifying GRANDPA proofs, and would trust the node that they receive their unfinalized block.  

What was 7.4.4 for?  

id:
1824
timestamp:
2025-04-14T09:16:43.085Z
sender:
@sourabhniyogi:matrix.org
content:
* After JAM light clients sync to the most recent finalized block through a GRANDPA proof, and receive a set of unfinalized blocks, what verification does the light client do when there is no GRANDPA proof for those unfinalized blocks, if anything at all?  It appears that JAM light clients only will be verifying GRANDPA proofs, and would trust the node that they receive their unfinalized block.  

What was 7.4.4 for?  

id:
1823
timestamp:
2025-04-14T09:18:29.351Z
sender:
@gav:polkadot.io
content:
> if anything at all?

None.

id:
1822
timestamp:
2025-04-14T09:19:04.406Z
sender:
@gav:polkadot.io
content:
Light-clients are not intended to use unfinalized blocks.

id:
1821
timestamp:
2025-04-14T09:19:52.273Z
sender:
@gav:polkadot.io
content:
User interfaces might interpret them and display them with some sort of "unconfirmed" annotation if there's any serious economic implication of them.

id:
1820
timestamp:
2025-04-14T09:19:59.248Z
sender:
@sourabhniyogi:matrix.org
content:
Ok thank you for clarifying!  Indeed, I have had a Bitcoin mind virus =)

id:
1819
timestamp:
2025-04-14T09:20:07.579Z
sender:
@gav:polkadot.io
content:
But light-clients generally have no means of knowing.

id:
1818
timestamp:
2025-04-14T09:20:14.907Z
sender:
@gav:polkadot.io
content:
Even re-executing the block doesn't help here.

id:
1817
timestamp:
2025-04-14T09:21:00.785Z
sender:
@gav:polkadot.io
content:
* Even re-executing the block doesn't help here - that's the Bitcoin mind virus. You'd have to not only re-execute the block but also re-execute all the cores (at least those that have causal-implications on the service storage you care about).

id:
1816
timestamp:
2025-04-14T09:21:31.224Z
sender:
@gav:polkadot.io
content:
* Even re-executing the block doesn't help here - that's the Bitcoin mind virus. You'd have to not only re-execute the block but also re-execute all the cores (at least those that have causal-implications on the service storage you care about, which given the ability for transfers to happen in a block means, theoretically, all of them which can send messages to your service).

id:
1815
timestamp:
2025-04-14T09:22:16.659Z
sender:
@gav:polkadot.io
content:
To re-execute the cores, you'd need to fetch all of the DA data they require and this is obviously waaaay too much to be doing for a light-client. It'd take > 6 seconds and then you may as well wait for Grandpa.

id:
1814
timestamp:
2025-04-14T09:22:20.690Z
sender:
@gav:polkadot.io
content:
* To re-execute the cores, you'd need to fetch all of the DA data they require and this is obviously waaaay too much to be doing for a light-client. It'd take > 6 seconds and then you may as well wait for GRANDPA.

id:
1813
timestamp:
2025-04-14T09:23:02.878Z
sender:
@gav:polkadot.io
content:
* To re-execute the cores, you'd need to fetch all of the DA data they require and this is obviously waaaay too much to be doing for a *light*-client. It'd take > 6 seconds and then you may as well wait for GRANDPA.

id:
1812
timestamp:
2025-04-14T09:23:37.249Z
sender:
@gav:polkadot.io
content:
So yeah, there's really no way around this, and that's by design. We want to off-load correctness entirely into the game-theoretic machinery. Not have it done by clients on the edges.

id:
1811
timestamp:
2025-04-14T09:23:53.357Z
sender:
@gav:polkadot.io
content:
* So yeah, there's really no way around this, and that's by design. We want to off-load correctness entirely into the game-theoretic (staked) machinery. Not have it done by clients on the edges.

id:
1810
timestamp:
2025-04-14T09:24:00.946Z
sender:
@gav:polkadot.io
content:
* So yeah, there's really no way around this, and that's by design. We want to off-load correctness entirely into the game-theoretic (staked) machinery. Not have it done by clients on the edges, light or otherwise.

id:
1809
timestamp:
2025-04-14T09:24:27.390Z
sender:
@gav:polkadot.io
content:
Only this way can we expect to be able to scale to the levels we need.

id:
1808
timestamp:
2025-04-14T09:24:37.337Z
sender:
@gav:polkadot.io
content:
* Only this way can we expect to be able to scale to the levels we need (and still not be centralised).

id:
1807
timestamp:
2025-04-14T09:27:40.962Z
sender:
@gav:polkadot.io
content:
One thing that we can almost certainly do, though not necessarily pre-1.0, is have GRANDPA finalize not only the block but also its post-state-root.

id:
1806
timestamp:
2025-04-14T09:27:46.629Z
sender:
@gav:polkadot.io
content:
* One thing that we can almost certainly do, though not necessarily pre-1.0, is have GRANDPA finalize not only the block but also its posterior state-root.

id:
1805
timestamp:
2025-04-14T09:28:51.286Z
sender:
@gav:polkadot.io
content:
* One thing that we can almost certainly do, though not necessarily pre-1.0, is have GRANDPA finalize not only the block but also its posterior state-root. Alistair is also considering introducing some other lightweight consensus over blocks produced, but not necessarily correct (i.e. not yet having passed Elves).

id:
1804
timestamp:
2025-04-14T09:29:42.088Z
sender:
@gav:polkadot.io
content:
* One thing that we can almost certainly do, though not necessarily pre-1.0, is have GRANDPA finalize not only the block but also its posterior state-root. This should be a pretty trivial addition and would mean light-clients are able to get proofs over storage one block earlier than otherwise. Alistair is also considering introducing some other lightweight consensus over blocks produced, but not necessarily correct (i.e. not yet having passed Elves).

id:
1803
timestamp:
2025-04-14T09:30:56.234Z
sender:
@gav:polkadot.io
content:
* One thing that we can almost certainly do, though not necessarily pre-1.0, is have GRANDPA finalize not only the block but also its posterior state-root. This should be a pretty trivial addition and would mean light-clients are able to get proofs over storage one block earlier than otherwise. Alistair is also considering introducing some other lightweight consensus over blocks produced, but not necessarily correct (i.e. not yet having passed Elves). This wouldn't help all that much for applications which have high economic implications but could help lower apparent UI latency for lesser applications.

id:
1802
timestamp:
2025-04-14T09:33:52.694Z
sender:
@gav:polkadot.io
content:
* One thing that we can almost certainly do, though not necessarily pre-1.0, is have GRANDPA finalize not only the block but also its posterior state-root. This should be a pretty trivial addition and would mean light-clients are able to get proofs over storage one block earlier than otherwise. Alistair is also considering introducing some other lightweight consensus over blocks produced, but not necessarily correct (i.e. not yet having passed Elves). This wouldn't help all that much for applications which have high economic implications but could help lower apparent UI latency for lesser applications especially if the finalised item included posterior state-root too.

id:
1801
timestamp:
2025-04-14T09:34:07.684Z
sender:
@gav:polkadot.io
content:
* One thing that we can almost certainly do, though not necessarily pre-1.0, is have GRANDPA finalize not only the block but also its posterior state-root. This should be a pretty trivial addition and would mean light-clients are able to get proofs over storage one block earlier than otherwise. Alistair is also considering introducing some other lightweight consensus over blocks produced, but not necessarily correct (i.e. not yet having passed Elves). This wouldn't help all that much for applications which have high economic implications but could help lower apparent UI latency for lesser applications especially if the finalised information included posterior state-root rather than just the header-hash.

id:
1800
timestamp:
2025-04-14T10:34:39.723Z
sender:
@sourabhniyogi:matrix.org
content:
Would you expect JAM light nodes going through [the light node path](https://hackmd.io/@polkadot/jamprize#Light-node) to verify this new (not necessarily ELVES-correct) lightweight "GRANDPA 2.0" consensus at the bleeding edge?  

The "best vs finalized" bits of in jip-2 is causing us to want to fill in the GRANDPA related pieces of JAMNP (which has no vote/commit/... GRANDPA yet so we made some up earlier) and go from little syncs to warp syncs.  Having the GRANDPA 1.0 proof and 2.0 path would be terrific as we are happily auditing WPs now and can connect the dots to "best block" now.

id:
1799
timestamp:
2025-04-14T10:47:39.938Z
sender:
@gav:polkadot.io
content:
> Would you expect JAM light nodes going through the light node path to verify this new (not necessarily ELVES-correct) lightweight "GRANDPA 2.0" consensus at the bleeding edge?

Possibly, assuming it is in the GP. But given it's presently little more than an idea, it's hard to say for sure.

id:
1798
timestamp:
2025-04-14T10:47:50.686Z
sender:
@gav:polkadot.io
content:
* > Would you expect JAM light nodes going through the light node path to verify this new (not necessarily ELVES-correct) lightweight "GRANDPA 2.0" consensus at the bleeding edge?

Possibly, assuming it is in the GP. But given it's presently little more than an idea, it's hard to say for sure if it would be in the GP by 1.0.

id:
1797
timestamp:
2025-04-14T10:49:32.129Z
sender:
@gav:polkadot.io
content:
> The "best vs finalized" bits of in jip-2 is causing us to want to fill in the GRANDPA related pieces of JAMNP

Sure. Already there's the GRANDPA paper if you want to begin implementing the internals. I expect we'll be able to introduce the protocol elements in the near future.

id:
1796
timestamp:
2025-04-14T10:49:56.565Z
sender:
@gav:polkadot.io
content:
It's not yet amalgamated in to the GP but there's not all that much to do on that front as GRANDPA is quite self-contained.

id:
1795
timestamp:
2025-04-14T10:51:36.730Z
sender:
@gav:polkadot.io
content:
* > The "best vs finalized" bits of in jip-2 is causing us to want to fill in the GRANDPA related pieces of JAMNP

Sure. Already there's the [GRANDPA paper](https://research.web3.foundation/Polkadot/protocols/finality) if you want to begin implementing the internals. I expect we'll be able to introduce the protocol elements in the near future.

id:
1794
timestamp:
2025-04-14T14:33:21.398Z
sender:
@sourabhniyogi:matrix.org
content:
* Would you expect JAM light nodes going through [the light node path](https://hackmd.io/@polkadot/jamprize#Light-node) to verify this new (not necessarily ELVES-correct) lightweight "GRANDPA 2.0" consensus at the bleeding edge?

The "best vs finalized" bits in jip-2 is causing us to want to fill in the GRANDPA related pieces of JAMNP (which has no vote/commit/... GRANDPA yet so we made some up earlier) and go from little syncs to warp syncs.  Having the GRANDPA 1.0 proof and 2.0 path would be terrific as we are happily auditing WPs now and can connect the dots to "best block" now.

id:
1793
timestamp:
2025-04-14T15:35:22.096Z
sender:
@sourabhniyogi:matrix.org
content:
Ok!  So that we may plan our GRANDPA execution, which milestones [there are now 5+5(+1)+3=14] require { GRANDPA finality, GRANDPA proof verification, warp syncing }?

Can you speak to how we may instrument our implementations to measure and report on required performance at the Lisbon meetup for all 3 paths?

id:
1792
timestamp:
2025-04-14T15:36:27.266Z
sender:
@gav:polkadot.io
content:
Yeah I’ll try to state it as well as possible. But for a basic idea it’ll be in line with smoldot. 

id:
695
timestamp:
2025-04-14T17:06:01.875Z
sender:
@charliewinston14:matrix.org
content:
Maybe I missed it but is there anything in the GP that mentions how to validate a justification received by CE 137?? I can generate one using the trace function but am not sure how a receiving node verifys the shard is correct using it.

id:
694
timestamp:
2025-04-14T17:12:36.321Z
sender:
@dave:parity.io
content:
In general the GP doesn't say how to implement anything, it just defines the required behaviour. The justification is a Merkle proof and can be verified in the usual way. There are lots of blogposts explaining the concept that you can find by googling "merkle proof"

id:
1791
timestamp:
2025-04-14T18:29:32.378Z
sender:
@p1sar:matrix.org
content:
Hello everyone!

This month’s JAM Implementers Call is happening a bit later than usual due to PBA — but we're back! 🎉
As always, this call is a space to foster knowledge sharing and collaboration among JAM developers.

📝 General notes & purpose of the call:
https://hackmd.io/@P1sarb/HJkLHpIwyl

📅 Agenda & invite link for this week’s call:
https://hackmd.io/@P1sarb/rkcn_pc0kl
Feel free to suggest update to the agenda directly on the page (you’ll need to be signed in 😉).

📍 Join us on Tuesday, April 15 at:
{7:30am PT, 8:30am Denver, 10:30am NY, 3:30pm Lisbon, 4:30pm Berlin, 8:00pm India ST, 10:30pm Beijing, 11:30pm Tokyo, 2:30am New Zealand (Apr 16)}

Everyone is welcome—see you there!

PS: All calls are recorded and uploaded to YouTube!

id:
1790
timestamp:
2025-04-15T00:37:23.838Z
sender:
@clearloop:matrix.org
content:
may I ask when is the Lisbon meetup? where can I find more information about the upcoming events of Jam?

id:
1789
timestamp:
2025-04-15T00:40:48.466Z
sender:
@sourabhniyogi:matrix.org
content:
https://lu.ma/ob0n7pdy

id:
1788
timestamp:
2025-04-15T14:24:47.935Z
sender:
@jay_ztc:matrix.org
content:
can someone share the call link for this months session? Thanks!

id:
1787
timestamp:
2025-04-15T14:26:15.969Z
sender:
@p1sar:matrix.org
content:
The JAM Implementers Call is starting in 5 minutes!
https://meet.google.com/ttn-tbct-ibi — Even if you don’t have anything specific to discuss, come by and say hi!

id:
693
timestamp:
2025-04-15T14:28:08.619Z
sender:
@charliewinston14:matrix.org
content:
Ok that helped I have the general idea now. One question to everyone, given each value in a justification how do you know if they represent a left or right node to be be able to calculate the right hash?

id:
1786
timestamp:
2025-04-15T14:36:33.966Z
sender:
@sourabhniyogi:matrix.org
content:
https://meet.google.com/rbb-cxhi-idp?authuser=0

id:
1785
timestamp:
2025-04-15T14:37:41.507Z
sender:
@p1sar:matrix.org
content:
* The JAM Implementers Call is starting in 5 minutes!
https://meet.google.com/rbb-cxhi-idp — Even if you don’t have anything specific to discuss, come by and say hi!

id:
692
timestamp:
2025-04-15T15:13:55.088Z
sender:
@greywolve:matrix.org
content:
I think some variant of this question has been asked but I didn't see a clear answer.

Assuming I want to submit a ticket extrinsic that will be included in the first block of a new epoch. At that point I have the last state of the previous epoch. 

1) Do I use eta_1 from the old state for the ring signature context? Given this will become eta_2 in the next epoch when the ticket will be verified.

2) What about the y_z, I only have access to the only y_z. I don't see a way to compute what the new one yet. Do I just sign with the old y_z and hope it doesn't change?

id:
691
timestamp:
2025-04-15T15:14:26.584Z
sender:
@greywolve:matrix.org
content:
* I think some variant of this question has been asked but I didn't see a clear answer.

Assuming I want to submit a ticket extrinsic that will be included in the first block of a new epoch. At that point I have the last state of the previous epoch.

1. Do I use eta\_1 from the old state for the ring signature context? Given this will become eta\_2 in the next epoch when the ticket will be verified.
2. What about the y\_z, I only have access to the old y\_z. I don't see a way to compute what the new one yet. Do I just sign with the old y\_z and hope it doesn't change?

id:
690
timestamp:
2025-04-15T15:15:18.845Z
sender:
@dave:parity.io
content:
You can determine the "path" (series of lefts/rights) from the shard index

id:
689
timestamp:
2025-04-15T15:17:49.727Z
sender:
@greywolve:matrix.org
content:
* I think some variant of this question has been asked but I didn't see a clear answer.

Assuming I want to submit a ticket extrinsic that will be included in the first block of a new epoch. At that point I have the last state of the previous epoch.

1. Do I use eta\_1 from the old state for the ring signature context? Given this will become eta\_2 in the next epoch when the ticket will be verified.
2. What about the y\_z, I only have access to the old y\_z. I don't see a way to compute what the new one yet. Do I just sign with the old y\_z and hope it doesn't change?

3. When importing this first block in the epoch, would we use the prior y_z or new y_z to verify tickets?

id:
688
timestamp:
2025-04-15T15:29:00.411Z
sender:
@greywolve:matrix.org
content:
* I think some variant of this question has been asked but I didn't see a clear answer.

Assuming I want to submit a ticket extrinsic that will be included in the first block of a new epoch. At that point I have the last state of the previous epoch.

1. Do I use eta\_1 from the old state for the ring signature context? Given this will become eta\_2 in the next epoch when the ticket will be verified.
2. What about the y\_z, I only have access to the old y\_z. I don't see a way to compute the new one yet. Do I just sign with the old y\_z and hope it doesn't change?
3. When importing this first block in the epoch, would we use the prior y\_z or new y\_z to verify tickets?

id:
687
timestamp:
2025-04-15T15:29:20.594Z
sender:
@greywolve:matrix.org
content:
* I think some variant of this question has been asked but I didn't see a clear answer.

Assuming I want to submit a ticket extrinsic that will be included in the first block of a new epoch. At that point I have the last state of the previous epoch.

1. Do I use eta\_1 from the old state for the ring signature context? Given this will become eta\_2 in the next epoch when the ticket will be verified.
2. What about the y\_z, I only have access to the old y\_z. I don't see a way to compute the new one yet. Do I just sign with the old y\_z and hope it doesn't change?
3. When importing this first block in the epoch, would I use the prior y\_z or new y\_z to verify tickets?

id:
686
timestamp:
2025-04-15T15:51:12.024Z
sender:
@greywolve:matrix.org
content:
* I think some variant of this question has been asked but I didn't see a clear answer.

Assuming I want to submit a ticket extrinsic that will be included in the first block of a new epoch. At that point I have the last state of the previous epoch.

1. Do I use eta\_1 from the old state for the ring signature context? Given this will become eta\_2 in the next epoch when the ticket will be verified.
2. What about the y\_z, I only have access to the old y\_z. I don't see a way to compute the new one yet. Do I just sign with the old y\_z and hope it doesn't change?
3. When importing this first block in the epoch, would I use the prior y\_z or new y\_z to verify tickets? [6.29 seems to indicate the prior](https://graypaper.fluffylabs.dev/#/68eaa1f/0f59000f5900?v=0.6.4)

id:
685
timestamp:
2025-04-15T15:51:34.544Z
sender:
@greywolve:matrix.org
content:
* I think some variant of this question has been asked but I didn't see a clear answer.

Assuming I want to submit a ticket extrinsic that will be included in the first block of a new epoch. At that point I have the last state of the previous epoch.

1. Do I use eta\_1 from the old state for the ring signature context? Given this will become eta\_2 in the next epoch when the ticket will be verified.
2. What about the y\_z, I only have access to the old y\_z. I don't see a way to compute the new one yet. Do I just sign with the old y\_z and hope it doesn't change?
3. When importing this first block in the epoch, would I use the prior y\_z or new y\_z to verify tickets? [6.29 seems to indicate the prior.](https://graypaper.fluffylabs.dev/#/68eaa1f/0f59000f5900?v=0.6.4)

id:
684
timestamp:
2025-04-15T15:52:32.965Z
sender:
@greywolve:matrix.org
content:
* I think some variant of this question has been asked but I didn't see a clear answer.

Assuming I want to submit a ticket extrinsic that will be included in the first block of a new epoch. At that point I have the last state of the previous epoch.

1. Do I use eta\_1 from the old state for the ring signature context? Given this will become eta\_2 in the next epoch when the ticket will be verified.
2. What about the y\_z, I only have access to the old y\_z. I don't see a way to compute the new one yet. Do I just sign with the old y\_z (and y_k) and hope it doesn't change?
3. When importing this first block in the epoch, would I use the prior y\_z or new y\_z to verify tickets? [6.29 seems to indicate the prior.](https://graypaper.fluffylabs.dev/#/68eaa1f/0f59000f5900?v=0.6.4)

id:
1784
timestamp:
2025-04-15T16:07:11.013Z
sender:
@jay_ztc:matrix.org
content:
Was this the room mentioned on the call for coordinating about JAMX? Will any critical messages/updates in the implementors room about jamx be bubbled up to this room (Let's JAM)?

id:
1783
timestamp:
2025-04-15T16:08:21.674Z
sender:
@jay_ztc:matrix.org
content:
* Was this the room mentioned on the call for coordinating about JAMX? Will any critical messages/updates in the implementors room about jamx be bubbled up to this room (Let's JAM)? I would like to stay in the loop since I will be attending, but not looking to participate in DAO at this time.

id:
1782
timestamp:
2025-04-15T16:11:41.103Z
sender:
@jay_ztc:matrix.org
content:
* Was this the room mentioned on the call for coordinating about JAMX? Will any critical messages/updates in the implementors room about jamx be bubbled up to this room (Let's JAM)? I would like to stay in the loop since I will be attending, but not looking to participate in the DAO at this time.

id:
1781
timestamp:
2025-04-15T17:41:09.177Z
sender:
@sourabhniyogi:matrix.org
content:
Daniel is hosting the event and will surely be the source of all critical message/updates -- is there a schedule we can look at and try to get ourselves aligned on danicuki | Jamixir since we're starting to get our travel plans together now?

id:
1780
timestamp:
2025-04-15T18:14:12.096Z
sender:
@wabkebab:matrix.org
content:
So, very quick as I have read “travel plans” , tomorrow, we - Pala Labs - will do a more official announcement but as a appetiser, the JAM action will continue in ETHLisbon with a JAM dedicated booth and a - probably in Sat - a keynote about JAM by Daniel C.

id:
1779
timestamp:
2025-04-15T18:14:36.637Z
sender:
@wabkebab:matrix.org
content:
* So, very quick as I have read “travel plans” , tomorrow, we - Pala Labs - will do a more official announcement but as a appetiser, the JAM action will continue in ETHLisbon with a JAM dedicated booth and a - probably in Sat as it is a moving target - a keynote about JAM by Daniel C.

id:
1778
timestamp:
2025-04-15T18:14:55.332Z
sender:
@wabkebab:matrix.org
content:
* So, very quick as I have read “travel plans” , tomorrow, we - Pala Labs - will do a more official announcement but as a appetiser, the JAM action will continue in ETHLisbon with a JAM dedicated booth and a - probably in Sat as it is a moving target - keynote about JAM by Daniel C.

id:
1777
timestamp:
2025-04-15T18:15:13.981Z
sender:
@wabkebab:matrix.org
content:
Stay tuned to the usual JAM channels!

id:
1776
timestamp:
2025-04-15T23:18:02.864Z
sender:
@danicuki:matrix.org
content:
* Unfortunately I missed today’s call to give some updates. The event agenda (beta version) was sent by email to all attendees last week. The organization team is very open to suggestions and ideas of anything different from what was sent. DM me anytime. The event is for everyone. I am just a mere enthusiast organizer.

id:
1775
timestamp:
2025-04-15T23:18:40.114Z
sender:
@danicuki:matrix.org
content:
> <@sourabhniyogi:matrix.org> Daniel is hosting the event and will surely be the source of all critical message/updates -- is there a schedule we can look at and try to get ourselves aligned on danicuki | Jamixir since we're starting to get our travel plans together now?

Unfortunately I missed today’s call to give some updates. The event agenda (beta version) was sent by email to all attendees last week. The organization team is very open to suggestions and ideas of anything different from what was sent. DM me anytime. The event is for everyone. I am just a mere enthusiast organizer.

id:
1774
timestamp:
2025-04-16T07:43:49.970Z
sender:
@gav:polkadot.io
content:
JAM SDK 0.1.21 is out and with it `jamt` and `jamtop`.

id:
1773
timestamp:
2025-04-16T07:44:24.625Z
sender:
@gav:polkadot.io
content:
* JAM SDK 0.1.21 is out and with it `jamt` and `jamtop`, usable by any node which has implemented jip2.

id:
683
timestamp:
2025-04-16T07:52:55.051Z
sender:
@gav:polkadot.io
content:
1. Yes.
3. This is a typo - it should read γ'z, not γz. I'll change this
2. You'll need to compute it - you have all the information necessary from the prior state.


id:
682
timestamp:
2025-04-16T07:53:14.538Z
sender:
@gav:polkadot.io
content:
* 1. Yes.
2. I think this is a typo (@davide - wdyt?) - it should read γ'z, not γz. I'll change this.
3. You'll need to compute it - you have all the information necessary from the prior state.

id:
681
timestamp:
2025-04-16T07:55:14.099Z
sender:
@greywolve:matrix.org
content:
Thanks!

id:
1772
timestamp:
2025-04-16T08:10:17.966Z
sender:
@xlchen:matrix.org
content:
I have some questions about JIP2:
- Can you confirm if it is JSON RPC over WebSocket? or something else?
- What encoding format is used? e.g. for Hash, is it hex string or JSON array of numbers?
- Only subscribe methods are defined but not unsubscribe?
- JSON RPC subscriptions requires another notification method as parameter, which is not included in the document

id:
1771
timestamp:
2025-04-16T08:28:39.293Z
sender:
@dave:parity.io
content:
It's JSON RPC over WS yes. We're using Parity's jsonrpsee crate + serde to implement it. I believe hashes are arrays of 0-255. Unsubscribe methods exist, I believe they're named eg unsubscribeX for subscribeX

id:
1770
timestamp:
2025-04-16T08:31:25.867Z
sender:
@xlchen:matrix.org
content:
those details are crucial for others to follow. can someone update the docs? best to have some example request/response 

id:
1769
timestamp:
2025-04-16T08:48:27.517Z
sender:
@clearloop:matrix.org
content:
I was trying to indicate these stuffs via a mock server XD, however, the panic messages of jamtop 

```
RUST_BACKTRACE=1 jamtop --rpc ws://0.0.0.0:6789
Error: Parse error: invalid type: map, expected a tuple of size 2
```
seems not yet well wrapped with helpful context


id:
680
timestamp:
2025-04-16T08:58:11.025Z
sender:
@davxy:matrix.org
content:
> <@greywolve:matrix.org> Thanks!

Yeah that is a typo

id:
1768
timestamp:
2025-04-16T09:14:05.845Z
sender:
@gav:polkadot.io
content:
Yeah, that'll be some invalid response data I expect.

id:
1767
timestamp:
2025-04-16T09:14:27.960Z
sender:
@gav:polkadot.io
content:
To debug you'll probably want to dump communications

id:
1766
timestamp:
2025-04-16T09:15:09.034Z
sender:
@gav:polkadot.io
content:
Bryan Chen | Laminar & Acala: Already stated in JIP2:

> - `Hash`: a 32 item array with each item numeric between 0 and 255 inclusive.

id:
1765
timestamp:
2025-04-16T09:15:42.478Z
sender:
@gav:polkadot.io
content:
See the text immediately below the section "RPC Specification":

id:
1764
timestamp:
2025-04-16T09:16:00.129Z
sender:
@gav:polkadot.io
content:
image.png

id:
1763
timestamp:
2025-04-16T09:16:21.600Z
sender:
@xlchen:matrix.org
content:
ok. that wasn't the most efficient encoding method so I had to confirm

id:
1762
timestamp:
2025-04-16T09:16:34.400Z
sender:
@gav:polkadot.io
content:
It's pretty explicit :P

id:
1761
timestamp:
2025-04-16T09:16:44.459Z
sender:
@gav:polkadot.io
content:
I'm not sure how I could have made it more so...

id:
1760
timestamp:
2025-04-16T09:17:30.418Z
sender:
@xlchen:matrix.org
content:
any reason it is not hex encoded just like substrate rpc?

id:
1759
timestamp:
2025-04-16T09:17:58.387Z
sender:
@gav:polkadot.io
content:
Yeah - this is the simplest way.

id:
1758
timestamp:
2025-04-16T09:20:44.073Z
sender:
@gav:polkadot.io
content:
JIP2 updated with the other clarifications.

id:
1757
timestamp:
2025-04-16T09:20:58.434Z
sender:
@gav:polkadot.io
content:
* jip2 updated with the other clarifications.

id:
1756
timestamp:
2025-04-16T09:58:57.045Z
sender:
@clearloop:matrix.org
content:
I can see the dashboard of `jamtop` now, from my case, it has no stdout on launch, but after `ctrl+c`, the dashboard shows

```
Services: 0 total, 0 active, 0 refining, 0 acc'ing                         0                                                                                                                        20:00:00
APU: 0% acc/0% xfer/100% idle;  Cores: 0%/0%/100% idle                     1                                                                                                                               0
Processed: 0 refines/0 gas; 0 accs/0 gas; 0 memos/0 gas                    A                                                                                                                       00000000…
D3L: 0 imports; 0 exports; 0 extrinsics/0B; 0 prov/0B                      X

ID       NAME                 FLGS ITMS REFS %RGC  ACCS %AGC  XFRS %XGC  PRVZ PRVS  IMPS EXPS XTS  XTZ   STRS STRZ  BAL   FBAL  MBAL
```

it's currently empty with my RPC, to make it alive, seems we have to fullfill the service features now

id:
1755
timestamp:
2025-04-16T15:05:16.529Z
sender:
@jimboj21:matrix.org
content:
Hey hey, I have a question about the accumulation test vector's pvm execution:

Why do the accumulation test vectors not result in a change to the service storage state? I dont believe the storage attribute is included in these tests. Unless I am mistaken, based on the test service code there should be a host write call which if executed correctly should update the jam state for that service. If the service storage state isn't tested here, isn't the pvm invocation in these tests basically just a no-op then as far as passing the tests is concerned? Am I missing something here?

id:
1754
timestamp:
2025-04-16T15:20:55.855Z
sender:
@dakkk:matrix.org
content:
> <@jimboj21:matrix.org> Hey hey, I have a question about the accumulation test vector's pvm execution:
> 
> Why do the accumulation test vectors not result in a change to the service storage state? I dont believe the storage attribute is included in these tests. Unless I am mistaken, based on the test service code there should be a host write call which if executed correctly should update the jam state for that service. If the service storage state isn't tested here, isn't the pvm invocation in these tests basically just a no-op then as far as passing the tests is concerned? Am I missing something here?

Have you tried to execute the pvm running those tests? 

id:
1753
timestamp:
2025-04-16T15:25:43.227Z
sender:
@jimboj21:matrix.org
content:
Yeah I have. I am working to fix a bug in my pvm right now, so I am not 100% on my execution. This is partly why I ask, maybe i am missing context from the correct execution, however, I still pass all the test vectors. 

id:
1752
timestamp:
2025-04-16T16:19:39.993Z
sender:
@sourabhniyogi:matrix.org
content:
For the notion of structured logs (which we can do some set up for), what do you think it should cover out of these:
(a) PVM execution: (i) auth/refine execution (ii) acc/transfer execution (iii) auditing (iv) jip-1 logs
(b) Refining [superset of (a)(i)] (i) bundle details (ii) work report/work result/availability spec generation 
(c) Accumulation / Transfer (i) wrangled results (ii) ordering (iii) MMR / Beefy accumulation roots (iv) statistics  (v) C1-C15 state changes (vi) service storage/preimage/lookup changes
(d) Block authoring [superset of (a)(ii), (c)] (i) sealing (ii) ticket generation 
(e) Validation errors
(f) Audit Announcements / Judgements
(g) Preimage Announcements / 
(h) Selected JAM NP sends/receives of: { UP0,  ... } not well covered by the above

id:
1751
timestamp:
2025-04-16T16:33:59.624Z
sender:
@jimboj21:matrix.org
content:
For me personally I am currently focusing on M1/block importing so a) and c) and i think e (if i understand what validation errors means) are the most useful to me at this moment.

The others all seem useful as well, but I will let others speak to that. I imagine the log level can be specified to determine which of these we wish to see? 

id:
1750
timestamp:
2025-04-16T16:38:28.165Z
sender:
@sourabhniyogi:matrix.org
content:
* For the notion of structured logs (which we can do some set up for), what do you think it should cover out of these:
(a) PVM execution: (i) auth/refine execution (ii) acc/transfer execution (iii) auditing (iv) jip-1 logs
(b) Refining \[superset of (a)(i)\] (i) bundle details (ii) work report/work result/availability spec generation 
(c) Accumulation / Transfer (i) wrangled results (ii) ordering (iii) MMR / Beefy accumulation roots (iv) statistics  (v) C1-C15 state changes (vi) service storage/preimage/lookup changes
(d) Block authoring \[superset of (a)(ii), (c)\] (i) sealing (ii) ticket generation 
(e) Validation errors
(f) Audit Announcements / Judgements
(g) Preimage Announcements / Sends
(h) Selected JAM NP sends/receives of: { UP0,  ... } not well covered by the above

id:
1749
timestamp:
2025-04-16T16:43:55.728Z
sender:
@sourabhniyogi:matrix.org
content:
* For the notion of structured logs (which we can do some set up for), what do you think it should cover out of these:
(a) PVM execution: (i) auth/refine execution (ii) acc/transfer execution (iii) auditing (iv) jip-1 logs
(b) Refining \[superset of (a)(i)\] (i) bundle details (ii) work report/work result/availability spec generation
(c) Accumulation / Transfer (i) wrangled results (ii) ordering (iii) MMR / Beefy accumulation roots (iv) statistics  (v) C1-C15 state changes (vi) service storage/preimage/lookup changes
(d) Block authoring \[superset of (a)(ii), (c)\] (UP0) (i) sealing (ii) ticket generation (CE 131/132)
(e) Block Validation errors
(f) Audit Announcements [CE 144] / Judgement Publication [CE 145]
(g) Preimage Announcements [CE 142] / Requests+Responses [CE 143]
(h) _Selected_ JAM NP sends/receives not well covered by the above:
* CE 133: Work-package submission, 
* CE 134: Work-package sharing,  
* CE 135: Work-report distribution. 
* CE 136: Work-report request, 
* CE 137: Shard distribution
* CE 138: Audit shard request,
* CE 139/140: Segment shard request, 
* CE 141: Assurance distribution


id:
1748
timestamp:
2025-04-16T16:56:56.123Z
sender:
@0xjunha:matrix.org
content:
I just tested with my implementation, and I could find the PVM code blobs of the accumulate test vectors include `ECALLI`. This is what we can guess from the test service code, which includes `jam_pvm_common::accumulate::set_storage`.

However, when I actually ran all accumulate test cases, and tracked trace of all executed opcodes, no single `ECALLI` was executed.
So my guess is that either `items` argument for the test service `accumulate` function is an empty vec, or `item.result` is not `Ok` and therefore no host function is executed even `accumulate` is invoked. (https://github.com/davxy/jam-test-vectors/blob/562f11d129d3a6ab35bfb91ad5973358ae0654fa/accumulate/test-service/src/main.rs/#L27C48-L27C74)

And I think this is just for simplicity, mainly focusing on Accumulate Queue/History STFs for now and I expect host function test cases might be added in the future.
My implementation is not complete yet, so there could be bugs in mine as well.\

id:
1747
timestamp:
2025-04-16T17:01:22.961Z
sender:
@gav:polkadot.io
content:
> <@sourabhniyogi:matrix.org> For the notion of structured logs (which we can do some set up for), what do you think it should cover out of these:
> (a) PVM execution: (i) auth/refine execution (ii) acc/transfer execution (iii) auditing (iv) jip-1 logs
> (b) Refining \[superset of (a)(i)\] (i) bundle details (ii) work report/work result/availability spec generation
> (c) Accumulation / Transfer (i) wrangled results (ii) ordering (iii) MMR / Beefy accumulation roots (iv) statistics  (v) C1-C15 state changes (vi) service storage/preimage/lookup changes
> (d) Block authoring \[superset of (a)(ii), (c)\] (UP0) (i) sealing (ii) ticket generation (CE 131/132)
> (e) Block Validation errors
> (f) Audit Announcements [CE 144] / Judgement Publication [CE 145]
> (g) Preimage Announcements [CE 142] / Requests+Responses [CE 143]
> (h) _Selected_ JAM NP sends/receives not well covered by the above:
> * CE 133: Work-package submission, 
> * CE 134: Work-package sharing,  
> * CE 135: Work-report distribution. 
> * CE 136: Work-report request, 
> * CE 137: Shard distribution
> * CE 138: Audit shard request,
> * CE 139/140: Segment shard request, 
> * CE 141: Assurance distribution
> 

It should be primarily for the various off-chain events (ie not accumulate) - the on-chain stuff is easily discoverable already. 

id:
1746
timestamp:
2025-04-16T17:02:44.350Z
sender:
@gav:polkadot.io
content:
Receiving a block, importing, authoring and publishing are all sensible things to be cataloguing too. But this is perhaps something to discuss in-person in Lisbon. 

id:
1745
timestamp:
2025-04-16T17:05:40.805Z
sender:
@rustybot:matrix.org
content:
> <@jimboj21:matrix.org> Hey hey, I have a question about the accumulation test vector's pvm execution:
> 
> Why do the accumulation test vectors not result in a change to the service storage state? I dont believe the storage attribute is included in these tests. Unless I am mistaken, based on the test service code there should be a host write call which if executed correctly should update the jam state for that service. If the service storage state isn't tested here, isn't the pvm invocation in these tests basically just a no-op then as far as passing the tests is concerned? Am I missing something here?

I need to check. If not I'll add a couple 

id:
1744
timestamp:
2025-04-17T06:42:47.577Z
sender:
@emielsebastiaan:matrix.org
content:
Can anybody be direct me to the specification jip2 please. 🙏🏻 

id:
1743
timestamp:
2025-04-17T06:43:45.505Z
sender:
@clearloop:matrix.org
content:
https://hackmd.io/@polkadot/jip2

id:
1742
timestamp:
2025-04-17T07:17:19.202Z
sender:
@emielsebastiaan:matrix.org
content:
* Can anybody direct me to the specification jip2 please. 🙏🏻

id:
1741
timestamp:
2025-04-17T09:11:25.057Z
sender:
@danicuki:matrix.org
content:
A sneak peek of the JAM Experience event T-Shirt

id:
1740
timestamp:
2025-04-17T09:11:32.487Z
sender:
@danicuki:matrix.org
content:
Screenshot 2025-04-17 at 10.09.52.png

id:
1739
timestamp:
2025-04-17T13:19:12.069Z
sender:
@wabkebab:matrix.org
content:
**JAM Toaster visit & JAM presence at ETHLisbon !**

Hey JAM fam!

We've got some exciting news - Pala Labs is bringing JAM to **ETHLisbon** (May 9-11)! Since many JAM-ers will already be in Lisbon for the **JAM Experience**, we thought: why not introduce JAM to the Ethereum crowd too?

**Quick highlights**

- A keynote session dedicated to JAM on ( tentatively) May 10
- We'll be having a cool JAM booth with some surprises …
- For builders, there's also a Smart Contract hackathon from 9 to 11

**Exclusive invite for JAM Implementers!**

We're planning an exclusive JAM Toaster visit just for JAM implementers on 6th May, 4 to 9 PM - free transportation, toaster visit, and poolside drinks & canapés. 

Limited spots available! ([Sign up here)](https://lu.ma/jamtoastervisit) 

And of course, cool JAM merch with fantastic designs, like the one just on top of these lines, from the inkwell of xylodrone  - our talented inhouse designer.

Want to help at the booth or just hang out? Reach out to us!

See you in Lisbon! 🚀

id:
1738
timestamp:
2025-04-17T13:19:38.038Z
sender:
@wabkebab:matrix.org
content:
* **JAM Toaster visit & JAM presence at ETHLisbon !**

Hey JAM fam!

We've got some exciting news - Pala Labs is bringing JAM to **ETHLisbon** (May 9-11)! Since many JAM-ers will already be in Lisbon for the **JAM Experience**, we thought: why not introduce JAM to the Ethereum crowd too?

**Quick highlights**

- A keynote session dedicated to JAM on ( tentatively) May 10
- We'll be having a cool JAM booth with some surprises …
- For builders, there's also a Smart Contract hackathon from 9 to 11

**Exclusive invite for JAM Implementers!**

We're planning an exclusive JAM Toaster visit just for JAM implementers on 6th May, 4 to 9 PM - free transportation, toaster visit, and poolside drinks & canapés.

Limited spots available! ([Sign up here](https://lu.ma/jamtoastervisit) )

And of course, cool JAM merch with fantastic designs, like the one just on top of these lines, from the inkwell of xylodrone  - our talented inhouse designer.

Want to help at the booth or just hang out? Reach out to us!

See you in Lisbon! 🚀

id:
1737
timestamp:
2025-04-17T13:25:55.239Z
sender:
@wabkebab:matrix.org
content:
* **JAM Toaster visit & JAM presence at ETHLisbon !**

Hey JAM fam!

We've got some exciting news - Pala Labs is bringing JAM to **ETHLisbon** (May 9-11)! Since many JAM-ers will already be in Lisbon for the **JAM Experience**, we thought: why not introduce JAM to the Ethereum crowd too?

**Quick highlights**

- A keynote session dedicated to JAM on ( tentatively) May 10
- We'll be having a cool JAM booth with some surprises …
- For builders, there's also a Smart Contract hackathon on Kusama from 9 to 11

**Exclusive invite for JAM Implementers!**

We're planning an exclusive JAM Toaster visit just for JAM implementers on 6th May, 4 to 9 PM - free transportation, toaster visit, and poolside drinks & canapés.

Limited spots available! ([Sign up here](https://lu.ma/jamtoastervisit) )

And of course, cool JAM merch with fantastic designs, like the one just on top of these lines, from the inkwell of xylodrone  - our talented inhouse designer.

Want to help at the booth or just hang out? Reach out to us!

See you in Lisbon! 🚀

id:
1736
timestamp:
2025-04-17T13:26:28.228Z
sender:
@wabkebab:matrix.org
content:
* **JAM Toaster visit & JAM presence at ETHLisbon !**

Hey JAM fam!

We've got some exciting news - Pala Labs is bringing JAM to **ETHLisbon** (May 9-11)! Since many JAM-ers will already be in Lisbon for the **JAM Experience**, we thought: why not introduce JAM to the Ethereum crowd too?

**Quick highlights**

- A keynote session dedicated to JAM on ( tentatively) May 10
- We'll be having a cool JAM booth with some surprises …
- For builders, there's also a Smart Contract hackathon on Kusama from 9 to 11 of May

**Exclusive invite for JAM Implementers!**

We're planning an exclusive JAM Toaster visit just for JAM implementers on 6th May, 4 to 9 PM - free transportation, toaster visit, and poolside drinks & canapés.

Limited spots available! ([Sign up here](https://lu.ma/jamtoastervisit) )

And of course, cool JAM merch with fantastic designs, like the one just on top of these lines, from the inkwell of xylodrone  - our talented inhouse designer.

Want to help at the booth or just hang out? Reach out to us!

See you in Lisbon! 🚀

id:
679
timestamp:
2025-04-17T16:04:00.167Z
sender:
@danicuki:matrix.org
content:
I have a doubt about core statistics formula:

Formula 13.9 points to w (lowercased) - reports in guarantees extrinsic: https://graypaper.fluffylabs.dev/#/68eaa1f/190d01190f01?v=0.6.4

Formula 13.10 points to W (uppercase) - available reports:
https://graypaper.fluffylabs.dev/#/68eaa1f/193e01193f01?v=0.6.4

Is this correct? If so, why is that? Also, in this context, it is very easy to incorrectly use W in the place of w, or vice-versa, since visually the difference is very subtle.

id:
1735
timestamp:
2025-04-17T21:03:03.315Z
sender:
@sourabhniyogi:matrix.org
content:
Here is a wishlist for a workshop agenda:
 (1) From Tiny Testnets to Everyone in the Toaster: jip2 [jamtop] + Telemetry, Structured Logging -- and how to do this in an "anti-fragile" way
 (2) Recompiler 101 + Advanced topics (must have Jan Bujak)
 (3) Building services with SDK + polkatool, CoreVM + guest programs

id:
1734
timestamp:
2025-04-17T21:35:01.282Z
sender:
@jay_ztc:matrix.org
content:
Will the Node RPC from JIP2 be used for milestone acceptance testing? I wonder if some full node implementations will choose to forego implementing RPC entirely, given that light clients are the stated preference & their development is now incentivized directly from the prize pool.

id:
1733
timestamp:
2025-04-17T21:35:11.324Z
sender:
@jay_ztc:matrix.org
content:
* Will the Node RPC from JIP2 be used in milestone acceptance testing? I wonder if some full node implementations will choose to forego implementing RPC entirely, given that light clients are the stated preference & their development is now incentivized directly from the prize pool.

id:
1732
timestamp:
2025-04-17T21:35:59.991Z
sender:
@jay_ztc:matrix.org
content:
* Will the Node RPC from JIP2 be used in milestone acceptance testing? I wonder if some full node implementations will choose to forego implementing JIP2 entirely, given that light clients are the stated preference & their development is now incentivized directly from the prize pool.

id:
1731
timestamp:
2025-04-17T21:36:23.263Z
sender:
@jay_ztc:matrix.org
content:
* Will the Node RPC from JIP2 be used in milestone acceptance testing? I wonder if some full node implementations will choose to forego implementing the JIP2 RPC, given that light clients are the stated preference & their development is now incentivized directly from the prize pool.

id:
1730
timestamp:
2025-04-17T21:38:44.533Z
sender:
@jay_ztc:matrix.org
content:
* Will the Node RPC from JIP2 be used in milestone acceptance testing? I wonder if some full node implementations will choose to forego implementing the spec, given that light clients are the stated preference & their development is now incentivized directly from the prize pool.

id:
1729
timestamp:
2025-04-17T22:37:00.033Z
sender:
@sourabhniyogi:matrix.org
content:
* Here is a wishlist for a workshop agenda:
(1) From Tiny Testnets to Everyone in the Toaster: jip2 \[jamtop\] + Telemetry, Structured Logging -- and how to do this in an "anti-fragile" way
(2) Recompiler 101 + Advanced topics (must have Jan Bujak)
(3) Building services with SDK + polkatool, CoreVM + guest programs
(4) Mind Viruses in the World and how to combat them

id:
1728
timestamp:
2025-04-18T04:12:23.498Z
sender:
@gav:polkadot.io
content:
Agreed we need to do a seminar on structured logging - I thnk this will be a crucial next stop for making serious use of the Toaster erin arkadiy David Emett ...

id:
1727
timestamp:
2025-04-18T04:13:46.801Z
sender:
@gav:polkadot.io
content:
We're pretty much ready to go with the tiny testnet; getting 1023 node active might be a challenge...

id:
1726
timestamp:
2025-04-18T04:13:59.273Z
sender:
@gav:polkadot.io
content:
* We're pretty much ready to go with the tiny testnet for any teams who want to try connecting; getting 1023 node active might be a challenge...

id:
1725
timestamp:
2025-04-18T04:15:44.256Z
sender:
@gav:polkadot.io
content:
I'll be happy to talk about SDK (minimal as it is right now) and hopefully a walkthrough of CoreVM. We're working hard on getting CoreVM releasable (along with PVM Doom) by May.

id:
1724
timestamp:
2025-04-18T04:16:25.035Z
sender:
@gav:polkadot.io
content:
If you're at M2 with a high-efficiency interpreter PVM you might be able to run DOOM on your own JAM.

id:
1723
timestamp:
2025-04-18T04:16:36.837Z
sender:
@gav:polkadot.io
content:
* If you're at M2 with a high-efficiency interpreter PVM you might be able to run DOOM on your own JAM. (We can with PolkaVM interpreter.)

id:
1722
timestamp:
2025-04-18T04:17:21.953Z
sender:
@gav:polkadot.io
content:
I'll give a presentation on the Polkadot Palace if people are interested, too.

id:
1721
timestamp:
2025-04-18T04:19:14.442Z
sender:
@gav:polkadot.io
content:
Parity PolkaJAM team (including myself) are having a longer retreat based out of the Lisbon Parity office for the following week and a half after the JAMXP; any teams who want to stick around and try to continue to get interop are ofc welcome to work from the Parity office too.

id:
1720
timestamp:
2025-04-18T05:25:12.637Z
sender:
@boymaas:matrix.org
content:
That’s amazing news. Just to confirm, is it permissible under the rules for teams to begin coordinating the networking aspects with one another in advance, provided all communication takes place through public channels?

id:
1719
timestamp:
2025-04-18T05:55:30.122Z
sender:
@sourabhniyogi:matrix.org
content:
Opening a thread right here so we can prep before during and after 😀

id:
1718
timestamp:
2025-04-18T06:48:01.255Z
sender:
@gav:polkadot.io
content:
Yes, provided comms are open and easily discoverable by other teams and any newcomers, and of course that it doesn't impact the nature of the clean-room rules then it's fine.

id:
1717
timestamp:
2025-04-18T06:48:09.452Z
sender:
@gav:polkadot.io
content:
* Yes, provided comms are open and easily discoverable by other teams/newcomers, and of course that it doesn't impact the nature of the clean-room rules then it's fine.

id:
1716
timestamp:
2025-04-18T06:48:19.999Z
sender:
@gav:polkadot.io
content:
* Yes, provided comms are open and easily discoverable by other teams/newcomers, and of course that it doesn't impact the nature of the clean-room rules, then it's fine.

id:
1715
timestamp:
2025-04-18T06:49:19.765Z
sender:
@gav:polkadot.io
content:
Generally speaking, if discourse happens openly and in easily discoverable channels then there shouldn't be a problem.

id:
1714
timestamp:
2025-04-18T06:52:27.275Z
sender:
@gav:polkadot.io
content:
Also, I'm cogniscant that forcing everything to happen in these two channels isn't ideal. So I've a bit of a plan here:

1. Get some sort of automation to periodically archive these two channels.
2. Host the archive in one or more obvious places (graypaper.com, jamcha.in, others?)
3. Request that others who want to have discourse away from these channels offer up the same thing.
4. Have some AI guy train an "official" AI model periodcally on the content of these channels as well as any lectures of mine, the Gray Paper &c.
5. Provide the model as a download for anyone to use with e.g. Jan.

id:
1713
timestamp:
2025-04-18T06:52:54.669Z
sender:
@gav:polkadot.io
content:
* Also, I'm cogniscant that forcing everything to happen in these two channels isn't ideal. So I've a bit of a plan here:

1. Get some sort of automation to periodically archive these two channels.
2. Host the archive in one or more obvious places (graypaper.com, jamcha.in, others?)
3. Request that others who want to have discourse away from these channels offer up the same thing and provide linking/hosting on these central spots for it.
4. Have some AI guy train an "official" AI model periodcally on the content of these channels as well as any lectures of mine, the Gray Paper &c.
5. Provide the model as a download for anyone to use with e.g. Jan.

id:
1712
timestamp:
2025-04-18T06:54:23.705Z
sender:
@gav:polkadot.io
content:
As long as *all* non-official channels data gets into this central easily-searchable/discoverable knowledge base in a timely fashion, then I don't see a huge problem with expanding out from the two official channels.

id:
1711
timestamp:
2025-04-18T06:54:36.778Z
sender:
@gav:polkadot.io
content:
Of course I won't necessarily be monitoring other channels.

id:
1710
timestamp:
2025-04-18T06:56:06.377Z
sender:
@gav:polkadot.io
content:
* Generally speaking, if discourse happens openly and in easily discoverable channels and it's solely about getting the interop working then there shouldn't be a problem.

id:
1709
timestamp:
2025-04-18T06:56:26.097Z
sender:
@gav:polkadot.io
content:
* Agreed we need to do a seminar on structured logging - I thnk this will be a crucial next step for making serious use of the Toaster erin arkadiy David Emett ...

id:
1708
timestamp:
2025-04-18T07:15:28.373Z
sender:
@gav:polkadot.io
content:
As a start to this I initiated https://hackmd.io/@polkadot/jip3 as a collaborative doc, which anyone can add their own messages to.

id:
1707
timestamp:
2025-04-18T07:15:39.393Z
sender:
@gav:polkadot.io
content:
* As a start to this I initiated https://hackmd.io/@polkadot/jip3 as a collaborative doc, which teams are welcome to add their own messages to.

id:
1706
timestamp:
2025-04-18T08:34:32.711Z
sender:
@boymaas:matrix.org
content:
Thanks gav , that’s clear. We’ll make sure all coordination stays open and easy to find, and respects the clean-room principles.

id:
1705
timestamp:
2025-04-18T12:18:13.052Z
sender:
@sourabhniyogi:matrix.org
content:
image.png

id:
1704
timestamp:
2025-04-18T12:18:37.213Z
sender:
@sourabhniyogi:matrix.org
content:
https://discord.gg/aEhVqngM

id:
1703
timestamp:
2025-04-18T12:20:37.427Z
sender:
@sourabhniyogi:matrix.org
content:
How would you improve on this:
"100% public channel for JAM Implementers to set up JAM testnets with each other in prep for participation in a JAM Toaster testnet.   

IMPORTANT:
This channel will be periodically archived and may be freely used to train an AI model and form a searchable/discoverable JAM knowledge base.    If you are not comfortable with this, do not participate in this channel.

See also: https://hackmd.io/@polkadot/jamprize
"
3/4/5 is super reasonable and easy to execute on 



id:
1702
timestamp:
2025-04-18T12:22:54.003Z
sender:
@subotic:matrix.org
content:
Hey gav, as I'm looking into how to best approach implementing the recompiler and PVM native execution, I'm trying out different things. Regarding the prize, some time ago you mentioned, that as long as more than 50% of the codebase is in one language (in our case Scala), it would be ok to write small parts of it in another language. In our case, the PVM interpreter is implemented in Scala, but I'm looking at using Rust to implement the native execution parts for the PVM. Would that be fine, or would the Rust code fall under a different language category, irrespective of the size?

id:
1701
timestamp:
2025-04-18T12:33:41.827Z
sender:
@jay_ztc:matrix.org
content:
according to https://hackmd.io/@polkadot/jamprize#Language-Sets, pvm is one of the components that doesn't impact an implementations language set categorization (I've done the same thing you are looking to do, but with Python<->rust)

id:
1700
timestamp:
2025-04-18T12:34:03.992Z
sender:
@jay_ztc:matrix.org
content:
* according to https://hackmd.io/@polkadot/jamprize#Language-Sets, pvm is one of the components that doesn't impact an implementations language set categorization (I'm in the same boat, but with Python\<->rust)

id:
1699
timestamp:
2025-04-18T12:36:53.140Z
sender:
@subotic:matrix.org
content:
Thank you very much! Great to hear.

id:
1698
timestamp:
2025-04-18T13:01:19.014Z
sender:
@jay_ztc:matrix.org
content:
* according to https://hackmd.io/@polkadot/jamprize#Language-Sets, pvm is one of the components that doesn't impact an implementations language set categorization

id:
1697
timestamp:
2025-04-18T13:12:19.585Z
sender:
@sourabhniyogi:matrix.org
content:
https://github.com/davxy/jam-test-vectors/issues/40

id:
678
timestamp:
2025-04-18T15:05:39.438Z
sender:
@jay_ztc:matrix.org
content:
Just to confirm my understanding- the PVM entry pc is NOT required to be the start of a basic block, correct? Jan Bujak 

id:
677
timestamp:
2025-04-18T15:11:05.401Z
sender:
@jan:parity.io
content:
Currently, yes. Note that this is only relevant for inner PVMs - the outer PVM always starts at 0.

id:
676
timestamp:
2025-04-18T15:11:45.990Z
sender:
@jan:parity.io
content:
* Currently, yes (because we need the ability to resume execution after a page fault). Note that this is only relevant for inner PVMs - the outer PVM always starts at 0.

id:
675
timestamp:
2025-04-18T15:16:23.725Z
sender:
@jay_ztc:matrix.org
content:
interesting point about resuming after a nested pvm page fault- thanks for clarifying. Aren't the outer invocation entry pcs non-zero though? For example accumulate initial pc is 5 right? https://graypaper.fluffylabs.dev/#/68eaa1f/2ec1002ec100?v=0.6.4

id:
674
timestamp:
2025-04-18T15:17:46.422Z
sender:
@jimboj21:matrix.org
content:
I would think the non-zero cases would have to be jumps right?

id:
673
timestamp:
2025-04-18T15:18:47.019Z
sender:
@jan:parity.io
content:
Right, sorry, since it's late here I had a brainfart there.

id:
672
timestamp:
2025-04-18T15:18:54.671Z
sender:
@jan:parity.io
content:
* Currently, yes (because we need the ability to resume execution after a page fault).

id:
671
timestamp:
2025-04-18T15:20:51.939Z
sender:
@jan:parity.io
content:
I suppose technically the outer PVM's entry points could end up in the middle of a basic block if you'd build a particularly cursed blob.

id:
670
timestamp:
2025-04-18T15:21:08.034Z
sender:
@jay_ztc:matrix.org
content:
No worries, thanks for clarifying. It sounds like it's acceptable for an outer entrypoint to be into the middle of a basic block. My apologies for @ing after hours.

id:
669
timestamp:
2025-04-18T15:23:01.636Z
sender:
@jan:parity.io
content:
I have considered disallowing such entry points in the past (basically allow only start-of-basic-block entry points and those needed for hostcall + page fault resumption), but we haven't made such change to the GP yet.

id:
668
timestamp:
2025-04-18T15:23:16.611Z
sender:
@jay_ztc:matrix.org
content:
cursed blob/ an intentional attempt to break consensus 😉

id:
667
timestamp:
2025-04-18T15:24:38.554Z
sender:
@jay_ztc:matrix.org
content:
That's how I approached in my reasoning as well, do you think this change is likely?

id:
666
timestamp:
2025-04-18T15:25:39.725Z
sender:
@jay_ztc:matrix.org
content:
* That's how I approached in my reasoning as well, do you think this change is likely? No worries if its too early to tell- just curious. Might end up tabling this in my impl.

id:
665
timestamp:
2025-04-18T15:30:20.039Z
sender:
@jan:parity.io
content:
Can't give you a 100% answer at this point, but it is possible. In general the plan for the final gas cost model is to charge gas only at the beginning of the basic blocks (because charging per instruction is very inefficient), so now if you allow entry points anywhere this potentially complicates the gas metering implementation. Unfortunately disallowing them doesn't necessarily alleviate the problem because you still need to allow entry after page faults, and since memory accesses are so common we don't want to make memory access instructions into basic block terminators.

id:
664
timestamp:
2025-04-18T15:35:55.280Z
sender:
@jay_ztc:matrix.org
content:
Thanks for clarifying, this is really helpful. I don't have enough context yet to form an opinion on the tradeoffs between allowing inner-instances being able to resume right after pageFault vs requiring them to restart/rollback to some basic block.

id:
663
timestamp:
2025-04-18T15:36:16.000Z
sender:
@jay_ztc:matrix.org
content:
* Thanks for clarifying, this is really helpful. I don't have enough context yet to form an opinion on the tradeoffs between allowing inner-instances being able to resume right after pageFault vs requiring them to restart/rollback to the start of a basic block.

id:
662
timestamp:
2025-04-18T15:37:57.022Z
sender:
@jan:parity.io
content:
Although we *need* some limitation when it comes to entry points, because without any restrictions you could e.g. jump into the middle of an instruction, and depending on the particular bytes used that might actually be a valid instruction. Supporting this in practice would be a nightmare in any other implementation that isn't a naive slow interpreter, so at very least jumping in the middle of instructions is something that we definitely do *not* want to support.

id:
661
timestamp:
2025-04-18T15:38:02.819Z
sender:
@jay_ztc:matrix.org
content:
context as far as the programs/usage of them on top of JAM I mean

id:
659
timestamp:
2025-04-18T15:39:22.283Z
sender:
@jan:parity.io
content:
Unfortunately it's impossible to require a restart at the start of a basic block as that'd screw up the program state.

id:
660
timestamp:
2025-04-18T15:39:22.366Z
sender:
@jay_ztc:matrix.org
content:
* *context as far as the programs/usage of programs on top of JAM

id:
658
timestamp:
2025-04-18T15:39:59.714Z
sender:
@jan:parity.io
content:
The already executed part of the basic block might have modified memory or registers in a way that is irreversible.

id:
657
timestamp:
2025-04-18T15:40:38.638Z
sender:
@jan:parity.io
content:
So a rollback is not possible without taking a snapshot at the start of every basic block which might page fault, and we do not want that as it'd be abysmally slow.

id:
656
timestamp:
2025-04-18T15:40:52.047Z
sender:
@jay_ztc:matrix.org
content:
the outer pvm instance would have access to gas host function and could potentially use its own memory as a backup before any unsafe calls right? Although, this gets way complicated fast

id:
655
timestamp:
2025-04-18T15:41:03.056Z
sender:
@jay_ztc:matrix.org
content:
lol we were thinking the same, yep

id:
654
timestamp:
2025-04-18T15:41:20.709Z
sender:
@jay_ztc:matrix.org
content:
* lol we were thinking the same, yep- agreed

id:
653
timestamp:
2025-04-18T15:42:23.735Z
sender:
@jay_ztc:matrix.org
content:
But at the least, the outer pvm could do gas check to determine if it wants to 'risk' another inner invocation right? Although being turing complete makes this pretty hard- unless there's some well-defined gas-estimate API contract between nested invocations

id:
652
timestamp:
2025-04-18T15:43:08.846Z
sender:
@jan:parity.io
content:
That's infeasible; if you could do that you'd become a very rich person as then it'd mean you solved the halting problem. :P

id:
651
timestamp:
2025-04-18T15:44:21.200Z
sender:
@jay_ztc:matrix.org
content:
good point lol, suppose that api idea would be practically useless due to such a small scope of applicability... 

id:
650
timestamp:
2025-04-18T15:46:14.519Z
sender:
@jay_ztc:matrix.org
content:
This discussion has provided a lot of clarity, and given me a few things to think about... Many thanks for your time Jan Bujak  🙏

id:
649
timestamp:
2025-04-18T17:05:47.321Z
sender:
@jimboj21:matrix.org
content:
image.png

id:
648
timestamp:
2025-04-18T17:06:47.524Z
sender:
@jimboj21:matrix.org
content:
image.png

id:
647
timestamp:
2025-04-18T17:07:45.053Z
sender:
@jimboj21:matrix.org
content:
Given the return signature: When 12.17 is called shouldnt the assignment order be o*, t*, b*, u* ?

id:
1696
timestamp:
2025-04-19T11:54:14.376Z
sender:
@sourabhniyogi:matrix.org
content:
* Here is a wishlist for a workshop agenda:
(1) From Tiny Testnets to Everyone in the Toaster: jip2 \[jamtop\] + Telemetry, Structured Logging -- and how to do this in an "anti-fragile" way
(2) Recompiler 101 + Advanced topics (must have Jan Bujak)
(3) Building services with SDK + polkatool, CoreVM + guest programs
(4) Mind Viruses in the World and how to combat them
(5) DOOM Dissection and how to sell JAM

id:
1695
timestamp:
2025-04-19T12:45:12.461Z
sender:
@ascriv:matrix.org
content:
Is it possible to get more clarity on what the M1 validation process will look like? Will it be a set of test vectors a la jam-test-vectors? Will they have the same format?

id:
646
timestamp:
2025-04-19T16:18:07.065Z
sender:
@ycc3741:matrix.org
content:
I just want to confirm something about STF.
Should the dispute when updating ψ′ happen before safrole?
Because based on what I see in this [link](https://graypaper.fluffylabs.dev/#/68eaa1f/0ea2000ea200?v=0.6.4),
updating gamma_k' requires ψ′_o.

id:
1694
timestamp:
2025-04-19T17:22:01.749Z
sender:
@gav:polkadot.io
content:
> <@ascriv:matrix.org> Is it possible to get more clarity on what the M1 validation process will look like? Will it be a set of test vectors a la jam-test-vectors? Will they have the same format?

Best check with [davxy](https://matrix.to/#/@davxy:matrix.org). 

id:
645
timestamp:
2025-04-19T17:23:23.213Z
sender:
@gav:polkadot.io
content:
> <@jimboj21:matrix.org> Given the return signature: When 12.17 is called shouldnt the assignment order be o*, t*, b*, u* ?

Yes. I think this is already fixed in main. 

id:
644
timestamp:
2025-04-19T17:26:49.453Z
sender:
@gav:polkadot.io
content:
> <@ycc3741:matrix.org> I just want to confirm something about STF.
> Should the dispute when updating ψ′ happen before safrole?
> Because based on what I see in this [link](https://graypaper.fluffylabs.dev/#/68eaa1f/0ea2000ea200?v=0.6.4),
> updating gamma_k' requires ψ′_o.

Yes pretty much. Order is technically an implementation detail - some language don’t have that concept - so I’m not going to tell you any order per se, but your reading is correct - key rotation is dependent on disputes. 

id:
1693
timestamp:
2025-04-19T18:47:47.400Z
sender:
@davxy:matrix.org
content:
The conformance testing tool will engage directly with your implementation (the target) in a more interactive fashion.

Naturally, passing all test vectors is a prerequisite, as the tool may expose your implementation to more complex or unforeseen scenarios. 

The overall process is relatively straightforward. Once the tool communicates the genesis state to the target, it will begin submitting procedurally generated blocks, which your implementation should attempt to import. It will then validate the resulting state root against the expected one. 

On failure a failure test vector is produced (prior raw state kv + block + posterior diff with expected state).

The content of these blocks is pseudo-random and determined by a specific seed. To reproduce a particular run, the corresponding seed must be known.

The format of the state will align with the trace format adopted by the community, i.e. the whole stare raw key value (refer to the jam duna traces, also see https://github.com/davxy/jam-test-vectors/issues/40). 

The communication between the conformance testing tool and the target implementation will occur over a simple pipe.

Additionally, the tool is capable of instructing the target to adopt a specific state, after which it can function as a pure fuzzer submitting millions of mutated versions of the same block (currently via libfuzzer, but we may experiment AFL as well)

Nothing is written in stone btw. The tool is currently a work in progress, and new ideas may emerge to refine it.  Further details can be discussed in Lisbon.

Some initial ideas were also drafted here:
https://github.com/w3f/jamtestvectors/issues/21

id:
1692
timestamp:
2025-04-19T18:59:14.435Z
sender:
@davxy:matrix.org
content:
* The conformance testing tool will engage directly with your implementation (the target) in a more interactive fashion.

Naturally, passing all test vectors is a prerequisite, as the tool may expose your implementation to more complex or unforeseen scenarios. 

The overall process is relatively straightforward. Once the tool communicates the genesis state to the target, it will begin submitting procedurally generated blocks, which your implementation should attempt to import. It will then validate the resulting state root against the expected one. 

On failure, a test vector is produced (prior raw state kv + block + posterior diff with expected state).

The content of these blocks is pseudo-random and determined by a specific seed. To reproduce a particular run, the corresponding seed must be known.

The format of the state will align with the trace format adopted by the community, i.e. the whole state raw key-value (refer to the jam duna traces, also see https://github.com/davxy/jam-test-vectors/issues/40). 

The communication between the conformance testing tool and the target implementation will occur over a simple pipe.

Additionally, the tool is capable of instructing the target to adopt a specific state, after which it can function as a pure fuzzer submitting millions of mutated versions of the same block (currently via libfuzzer, but we may experiment AFL as well)

Nothing is written in stone btw. The tool is currently a work in progress, and new ideas may emerge to refine it.  Further details can be discussed in Lisbon.

Some initial ideas were also drafted here:
https://github.com/w3f/jamtestvectors/issues/21

id:
1691
timestamp:
2025-04-20T00:42:32.103Z
sender:
@jaymansfield:matrix.org
content:
Screenshot 2025-04-19 at 8.41.23 PM.png

id:
1690
timestamp:
2025-04-20T00:42:34.447Z
sender:
@jaymansfield:matrix.org
content:
Are there any teams that have implemented JIP-2 and have jamtop working with their implementations? I finished off all functionality for JIP-2 and running into some difficulties with jamtop. For me it starts up and requests parameters, best block and service info (data and preimages), subscribes to statistics, and then basically stops updating the interface. No errors or anything are shown.

id:
1689
timestamp:
2025-04-20T00:42:56.391Z
sender:
@jaymansfield:matrix.org
content:
* Are there any teams that have implemented JIP-2 and have jamtop working with their implementations? I finished off all functionality for JIP-2 and running into some difficulties with jamtop. For me it starts up and requests parameters, best block and service info (data and preimages), subscribes to statistics, and then basically stops updating the interface and sending requests. No errors or anything are shown.

id:
1688
timestamp:
2025-04-20T04:06:15.549Z
sender:
@gav:polkadot.io
content:
> <@jaymansfield:matrix.org> Are there any teams that have implemented JIP-2 and have jamtop working with their implementations? I finished off all functionality for JIP-2 and running into some difficulties with jamtop. For me it starts up and requests parameters, best block and service info (data and preimages), subscribes to statistics, and then basically stops updating the interface. No errors or anything are shown.

The interface updates are driven by updates from the statistics subscription, which should come once per block. Are you sure your node is sending these?

id:
643
timestamp:
2025-04-20T14:18:18.185Z
sender:
@gav:polkadot.io
content:
* Yes pretty much. Order is technically an implementation detail - some languages don’t have the concept of ordering - so I’m not going to tell you any order per se, but your reading is correct - key rotation is dependent on disputes.

id:
642
timestamp:
2025-04-20T14:21:01.323Z
sender:
@gav:polkadot.io
content:
Yes your reading is correct. It is in order to get as much information as possible in statistics, not just reported stuff (which represents the most recent computation work done on cores by guarantors) but also what data - required by each core - has recently been made available through assurers

id:
1687
timestamp:
2025-04-20T18:45:26.276Z
sender:
@sourabhniyogi:matrix.org
content:
I suggest we reuse JAMSNP's exact encoding method to the fullest extent possible, and do every single CE -- modeled here

https://hackmd.io/@sourabhniyogi/jip3-recommendation

Any changes going from JAMSNP to JAMNP, including codec encoding of the JAM representations, would be expected to follow in JIP3, making telemetry be a "whatever you do in QUIC, you send to the telemetry server".

This enables each teams with a working tiny testnet to simply share their entire logs with another team prior to working with them by simply providing their telemetry server (and their genesis state, which includes the validator set). This can be used as a prequisite to enter into a larger network, including the "full" JAM Toaster.

What do you think of this?

id:
1686
timestamp:
2025-04-20T19:26:54.585Z
sender:
@sourabhniyogi:matrix.org
content:
* I suggest we reuse JAMSNP's exact encoding method to the fullest extent possible, and do every single CE -- modeled here

https://hackmd.io/@sourabhniyogi/jip3-recommendation

Any changes going from JAMSNP to JAMNP, including codec encoding of the JAM representations, would be expected to follow in JIP3, making telemetry be a "whatever you do in QUIC, you send to the telemetry server".

This enables each team with a working [tiny] testnet to simply share their entire logs with another team prior to working with them by simply providing their telemetry server endpoint (and their genesis state, which includes the validator set). This can be used as a prequisite to enter into a larger network, including the "full" JAM Toaster.

What do you think of this?

id:
1685
timestamp:
2025-04-20T23:20:39.189Z
sender:
@sourabhniyogi:matrix.org
content:
* I suggest we reuse JAMSNP's exact encoding method to the fullest extent possible, and do every single CE -- modeled here

https://hackmd.io/@sourabhniyogi/jip3-recommendation

Any changes going from JAMSNP to JAMNP, including codec encoding of the JAM representations, would be expected to follow in JIP3, making telemetry be a "whatever you do in QUIC, you send to the telemetry server".

This enables each team with a working \[tiny\] testnet to simply share their entire logs with another team prior to working with them by simply providing their telemetry server endpoint (and their genesis state, which includes the validator set). This can be used as a prequisite to enter into a larger network, including the "full" JAM Toaster.

What do you think of this?

And... Happy Birthday JAM Gray Paper! =)

id:
1684
timestamp:
2025-04-21T03:22:01.620Z
sender:
@sourabhniyogi:matrix.org
content:
* I suggest we reuse JAMSNP's exact encoding method to the fullest extent possible, and do every single CE -- modeled here

https://hackmd.io/@sourabhniyogi/jip3-recommendation

Any changes going from JAMSNP to JAMNP, including codec encoding of the JAM representations, would be expected to follow in JIP3, making telemetry be a "whatever you do in QUIC, you send to the telemetry server".

This enables each team with a working \[tiny\] testnet to simply share their entire logs with another team prior to working with them by simply providing their telemetry server endpoint (and their genesis state, which includes the validator set). This can be used as a prerequisite to enter into a larger network, including the "full" JAM Toaster.

What do you think of this?

And... Happy Birthday JAM Gray Paper! =)

id:
1683
timestamp:
2025-04-21T09:01:56.841Z
sender:
@gav:polkadot.io
content:
Yes, just needs proper linkage from community centrepoints like graypaper.com and jamcha.in

id:
1682
timestamp:
2025-04-21T09:03:30.132Z
sender:
@gav:polkadot.io
content:
Loggin the receipt of network messages can be helpful, but it's only scratching the surface.

id:
1681
timestamp:
2025-04-21T09:03:38.462Z
sender:
@gav:polkadot.io
content:
* Loggin the receipt (and even transmission) of network messages can be helpful, but it's only scratching the surface.

id:
1680
timestamp:
2025-04-21T09:04:05.666Z
sender:
@gav:polkadot.io
content:
Really what we care about is uncovering and describing internal state of the off-chain mechanisms and visualizing them en-mass.

id:
1679
timestamp:
2025-04-21T09:04:10.525Z
sender:
@gav:polkadot.io
content:
* Really what we care about is uncovering and describing internal state of the off-chain mechanisms and visualizing them in aggregate

id:
1678
timestamp:
2025-04-21T10:58:53.557Z
sender:
@knight1205:matrix.org
content:
Any testcases for this function or merklization functions?

id:
1677
timestamp:
2025-04-21T12:13:37.780Z
sender:
@danicuki:matrix.org
content:
> <@sourabhniyogi:matrix.org> I suggest we reuse JAMSNP's exact encoding method to the fullest extent possible, and do every single CE -- modeled here
> 
> https://hackmd.io/@sourabhniyogi/jip3-recommendation
> 
> Any changes going from JAMSNP to JAMNP, including codec encoding of the JAM representations, would be expected to follow in JIP3, making telemetry be a "whatever you do in QUIC, you send to the telemetry server".
> 
> This enables each team with a working \[tiny\] testnet to simply share their entire logs with another team prior to working with them by simply providing their telemetry server endpoint (and their genesis state, which includes the validator set). This can be used as a prerequisite to enter into a larger network, including the "full" JAM Toaster.
> 
> What do you think of this?
> 
> And... Happy Birthday JAM Gray Paper! =)

Happy Birthday JAM. It is amazing to see how much we’ve built in just one year. And we are just getting started! Let’s JAM 

id:
1676
timestamp:
2025-04-21T14:27:24.333Z
sender:
@sourabhniyogi:matrix.org
content:
To support M3 PVM interpretation => recompilation, we should desire the timing (# of milliseconds) spent executing all the timings of the key PVM operations + erasure encoding + justification generation
* 0 [block announcement for authoring]
* 131/132 [ticket generation]
* 133/134/145 [work package refinement/auditing].
* 137/138/139/140 [DA encoding]
* 224/225 [BLS Signature Generation]
The telemetry server can usefully pipe this into Jaeger tracing (or similar) for visualization, but from attempting multihop XCM visualization, but it probably helps to also have work package hash (133/134/145/137/138/139/140) and header hash (0/192/193/224/225) so the telemetry server doesn't have to do any JAM-specific indexing. 

With benchmark workpackages, a implementer leaderboard should be developed, focussing on KPIs central to meeting M3/M4 "Kusama-performance" and "Polkadot-performance" -- where we need guidance on what this means exactly so it falls out of the telemetry server dataset.  What else is required?

id:
1675
timestamp:
2025-04-21T14:27:50.057Z
sender:
@sourabhniyogi:matrix.org
content:
* To support M3 PVM interpretation => recompilation, we should desire the timing (# of milliseconds) spent executing all the timings of the key PVM operations + erasure encoding + justification generation

- 0 \[block announcement for authoring\]
- 131/132 \[ticket generation\]
- 133/134/145 \[work package refinement/auditing\].
- 137/138/139/140 \[DA encoding\]
- 224/225 \[BLS Signature Generation\]

The telemetry server can usefully pipe this into Jaeger tracing (or similar) for visualization, but from attempting multihop XCM visualization, but it probably helps to also have work package hash (133/134/145/137/138/139/140) and header hash (0/192/193/224/225) so the telemetry server doesn't have to do any JAM-specific indexing.

With benchmark workpackages, a implementer leaderboard should be developed, focussing on KPIs central to meeting M3/M4 "Kusama-performance" and "Polkadot-performance" -- where we need guidance on what this means exactly so it falls out of the telemetry server dataset.  What else is required?

id:
1674
timestamp:
2025-04-21T14:48:01.450Z
sender:
@sourabhniyogi:matrix.org
content:
* To support M3 PVM interpretation => recompilation, we should desire the timing (# of milliseconds) spent executing all the timings of the key PVM operations + then do similar erasure encoding + justification generation

- 0 \[block announcement for authoring\]
- 131/132 \[ticket generation\]
- 133/134/145 \[work package refinement/auditing\].
- 137/138/139/140 \[DA encoding\]
- 224/225 \[BLS Signature Generation\]

The telemetry server can usefully pipe this into Jaeger tracing (or similar) for visualization, but from attempting multihop XCM visualization, but it probably helps to also have work package hash (133/134/145/137/138/139/140) and header hash (0/192/193/224/225) so the telemetry server doesn't have to do any JAM-specific indexing.

Having data streams to attack bottlenecks that motivate [RFC-139 - Faster Erasure Coding](https://github.com/ordian/RFCs/blob/main/text/0139-faster-erasure-coding.md) and  [super-I/O centric NOMT](https://github.com/thrumdev/nomt) would be cool.

With benchmark workpackages + services, a implementer leaderboard should be developed, focussing on KPIs central to meeting M3/M4 "Kusama-performance" and "Polkadot-performance" -- where we need guidance on what this means exactly so it falls out of the telemetry server dataset.  What else is required?

id:
1673
timestamp:
2025-04-21T14:51:55.964Z
sender:
@sourabhniyogi:matrix.org
content:
* To support M3 PVM interpretation => recompilation, we should desire the timing (# of milliseconds) spent executing all the timings of the key PVM operations + then do similar erasure encoding + justification generation

- 0 \[block announcement for authoring\]
- 131/132 \[ticket generation\]
- 133/134/145 \[work package refinement/auditing\].
- 137/138/139/140 \[DA encoding\]
- 224/225 \[BLS Signature Generation\]

The telemetry server can usefully pipe this into Jaeger tracing (or similar) for visualization, but from attempting multihop XCM visualization, it probably helps to also have work package hash (133/134/145/137/138/139/140) and header hash (0/192/193/224/225) so the telemetry server doesn't have to do any JAM-specific indexing -- then you can see a "trace" for any workpackage (refining and auditing) or a header hash (authoring, validating, finalizing).  

Having data streams to attack bottlenecks that motivate [RFC-139 - Faster Erasure Coding](https://github.com/ordian/RFCs/blob/main/text/0139-faster-erasure-coding.md) and  [super-I/O centric NOMT](https://github.com/thrumdev/nomt) would be cool.

With benchmark workpackages + services, a implementer leaderboard should be developed, focussing on KPIs central to meeting M3/M4 "Kusama-performance" and "Polkadot-performance" -- where we need guidance on what this means exactly so it falls out of the telemetry server dataset.  What else is required?

id:
1672
timestamp:
2025-04-21T15:00:10.906Z
sender:
@jaymansfield:matrix.org
content:
What is 224/225 referring to here?

id:
1671
timestamp:
2025-04-21T15:46:35.681Z
sender:
@jaymansfield:matrix.org
content:
Yes it is sending statistics for each block. I noticed one thing not defined in JIP-2 and I'm wondering if it’s related to that.

Initial subscription:
{“jsonrpc":"2.0","id":1,"method":"subscribeStatistics","params":[false]}

Response:
{“jsonrpc":"2.0","id":1,"result":"7c0f6de3-75f6-4056-b24f-da377bb00a09"}

Notification:
<- {“jsonrpc":"2.0","method":"subscribeStatistics","params":{"subscription":["7c0f6de3-75f6-4056-b24f-da377bb00a09"],"result”:[[0,0,0,……]]}}

For the actual notifications/subscription events, what should the “method” be populated as? Right now for these I’m using the original method they subscribed too , for example: “subscribeStatistics” as there doesn’t seem to be a standard set in the JSON-RPC spec. Some other RPC implementations use method: “subscription”, “notification”, “event”, etc

id:
1670
timestamp:
2025-04-21T15:46:54.012Z
sender:
@jaymansfield:matrix.org
content:
* Yes it is sending statistics for each block. I noticed one thing not defined in JIP-2 and I'm wondering if it’s related to that.

Initial subscription:
{“jsonrpc":"2.0","id":1,"method":"subscribeStatistics","params":\[false\]}

Response:
{“jsonrpc":"2.0","id":1,"result":"7c0f6de3-75f6-4056-b24f-da377bb00a09"}

Notification:
{“jsonrpc":"2.0","method":"subscribeStatistics","params":{"subscription":\["7c0f6de3-75f6-4056-b24f-da377bb00a09"\],"result”:\[\[0,0,0,……\]\]}}

For the actual notifications/subscription events, what should the “method” be populated as? Right now for these I’m using the original method they subscribed too , for example: “subscribeStatistics” as there doesn’t seem to be a standard set in the JSON-RPC spec. Some other RPC implementations use method: “subscription”, “notification”, “event”, etc

id:
1669
timestamp:
2025-04-21T15:47:56.532Z
sender:
@jaymansfield:matrix.org
content:
* Yes it is sending statistics for each block. I noticed one thing not defined in JIP-2 and I'm wondering if it’s related to that.

Initial subscription:
{“jsonrpc":"2.0","id":1,"method":"subscribeStatistics","params":\[false\]}

Response:
{“jsonrpc":"2.0","id":1,"result":"7c0f6de3-75f6-4056-b24f-da377bb00a09"}

Notification:
{“jsonrpc":"2.0","method":"subscribeStatistics","params":{"subscription":\["7c0f6de3-75f6-4056-b24f-da377bb00a09"\],"result”:\[\[0,0,0,……\]\]}}

For the actual notifications/subscription events, what should the “method” be populated as? Right now for these I’m using the original method they subscribed too , for example: “subscribeStatistics” as there doesn’t seem to be a standard set in the JSON-RPC spec for this and seems it's up to the implementation. Some other RPC implementations use method: “subscription”, “notification”, “event”, etc

id:
1668
timestamp:
2025-04-21T15:49:09.684Z
sender:
@jaymansfield:matrix.org
content:
* Yes it is sending statistics for each block. I noticed one thing not defined in JIP-2 and I'm wondering if it’s related to that.

Initial subscription:
{“jsonrpc":"2.0","id":1,"method":"subscribeStatistics","params":\[false\]}

Response:
{“jsonrpc":"2.0","id":1,"result":"7c0f6de3-75f6-4056-b24f-da377bb00a09"}

Notification:
{“jsonrpc":"2.0","method":"subscribeStatistics","params":{"subscription":\["7c0f6de3-75f6-4056-b24f-da377bb00a09"\],"result”:\[\[0,0,0,……\]\]}}

For the actual notifications/subscription events, what should the “method” be populated as? Right now for these I’m using the original method they subscribed too , for example: “subscribeStatistics” as there doesn’t seem to be a standard set in the JSON-RPC spec for this and seems it's up to the implementation. I also tried just "statistics". Some other RPC implementations use method: “subscription”, “notification”, “event”, etc

id:
1667
timestamp:
2025-04-21T15:50:42.937Z
sender:
@jaymansfield:matrix.org
content:
* Yes it is sending statistics for each block. I noticed one thing not defined in JIP-2 and I'm wondering if it’s related to that.

Initial subscription:
{“jsonrpc":"2.0","id":1,"method":"subscribeStatistics","params":\[false\]}

Response:
{“jsonrpc":"2.0","id":1,"result":"7c0f6de3-75f6-4056-b24f-da377bb00a09"}

Notification:
{“jsonrpc":"2.0","method":"subscribeStatistics","params":{"subscription":\["7c0f6de3-75f6-4056-b24f-da377bb00a09"\],"result”:\[\[0,0,0,……\]\]}}

For the actual notifications/subscription events, what should the “method” be populated as? Right now for these I’m using the original method they subscribed too , for example: “subscribeStatistics” as there doesn’t seem to be a standard set in the JSON-RPC spec for this and seems it's up to the implementation. I also tried just "statistics". Some other RPC implementations use method: “subscription”, “notification”, “event”, etc but not sure what jamtop is expecting here.

id:
1666
timestamp:
2025-04-21T16:16:54.855Z
sender:
@sourabhniyogi:matrix.org
content:
Some placeholders for [Section 18](https://graypaper.fluffylabs.dev/#/5f542d7/1e78011e7801?v=0.6.2) that haven't been fully worked out yet.  Just a total guess on my part but its been modeled in the GP some already.

id:
1665
timestamp:
2025-04-21T16:17:58.657Z
sender:
@jaymansfield:matrix.org
content:
Thanks!

id:
1664
timestamp:
2025-04-21T16:18:42.999Z
sender:
@jaymansfield:matrix.org
content:
Btw, gamifying the optimization process for M3 with a leaderboard is a great idea.

id:
1663
timestamp:
2025-04-21T18:10:40.893Z
sender:
@sourabhniyogi:matrix.org
content:
* To support M3 PVM interpretation => recompilation, we should desire the timing (# of microseconds) spent executing all the timings of the key PVM operations + then do similar erasure encoding + justification generation

- 0 \[block announcement for authoring\]
- 131/132 \[ticket generation\]
- 133/134/145 \[work package refinement/auditing\].
- 137/138/139/140 \[DA encoding\]
- 224/225 \[BLS Signature Generation\]

The telemetry server can usefully pipe this into Jaeger tracing (or similar) for visualization, but from attempting multihop XCM visualization, it probably helps to also have work package hash (133/134/145/137/138/139/140) and header hash (0/192/193/224/225) so the telemetry server doesn't have to do any JAM-specific indexing -- then you can see a "trace" for any workpackage (refining and auditing) or a header hash (authoring, validating, finalizing).

Having data streams to attack bottlenecks that motivate [RFC-139 - Faster Erasure Coding](https://github.com/ordian/RFCs/blob/main/text/0139-faster-erasure-coding.md) and  [super-I/O centric NOMT](https://github.com/thrumdev/nomt) would be cool.

With benchmark workpackages + services, a implementer leaderboard should be developed, focussing on KPIs central to meeting M3/M4 "Kusama-performance" and "Polkadot-performance" -- where we need guidance on what this means exactly so it falls out of the telemetry server dataset.  What else is required?

id:
641
timestamp:
2025-04-21T21:25:05.975Z
sender:
@ascriv:matrix.org
content:
I think the GP is not clear on how to serialize elements of N? And therefore, e.g. state serialization (D.2) is not clear on how to serialize the validator statistics component C, which has components in N

id:
640
timestamp:
2025-04-21T21:47:04.282Z
sender:
@danicuki:matrix.org
content:
N you mean integers? 

id:
639
timestamp:
2025-04-21T21:47:24.851Z
sender:
@ascriv:matrix.org
content:

 bold N , naturals

id:
638
timestamp:
2025-04-21T21:49:14.373Z
sender:
@danicuki:matrix.org
content:
Formula C.6 is how you serialize numbers in N

id:
637
timestamp:
2025-04-21T21:53:26.382Z
sender:
@ascriv:matrix.org
content:
Makes sense, wanted to make sure we’re intentionally using the general natural serialization here instead of accidentally not specifying the subscript. Since usually we only use the general one for the length discriminator 

id:
636
timestamp:
2025-04-21T21:55:01.368Z
sender:
@ascriv:matrix.org
content:
It’s just not very consistent. Also in C(13) we are clear to use E_4 for pi_V and pi_L which have components in bold N 

id:
635
timestamp:
2025-04-21T21:55:09.516Z
sender:
@danicuki:matrix.org
content:
> <@ascriv:matrix.org> Makes sense, wanted to make sure we’re intentionally using the general natural serialization here instead of accidentally not specifying the subscript. Since usually we only use the general one for the length discriminator 

Yes. It is used for the first time in latest version of GP

id:
634
timestamp:
2025-04-21T21:56:57.262Z
sender:
@danicuki:matrix.org
content:
> <@ascriv:matrix.org> It’s just not very consistent. Also in C(13) we are clear to use E_4 for pi_V and pi_L which have components in bold N 

It is used to save storage space, as statistics number can grow in definetly (or not). 

id:
633
timestamp:
2025-04-21T21:57:13.692Z
sender:
@danicuki:matrix.org
content:
> <@ascriv:matrix.org> It’s just not very consistent. Also in C(13) we are clear to use E_4 for pi_V and pi_L which have components in bold N 

 * It is used to save storage space, as statistics number can grow indefinetly (or not).

id:
632
timestamp:
2025-04-21T21:58:34.351Z
sender:
@ascriv:matrix.org
content:
But couldn’t pi_V and _L also? Yet we implicitly limit them based on the usage of E_4 serializing them

id:
1662
timestamp:
2025-04-21T23:19:59.126Z
sender:
@sourabhniyogi:matrix.org
content:
Ok, thanks for the encouragement -- I'll make a Dune dashboard out of it before Lisbon. 

id:
631
timestamp:
2025-04-22T11:22:43.272Z
sender:
@gav:polkadot.io
content:
The lack of subscript is intentional.

id:
630
timestamp:
2025-04-22T11:25:14.175Z
sender:
@gav:polkadot.io
content:
Yes, the encoding of integers is not presently entirely uniform. Some are encoded for size savings (statistics, where there's relatively a lot of data in a place where bandwidth is very tight), others for the ability to swiftly/efficiently decode (e.g. in PVM I/O). It will be reviewed during the 0.6 series under this issue https://github.com/gavofyork/graypaper/issues/293.

id:
629
timestamp:
2025-04-22T11:26:50.030Z
sender:
@gav:polkadot.io
content:
pi_V and pi_L might yet be changed, or possibly pi_C/pi_S.

id:
628
timestamp:
2025-04-22T11:26:59.105Z
sender:
@gav:polkadot.io
content:
* The encoding used for pi\_V and pi\_L might yet be changed, or possibly pi\_C/pi\_S.

id:
627
timestamp:
2025-04-22T11:27:12.171Z
sender:
@gav:polkadot.io
content:
Again, the above issue will resolve this.

id:
626
timestamp:
2025-04-22T11:28:20.895Z
sender:
@gav:polkadot.io
content:
GP [v0.6.5](https://github.com/gavofyork/graypaper/releases/tag/v0.6.5) is out.

id:
625
timestamp:
2025-04-22T11:29:41.770Z
sender:
@gav:polkadot.io
content:
It's mostly corrections, with two small protocol alterations:
- There's now a gas limit in the accumulation operand tuple.
- There's a new host-call to allow services to provide preimages to other services directly without going through the regular off-chain preimage process.

id:
624
timestamp:
2025-04-22T12:34:04.999Z
sender:
@jay_ztc:matrix.org
content:
Got a few minor questions to confirm my understanding of the PVM spec-> 

1. If c[0] isn't a valid opcode, this would result in a runtime error, *if and only if* the program attempts to execute c[0], correct?

2. If an instruction (this time corresponding to an index from the bitmask, k) contains an invalid opcode, this would result in a runtime error, *if and only if* the program attempts to execute it (runtime), correct?

3. if the skip length function doesn't find any bitmask-marked opcodes within the subsequent 24 octets, this bumps the pc to current+24 correct? So if there happened to be a valid opcode & args at that new pc, it would continue executing at the new pc in the same manner as if it were marked by the instruction bitmask, correct?


 Jan Bujak 

id:
623
timestamp:
2025-04-22T12:34:40.460Z
sender:
@jay_ztc:matrix.org
content:
* Got a few minor questions to confirm my understanding of the PVM spec->

1. If c\[0\] isn't a valid opcode, this would result in a runtime error, _if and only if_ the program attempts to execute c\[0\], correct?
2. If an instruction (this time corresponding to an index from the bitmask, k) contains an invalid opcode, this would result in a runtime error, _if and only if_ the program attempts to execute it, correct?
3. if the skip length function doesn't find any bitmask-marked opcodes within the subsequent 24 octets, this bumps the pc to current+24 correct? So if there happened to be a valid opcode & args at that new pc, it would continue executing at the new pc in the same manner as if it were marked by the instruction bitmask, correct?

Jan Bujak

id:
622
timestamp:
2025-04-22T12:34:59.552Z
sender:
@jay_ztc:matrix.org
content:
* Got a few minor questions to confirm my understanding of the PVM spec->

1. If c\[0\] isn't a valid opcode, this would result in a runtime error, _if and only if_ the program attempts to execute c\[0\], correct?
2. If an instruction (this time corresponding to an index from the bitmask, k) contains an invalid opcode, this would result in a runtime error, _if and only if_ the program attempts to execute it, correct?
3. if the skip length function doesn't find any bitmask-marked opcodes within the subsequent 24 octets, this bumps the pc to current+24. So if there happened to be a valid opcode & args at that new pc, it would continue executing at the new pc in the same manner as if it were marked by the instruction bitmask, correct?

Jan Bujak

id:
621
timestamp:
2025-04-22T12:35:44.080Z
sender:
@jay_ztc:matrix.org
content:
* Got a few minor questions about PVM edge cases->

1. If c\[0\] isn't a valid opcode, this would result in a runtime error, _if and only if_ the program attempts to execute c\[0\], correct?
2. If an instruction (this time corresponding to an index from the bitmask, k) contains an invalid opcode, this would result in a runtime error, _if and only if_ the program attempts to execute it, correct?
3. if the skip length function doesn't find any bitmask-marked opcodes within the subsequent 24 octets, this bumps the pc to current+24. So if there happened to be a valid opcode & args at that new pc, it would continue executing at the new pc in the same manner as if it were marked by the instruction bitmask, correct?

Jan Bujak

id:
620
timestamp:
2025-04-22T12:35:58.996Z
sender:
@jay_ztc:matrix.org
content:
* Got a few small questions about PVM edge cases->

1. If c\[0\] isn't a valid opcode, this would result in a runtime error, _if and only if_ the program attempts to execute c\[0\], correct?
2. If an instruction (this time corresponding to an index from the bitmask, k) contains an invalid opcode, this would result in a runtime error, _if and only if_ the program attempts to execute it, correct?
3. if the skip length function doesn't find any bitmask-marked opcodes within the subsequent 24 octets, this bumps the pc to current+24. So if there happened to be a valid opcode & args at that new pc, it would continue executing at the new pc in the same manner as if it were marked by the instruction bitmask, correct?

Jan Bujak

id:
619
timestamp:
2025-04-22T12:36:31.867Z
sender:
@jay_ztc:matrix.org
content:
* Got a few small questions about PVM edge cases, to confirm my understanding->

1. If c\[0\] isn't a valid opcode, this would result in a runtime error, _if and only if_ the program attempts to execute c\[0\], correct?
2. If an instruction (this time corresponding to an index from the bitmask, k) contains an invalid opcode, this would result in a runtime error, _if and only if_ the program attempts to execute it, correct?
3. if the skip length function doesn't find any bitmask-marked opcodes within the subsequent 24 octets, this bumps the pc to current+24. So if there happened to be a valid opcode & args at that new pc, it would continue executing at the new pc in the same manner as if it were marked by the instruction bitmask, correct?

Jan Bujak

id:
618
timestamp:
2025-04-22T12:55:35.544Z
sender:
@jay_ztc:matrix.org
content:
* Got a few small questions about PVM edge cases, to confirm my understanding->

1. If c\[0\] isn't a valid opcode, this would result in a panic, _if and only if_ the program attempts to execute c\[0\], correct?
2. If an instruction (this time corresponding to an index from the bitmask, k) contains an invalid opcode, this would result in a panic, _if and only if_ the program attempts to execute it, correct?
3. if the skip length function doesn't find any bitmask-marked opcodes within the subsequent 24 octets, this bumps the pc to current+24. So if there happened to be a valid opcode & args at that new pc, it would continue executing at the new pc in the same manner as if it were marked by the instruction bitmask, correct?

Jan Bujak

id:
617
timestamp:
2025-04-22T13:00:33.687Z
sender:
@jan:parity.io
content:
Yes, currently "invalid" instructions don't make the program invalid, and only have an effect if executed. Yes, if there's no `1` found in the bitmask then the skip is assumed to be 24. Note that IIRC currently even if the next instruction has `0` in its opcode bitmask bit (i.e. the next bit after the last bit that the bitmask scan checks) the instruction will also be executed, but this is not intended behavior and is on my TODO list to add to the GP that every instruction must have a `1` in the bitmask to be considered valid (otherwise we'll run into some nasty corner cases).

id:
616
timestamp:
2025-04-22T13:00:51.103Z
sender:
@jan:parity.io
content:
* Yes, currently "invalid" instructions don't make the program invalid, and only have an effect if executed (they're effectively treated as a trap). Yes, if there's no `1` found in the bitmask then the skip is assumed to be 24. Note that IIRC currently even if the next instruction has `0` in its opcode bitmask bit (i.e. the next bit after the last bit that the bitmask scan checks) the instruction will also be executed, but this is not intended behavior and is on my TODO list to add to the GP that every instruction must have a `1` in the bitmask to be considered valid (otherwise we'll run into some nasty corner cases).

id:
615
timestamp:
2025-04-22T13:04:56.702Z
sender:
@jay_ztc:matrix.org
content:
Many thanks for the quick response 🙏, your insight on the TODO is much appreciated. enjoy your evening.

id:
614
timestamp:
2025-04-22T13:08:41.917Z
sender:
@jan:parity.io
content:
Also, while we're at the topic parsing, (this is going to be relevant to people aiming for M3 and M4) in case you're wondering why the limit is 24 - it was deliberately picked to allow for fast parsing. To parse a PVM instruction at a given position you need to read only two values from memory: a 128-bit integer from the instructions slice, and a 32-bit integer from the bitmask slice, and then you can easily parse it in an efficient manner with bitshifts etc. (for the bitmask one can use the "leading zeros" intrinsic/method which is a single assembly instruction on modern CPUs to cheaply get the skip to get to the next instruction, hence the maximum is 24)

id:
613
timestamp:
2025-04-22T13:10:07.440Z
sender:
@jay_ztc:matrix.org
content:
corner case of the corner case might be a  'greater than 24 octet' break in instructions after a valid program termination instruction-> (ie is the subsequent 'valid' instruction also a 'valid' jump target, or is the jump target the 'invalid' 24th octet)

id:
611
timestamp:
2025-04-22T13:12:45.764Z
sender:
@jan:parity.io
content:
IIRC the jumps actually check that the previous instruction has the `1` set in its bitmask (because you can only jump either to offset=0 or after block terminators)

id:
612
timestamp:
2025-04-22T13:12:48.897Z
sender:
@jay_ztc:matrix.org
content:
* corner case of the corner case might be a  'greater than 24 octet' break in instructions after a valid basic block termination instruction-> (ie is the subsequent 'valid' instruction also a 'valid' jump target, or is the jump target the 'invalid' 24th octet)

edit: 'program termination' -> 'basic block'

id:
610
timestamp:
2025-04-22T13:13:06.945Z
sender:
@jay_ztc:matrix.org
content:
* corner case of the corner case might be a  'greater than 24 octet' break in instructions after a valid basic block termination instruction-> (ie is the subsequent 'valid' instruction also a 'valid' jump target, or is the jump target the 'invalid' 24th octet)

edit: 'program termination' -> 'basic block' (brain fart)

id:
609
timestamp:
2025-04-22T13:14:24.671Z
sender:
@jan:parity.io
content:
So with the current way things are you might get a paradoxical block terminator that would execute when you arrive at it from the previous instruction, but wouldn't be a valid target for a jump.

id:
608
timestamp:
2025-04-22T13:14:43.495Z
sender:
@jan:parity.io
content:
(again, I want to require all instructions to have 1 set in its bitmask to prevent such potential corner cases)

id:
607
timestamp:
2025-04-22T13:15:56.420Z
sender:
@jan:parity.io
content:
* So with the current way things are you might get a paradoxical block terminator that would execute when you arrive at it from the previous instruction, but the next instruction wouldn't be a valid target for a jump.

id:
606
timestamp:
2025-04-22T13:22:23.123Z
sender:
@jay_ztc:matrix.org
content:
* jumps target indexes in w (basic block index set), which is calculated by applying the skip-length function to each opcode index in the k bitmask*-> so this would result in a jump target of the 24th octet (our 'invalid' instruction)

edit: *k also includes c[0] always

id:
605
timestamp:
2025-04-22T13:22:36.161Z
sender:
@jay_ztc:matrix.org
content:
* jumps target indexes in w (basic block index set), which is calculated by applying the skip-length function to each opcode index in the k bitmask\*-> so this would result in a jump target of the 24th octet (our 'invalid' instruction)

edit: \*k always includes c\[0\]

id:
604
timestamp:
2025-04-22T13:23:06.218Z
sender:
@jay_ztc:matrix.org
content:
* jumps target indexes in w (basic block index set), which is calculated by applying the skip-length function to each opcode index in the k bitmask\*-> so this would result in a jump target of the 24th octet (our 'invalid' instruction)



edit: \*additionally, k always includes c\[0\]

id:
603
timestamp:
2025-04-22T13:23:15.885Z
sender:
@jay_ztc:matrix.org
content:
* jumps target indexes in w (basic block index set), which is calculated by applying the skip-length function to each opcode index in the k bitmask\*-> so this would result in a jump target of the 24th octet (our 'invalid' instruction).


edit: \*additionally, k always includes c\[0\]

id:
602
timestamp:
2025-04-22T13:24:31.764Z
sender:
@jay_ztc:matrix.org
content:
* jumps target indexes in w (basic block index set), which is calculated by applying the skip-length function to each opcode index in the k bitmask\*-> so this would result in a jump target of the 24th octet (our 'invalid' instruction).

edit: *additionally, k always includes offset=0 (c[0])

id:
601
timestamp:
2025-04-22T13:30:13.158Z
sender:
@jay_ztc:matrix.org
content:
actually, looks like both branch & jump use the 'beginning of basic blocks' collection, *w* (which itself uses the skip-length).

id:
600
timestamp:
2025-04-22T13:37:04.267Z
sender:
@jay_ztc:matrix.org
content:
Didn't intend for this chat to continue into your off-hours, my apologies. Don't worry about looking at this tonight especially given its a non-urgent corner case.

id:
1661
timestamp:
2025-04-22T14:10:56.979Z
sender:
@jay_ztc:matrix.org
content:
Many thanks for organizing this event 🙏. When can we expect approvals to start being processed for this registration?

id:
599
timestamp:
2025-04-22T14:32:29.235Z
sender:
@knight1205:matrix.org
content:
Here: https://graypaper.fluffylabs.dev/#/68eaa1f/1bcb011b1702?v=0.6.4

In Eqn 14.14 and 14.15 we are using b which is defined as all the segments exported by all work packages, 
and then we are calculating it's merkle root and comparing with segment root of some previously exported segments of any work package. 

Is this what it implies? As segments exported from a work package leads to different segment roots, then in that case how can it be same as segment root of all the segments (from all wps)?

id:
598
timestamp:
2025-04-22T14:32:57.226Z
sender:
@knight1205:matrix.org
content:
* Here: https://graypaper.fluffylabs.dev/#/68eaa1f/1bcb011b1702?v=0.6.4

In Eqn 14.14 and 14.15 we are using s (now b) which is defined as all the segments exported by all work packages, 
and then we are calculating it's merkle root and comparing with segment root of some previously exported segments of any work package. 

Is this what it implies? As segments exported from a work package leads to different segment roots, then in that case how can it be same as segment root of all the segments (from all wps)?

id:
597
timestamp:
2025-04-22T18:33:48.940Z
sender:
@sourabhniyogi:matrix.org
content:
We have found our development life has improved with the "metadata" attached to preimages, specifically when the preimage is for service code, because we can then have tools show that metadata as strings like "fib" and "gameoflife".   Can we get the same for workpackages+bundles so that we can attach metadata like "fib(93)", "gameoflife(314)", or should that be in the payload as a mere convention?

id:
596
timestamp:
2025-04-22T19:56:37.193Z
sender:
@sourabhniyogi:matrix.org
content:
Another quick request: Would you mind assigning `provide` a number like 17 or 27, and perhaps starting historical_lookup at 32 or 64 or 128? That would leave a bit of space between the two big groups of host functions for "one more".

id:
1660
timestamp:
2025-04-22T19:58:38.637Z
sender:
@wabkebab:matrix.org
content:
I don't have the exact date but it will be very soon. You can approach Mila | Pala Labs as she can provide you more details about it

id:
595
timestamp:
2025-04-22T20:15:49.884Z
sender:
@sourabhniyogi:matrix.org
content:
* Another quick request: Would you mind assigning `provide` a number like 17 or 27, and perhaps starting historical\_lookup at 32 or 64 or 128? That would leave a bit of space between the two big groups of host functions for "one more".

Wondering why there is no a_l adjustment similar to [12.40](https://graypaper.fluffylabs.dev/#/68eaa1f/18a50118a501?v=0.6.4) for this new `provide`-based preimage submission -- I would expect a_l to have the accumulate's timeslot.  

id:
594
timestamp:
2025-04-22T20:21:16.116Z
sender:
@sourabhniyogi:matrix.org
content:
* Another quick request: Would you mind assigning `provide` a number like 17 or 27, and perhaps starting historical\_lookup at 32 or 64 or 128? That would leave a bit of space between the two big groups of host functions for "one more".

Wondering why there is no a\_l adjustment similar to [12.40](https://graypaper.fluffylabs.dev/#/68eaa1f/18a50118a501?v=0.6.4) for this new `provide`-based preimage submission -- I would expect a\_l to have the accumulate's timeslot.  How could this `provide`d preimage be followed by a `forget` and by which service?

id:
1659
timestamp:
2025-04-23T04:47:58.900Z
sender:
@ewaddicor:matrix.org
content:
I'm thinking about submitting a JAM interest form... does anyone know which programming languages have not yet been used?

id:
1658
timestamp:
2025-04-23T04:51:40.407Z
sender:
@xlchen:matrix.org
content:
this is a non-complete list of what languages are used https://graypaper.com/clients/

id:
1657
timestamp:
2025-04-23T04:54:35.196Z
sender:
@ewaddicor:matrix.org
content:
thank you

id:
593
timestamp:
2025-04-23T13:24:02.320Z
sender:
@jay_ztc:matrix.org
content:
Jan Bujak:  I found this very helpful post of yours in a thread from last September. Do you think holding off on implementing sbrk is still relevant advice? Do you have any new thoughts since this message was posted that you can share?

id:
592
timestamp:
2025-04-23T13:27:12.371Z
sender:
@jay_ztc:matrix.org
content:
for some reason, matrix isn't showing the original post-> so I'm reposing here::

https://matrix.to/#/!ddsEwXlCWnreEGuqXZ:polkadot.io/$_RkIlMDNZrROw_6WDXpbllO2VSbjY1FNTIfDjVZhhdw?via=polkadot.io&via=matrix.org&via=parity.io

Memory allocation/deallocation handling is still a work-in-progress, and it's possible the sbrk instruction will get modified and/or removed. I'd suggest you temporarily skip it and focus on other parts of JAM and/or PVM.

If you're interested in some history as to why sbrk is there then let me give you some background.

Historically I designed PolkaVM (on which PVM in the GP is based on) to be a VM which is as "powerful" as WASM VMs (so it can completely replace our current WASM-based executor in Polkadot 1.0 and our WASM-based smart contracts VM) while being as simple as possible to implement, and without sacrificing any performance.
So this is where the idea for the sbrk came from (which is similar to what WASM has): the VM maintains a heap pointer, and the guest program can use sbrk to query that pointer and/or to bump it up. And every time it crosses a page boundary the VM allocates new memory for the program.

So this design has numerous benefits. First, it's very simple to use as a guest program (pseudo code):
```
// Get a pointer to the new allocation.
let pointer = sbrk(0);
// Actually allocate it.
if sbrk(size) != 0 {
    // Allocation succeeded.
    // Now `pointer` points to `size` bytes you can use.
}
```

This is also great for use cases like e.g. tiny smart contracts which can use this as directly as an allocator without having to bring a heavyweight allocator of their own (which would consume a lot of space).

Secondly, it's simple to implement in the VM, something like that (pseudo code again):
```
fn sbrk(size) -> Pointer {
    if size == 0 {
        // The guest wants to know the current heap pointer.
        return current_heap_pointer;
    }

    // The guest wants to allocate.
    let new_heap_pointer = current_heap_pointer + size;
    if new_heap_pointer > max_heap_pointer {
        // Allocation failed.
        return 0;
    }


    let next_page_boundary = align_to_page_size(current_heap_pointer);
    if new_heap_pointer > next_page_boundary {
        allocate_new_pages(next_page_boundary..align_to_page_size(new_heap_pointer));
    }

    current_heap_pointer += size;
    return current_heap_pointer;
```

And this (along with the memory map I came up with, which is what we now call "standard program initialization") also makes in very easy to write an interpreter for this, because when handling loads/stores from memory you only have to do something like this:
```
fn load_value32(address) -> value {
    if address >= stack_address && address + 4 <= stack_address_end {
        return stack[address - stack_address];
    } else if address >= rw_data_address && address + 4 <= align_to_page_size(current_heap_pointer) {
        return rw_data[address - stack_address];
    } else if address >= ro_data_address && address + 4 <= ro_data_address_end {
        return ro_data[address - stack_address];
    } else {
        // Address is inaccessible.
        return Err;
    }
}
```

It's cheap, fast, and doesn't require any crazy data structures and doesn't require any handling of corner cases (for example, accesses which could read both from the stack and from RW data don't have to be handled, because they're impossible by definition; the interpreter can just keep them in separate arrays, and call it a day).

So that's how (any why) it was originally designed, but then came JAM and changed things. (: (Again, remember, I started working on this before JAM, and some things were just grandfathered into JAM.)

What JAM introduces is a concept of inner VMs (see machine, peek, poke, invoke and expunge host functions in section B.8 of the GP) where one VM can spawn another VM, and as it is currently designed those inner VMs are extremely flexible and have completely free-form memories and are dynamically paged.
What this essentially means is that all of those nice properties of sbrk that I've listed - simple and easy to implement, fast, doesn't require fancy data structures - they all now go out of the window!

So we will probably be replacing sbrk with something else that's more appropriate for the more flexible inner VM model. And unfortunately also most likely orders of magnitude harder to implement (at least if you want to reach at least the half-speed milestone), but it is what it is. I'm still finishing some other stuff up, but I'll most likely be working on this soon-ish. (If any of you have any good and/or crazy ideas feel free to message me!)

id:
591
timestamp:
2025-04-23T13:27:52.237Z
sender:
@jay_ztc:matrix.org
content:
* reposting below for context::

https://matrix.to/#/!ddsEwXlCWnreEGuqXZ:polkadot.io/$\_RkIlMDNZrROw\_6WDXpbllO2VSbjY1FNTIfDjVZhhdw?via=polkadot.io&via=matrix.org&via=parity.io

Memory allocation/deallocation handling is still a work-in-progress, and it's possible the sbrk instruction will get modified and/or removed. I'd suggest you temporarily skip it and focus on other parts of JAM and/or PVM.

If you're interested in some history as to why sbrk is there then let me give you some background.

Historically I designed PolkaVM (on which PVM in the GP is based on) to be a VM which is as "powerful" as WASM VMs (so it can completely replace our current WASM-based executor in Polkadot 1.0 and our WASM-based smart contracts VM) while being as simple as possible to implement, and without sacrificing any performance.
So this is where the idea for the sbrk came from (which is similar to what WASM has): the VM maintains a heap pointer, and the guest program can use sbrk to query that pointer and/or to bump it up. And every time it crosses a page boundary the VM allocates new memory for the program.

So this design has numerous benefits. First, it's very simple to use as a guest program (pseudo code):

```
// Get a pointer to the new allocation.
let pointer = sbrk(0);
// Actually allocate it.
if sbrk(size) != 0 {
    // Allocation succeeded.
    // Now `pointer` points to `size` bytes you can use.
}
```

This is also great for use cases like e.g. tiny smart contracts which can use this as directly as an allocator without having to bring a heavyweight allocator of their own (which would consume a lot of space).

Secondly, it's simple to implement in the VM, something like that (pseudo code again):

```
fn sbrk(size) -> Pointer {
    if size == 0 {
        // The guest wants to know the current heap pointer.
        return current_heap_pointer;
    }

    // The guest wants to allocate.
    let new_heap_pointer = current_heap_pointer + size;
    if new_heap_pointer > max_heap_pointer {
        // Allocation failed.
        return 0;
    }


    let next_page_boundary = align_to_page_size(current_heap_pointer);
    if new_heap_pointer > next_page_boundary {
        allocate_new_pages(next_page_boundary..align_to_page_size(new_heap_pointer));
    }

    current_heap_pointer += size;
    return current_heap_pointer;
```

And this (along with the memory map I came up with, which is what we now call "standard program initialization") also makes in very easy to write an interpreter for this, because when handling loads/stores from memory you only have to do something like this:

```
fn load_value32(address) -> value {
    if address >= stack_address && address + 4 <= stack_address_end {
        return stack[address - stack_address];
    } else if address >= rw_data_address && address + 4 <= align_to_page_size(current_heap_pointer) {
        return rw_data[address - stack_address];
    } else if address >= ro_data_address && address + 4 <= ro_data_address_end {
        return ro_data[address - stack_address];
    } else {
        // Address is inaccessible.
        return Err;
    }
}
```

It's cheap, fast, and doesn't require any crazy data structures and doesn't require any handling of corner cases (for example, accesses which could read both from the stack and from RW data don't have to be handled, because they're impossible by definition; the interpreter can just keep them in separate arrays, and call it a day).

So that's how (any why) it was originally designed, but then came JAM and changed things. (: (Again, remember, I started working on this before JAM, and some things were just grandfathered into JAM.)

What JAM introduces is a concept of inner VMs (see machine, peek, poke, invoke and expunge host functions in section B.8 of the GP) where one VM can spawn another VM, and as it is currently designed those inner VMs are extremely flexible and have completely free-form memories and are dynamically paged.
What this essentially means is that all of those nice properties of sbrk that I've listed - simple and easy to implement, fast, doesn't require fancy data structures - they all now go out of the window!

So we will probably be replacing sbrk with something else that's more appropriate for the more flexible inner VM model. And unfortunately also most likely orders of magnitude harder to implement (at least if you want to reach at least the half-speed milestone), but it is what it is. I'm still finishing some other stuff up, but I'll most likely be working on this soon-ish. (If any of you have any good and/or crazy ideas feel free to message me!)

id:
590
timestamp:
2025-04-23T15:22:36.859Z
sender:
@celadari:matrix.org
content:
Hello,

I have a question regarding the `eject` host function, specifically about the condition described here: https://graypaper.fluffylabs.dev/#/68eaa1f/328e03329103?v=0.6.4.

I want to confirm my understanding:
Does it mean that in order for a caller `s` to successfully eject account `d`, the `codeHash` of account `d` must be equal to the service index of the caller `s` ?

Thanks in advance for the clarification!

id:
589
timestamp:
2025-04-23T15:23:07.567Z
sender:
@celadari:matrix.org
content:
To be more precise:


id:
588
timestamp:
2025-04-23T15:23:10.948Z
sender:
@celadari:matrix.org
content:
image.png

id:
587
timestamp:
2025-04-23T16:45:41.667Z
sender:
@knight1205:matrix.org
content:
David Emett: Can you please clarify on this?

id:
586
timestamp:
2025-04-23T16:50:03.835Z
sender:
@dave:parity.io
content:
In S and J, M(s) = L(r) means s is the sequence of segments with the specified root

id:
585
timestamp:
2025-04-23T16:53:11.946Z
sender:
@knight1205:matrix.org
content:
but it is specified as all the segments exported by all the work packages exporting a segment. is this right? or this?

> s is the sequence of segments with the specified root



id:
584
timestamp:
2025-04-23T16:53:31.529Z
sender:
@dave:parity.io
content:
Not sure where you're getting that from?

id:
583
timestamp:
2025-04-23T16:54:17.877Z
sender:
@knight1205:matrix.org
content:
according to 0.6.5:


id:
582
timestamp:
2025-04-23T16:54:20.744Z
sender:
@knight1205:matrix.org
content:
image.png

id:
581
timestamp:
2025-04-23T16:54:41.330Z
sender:
@knight1205:matrix.org
content:
```
Note that while S and J are both formulated using the
inner term b (all segments exported by all work-packages
exporting a segment to be imported)
```
 

id:
580
timestamp:
2025-04-23T16:59:50.950Z
sender:
@dave:parity.io
content:
Hmm not sure about the wording of that sentence. The point there is that if a work-package A is importing a single segment exported by work-package B, you do not need to fetch _all_ of the segments exported by B

id:
579
timestamp:
2025-04-23T17:00:15.063Z
sender:
@dave:parity.io
content:
You only need to fetch the segment you care about, plus the corresponding proof segment

id:
578
timestamp:
2025-04-23T17:00:53.109Z
sender:
@knight1205:matrix.org
content:
got it. thanks for the clarification. Though I am still not sure about the wordings.

id:
577
timestamp:
2025-04-23T17:01:14.977Z
sender:
@dave:parity.io
content:
Yeah I think that wording could be improved

id:
576
timestamp:
2025-04-23T18:58:38.599Z
sender:
@charliewinston14:matrix.org
content:
Hello. Two questions about availability assurances.

1. Should these be sent out every slot even if the bitfield didn’t change from the previous distribution?


2. For an assurer to say a core is available in their assurance bitfield, does that mean they have access to just their shard matching their validator index, or are they saying they have access to ALL shards for a given core?


id:
575
timestamp:
2025-04-23T18:59:05.920Z
sender:
@charliewinston14:matrix.org
content:
* Hello. Two questions about availability assurances.

1. Should these be sent out every slot even if the bitfield didn’t change from the previous distribution?
2. For an assurer to say a core is available in their assurance bitfield, does that mean they have access to just the shards matching their validator index, or are they saying they have access to ALL shards for a given core?

id:
574
timestamp:
2025-04-23T19:02:35.577Z
sender:
@sourabhniyogi:matrix.org
content:
1. Yes, its critical that they are since assurances are anchored to a header hash. The only exception is if the entire bitfield is zero, in which case I don't believe there is a point to do so unless some reward exists for liveness when all cores are idle (which would be sad but possible!)
2. Just their shards matching their validator index.  If they had access to ALL the shards, that would be too much in the large!  

id:
573
timestamp:
2025-04-23T19:03:46.052Z
sender:
@sourabhniyogi:matrix.org
content:
* 1. Yes, its critical that they are since assurances are anchored to a header hash. The only exception is if the entire bitfield is zero, in which case I don't believe there is a point to do so unless some reward exists for liveness when all cores are idle (which would be sad but possible!)
2. Just their shards matching their validator index.  If they had access to ALL the shards, that would be too much in the large!  But of course they can do whatever they want, and they might be one of the guarantors.

id:
572
timestamp:
2025-04-23T19:04:57.289Z
sender:
@vinsystems:matrix.org
content:
Hi, question about the accumulate `transfer = 11` function. 

When Service A sends an amount X to Service B, a deferred transfer (i.e. `s: A, d: B, a: X, m: ..., g: ...`) is created and the amount to be sent [is substracted](https://graypaper.fluffylabs.dev/#/68eaa1f/325702325b02?v=0.6.4) from the Service A. 

But I cannot find in the GP where it says to update the balance of the receiver (service B).

I thought that it was done when [all the deferred effects of the transfers are applied](https://graypaper.fluffylabs.dev/#/68eaa1f/17a00417a004?v=0.6.4), but the `on_transfer` functions only modify the subject's account storage.

id:
571
timestamp:
2025-04-23T19:05:56.635Z
sender:
@vinsystems:matrix.org
content:
* Hi, question about the accumulate `transfer = 11` function.

When Service A sends an amount X to Service B, a deferred transfer (i.e. `s: A, d: B, a: X, m: ..., g: ...`) is created and the amount to be sent [is substracted](https://graypaper.fluffylabs.dev/#/68eaa1f/325702325b02?v=0.6.4) from the Service's A balance.

But I cannot find in the GP where it says to update the balance of the receiver (service B).

I thought that it was done when [all the deferred effects of the transfers are applied](https://graypaper.fluffylabs.dev/#/68eaa1f/17a00417a004?v=0.6.4), but the `on_transfer` functions only modify the subject's account storage.

id:
570
timestamp:
2025-04-23T19:21:41.119Z
sender:
@sourabhniyogi:matrix.org
content:
https://graypaper.fluffylabs.dev/#/cc517d7/2fe2002fe200?v=0.6.5

id:
569
timestamp:
2025-04-23T19:28:22.436Z
sender:
@vinsystems:matrix.org
content:
Thanks! ☺️ 

id:
568
timestamp:
2025-04-23T19:31:55.149Z
sender:
@sourabhniyogi:matrix.org
content:
* 1. Yes, its critical that they are since assurances are anchored to the immediate parent header hash. The only exception is if the entire bitfield is zero, in which case I don't believe there is a point to submitting an assurance unless some reward exists for liveness when all cores are idle (which would be sad ... but possible!)
2. Just their shards matching their validator index.  If they had access to ALL the shards, that would be too much in the large!  But of course they can do whatever they want, and they might be one of the guarantors.

id:
567
timestamp:
2025-04-23T19:35:10.419Z
sender:
@celadari:matrix.org
content:
* Hello,
I have a question regarding the eject host function, specifically about the condition described here: https://graypaper.fluffylabs.dev/#/68eaa1f/328e03329103?v=0.6.4.
I want to confirm my understanding:
Does it mean that in order for a caller s to successfully eject account d, the codeHash of account d must be equal to the (hash) service index of the caller s ?
Thanks in advance for the clarification!


id:
566
timestamp:
2025-04-23T19:42:52.476Z
sender:
@mkchung:matrix.org
content:
For CE140 justification: j++[b]++T(s,i,H), what exactly does the s in T(s, i, H) denote? Is it really the raw exported segment shards for the entire workpakge at given shard index? Without hashing the s term, the co-path in T(s, i, H) would grow linearly as the number of exports increases & each DA would need to store its sibling's entire "raw exported segment shards" as part of the co-path proof? 

id:
565
timestamp:
2025-04-23T19:44:11.695Z
sender:
@mkchung:matrix.org
content:
Let's say there are 100 exported segments for a given workpackage, can you provide an estimated justification size for requesting just one segment?  I'm estimating the justification to be somewhere around  66~410435 byte in tiny setting(where validator=6, W_P=1026) and around 297~3866 bytes in full setting(where validator=1023, W_P=6). Does my estimate look reasonable to you?

id:
564
timestamp:
2025-04-23T20:33:12.931Z
sender:
@jaymansfield:matrix.org
content:
"s is the full sequence of segment shards with the given shard index."

id:
563
timestamp:
2025-04-23T20:33:23.292Z
sender:
@jaymansfield:matrix.org
content:
The CE140 justification basically allows you to first calculate the segment root, which is then used to validate the erasure root

id:
562
timestamp:
2025-04-23T20:34:35.776Z
sender:
@jaymansfield:matrix.org
content:
* The CE140 justification basically allows you to first calculate the segment root for a given segment index, which is then used to validate the erasure root

id:
561
timestamp:
2025-04-23T22:02:56.832Z
sender:
@mkchung:matrix.org
content:
That's for the recepient/consumer of a CE140 justification. But the burden of producing such proof seems to fall on DA?

if you were the "generator" of  CE140 justification(i.e someone requested the justification from you), wouldn't you also need to fetch the raw "s" from your sibling's just to provide a co-path to the segment shard that you are responsible to store?

id:
560
timestamp:
2025-04-23T22:03:30.093Z
sender:
@mkchung:matrix.org
content:
I think I'm confused as why "S" not being hashed before building the segments root

id:
559
timestamp:
2025-04-24T03:47:49.422Z
sender:
@sourabhniyogi:matrix.org
content:
Found it here https://graypaper.fluffylabs.dev/#/cc517d7/17b00217b002?v=0.6.5

id:
558
timestamp:
2025-04-24T04:12:34.551Z
sender:
@sourabhniyogi:matrix.org
content:
* Another quick request: Would you mind assigning `provide` a number like 17 or 27, and perhaps starting historical\_lookup at 32 or 64 or 128? That would leave a bit of space between the two big groups of host functions for "one more".


id:
557
timestamp:
2025-04-24T04:12:46.225Z
sender:
@sourabhniyogi:matrix.org
content:
* Another quick request: Would you mind assigning `provide` a number like 17 or 27, and perhaps starting historical\_lookup at 32 or 64 or 128? That would leave a bit of space between the two big groups of host functions for "one more".


id:
556
timestamp:
2025-04-24T04:12:57.624Z
sender:
@sourabhniyogi:matrix.org
content:
* Another quick request: Would you mind assigning `provide` a number like 17 or 27, and perhaps starting historical\_lookup at 32 or 64 or 128? That would leave a bit of space between the two big groups of host functions for "one more".


id:
555
timestamp:
2025-04-24T04:13:08.523Z
sender:
@sourabhniyogi:matrix.org
content:
* Another quick request: Would you mind assigning `provide` a number like 17 or 27, and perhaps starting historical\_lookup at 32 or 64 or 128? That would leave a bit of space between the two big groups of host functions for "one more".

id:
554
timestamp:
2025-04-24T04:13:34.357Z
sender:
@sourabhniyogi:matrix.org
content:
* Found it here -sorryhttps://graypaper.fluffylabs.dev/#/cc517d7/17b00217b002?v=0.6.5

id:
553
timestamp:
2025-04-24T04:13:38.494Z
sender:
@sourabhniyogi:matrix.org
content:
* Found it here -sorry
https://graypaper.fluffylabs.dev/#/cc517d7/17b00217b002?v=0.6.5

id:
552
timestamp:
2025-04-24T04:35:18.974Z
sender:
@sourabhniyogi:matrix.org
content:
* Another quick request: Would you mind assigning `provide` a number like 17 or 27, and perhaps starting historical\_lookup at 32 or 64 or 128? That would leave a bit of space between the two big groups of host functions for "one more".


id:
551
timestamp:
2025-04-24T04:40:54.471Z
sender:
@jan:parity.io
content:
We are going to remove the sbrk instruction and replace it with a hostcall soon.

id:
1656
timestamp:
2025-04-24T08:53:51.316Z
sender:
@xlchen:matrix.org
content:
Does PolkaVM already have some benchmarking script that we can reuse to check the performance of our PVM implementation? I don't expect our current interpreter to be fast but I want to know how slow it is

id:
1655
timestamp:
2025-04-24T09:00:32.378Z
sender:
@jan:parity.io
content:
Yes, I have some benchmarks, but they might be tricky to run for you. (Making a universal harness for PVMs to be able to run them is on my TODO list.)

The benches are here: https://github.com/paritytech/polkavm/tree/master/guest-programs

The number are here: https://github.com/paritytech/polkavm/blob/master/BENCHMARKS.md

Here's the binary I use to run them: https://github.com/paritytech/polkavm/tree/master/tools/benchtool

I run these benchmarks across a lot of different VMs, so the tool has multiple backends: https://github.com/paritytech/polkavm/tree/master/tools/benchtool/src/backend
So in theory you could add your VM there.

id:
1654
timestamp:
2025-04-24T09:09:04.663Z
sender:
@jan:parity.io
content:
You could parse the `.polkavm` blobs with https://docs.rs/polkavm/latest/polkavm/struct.ProgramParts.html and https://docs.rs/polkavm/latest/polkavm/struct.ProgramBlob.html and load them into your VM.  Here's some code for reference how a JAM blob is made from a `.polkavm` blob: https://docs.rs/jam-pvm-builder/0.1.21/src/jam_pvm_builder/lib.rs.html#282

id:
1653
timestamp:
2025-04-24T10:16:58.581Z
sender:
@knight1205:matrix.org
content:
Any updates to this? Till now we only have pull protocol. I guess each assurer have to request chunks when it receives report? 

Shouldn't the guarantor itself ensure that each validator receives chunk when it is erasure coded as per GP?

id:
1652
timestamp:
2025-04-24T10:37:45.224Z
sender:
@oliver.tale-yazdi:parity.io
content:
The code to parse those blobs is not relevant fort he GP, right? So we can take a look at it?

id:
1651
timestamp:
2025-04-24T10:37:51.220Z
sender:
@oliver.tale-yazdi:parity.io
content:
* The code to parse those blobs is not relevant for the GP, right? So we can take a look at it?

id:
1650
timestamp:
2025-04-24T10:46:59.309Z
sender:
@jan:parity.io
content:
Correct.

id:
1649
timestamp:
2025-04-24T10:50:15.903Z
sender:
@jan:parity.io
content:
Here's a pastebin of the relevant code to parse them: https://pastebin.com/L2CsuVNR

id:
1648
timestamp:
2025-04-24T10:59:02.503Z
sender:
@jan:parity.io
content:
Essentially the format looks like this:

```
struct Blob {
    magic: [u8; 4],
    version: u8,
    length: u64,
    sections: [Section]
}

struct Section {
    kind: u8,
    length: varint,
    body: [u8; Self::length]
}

struct MemoryConfigBody { // kind == 1
    ro_data_size: varint,
    rw_data_size: varint,
    stack_size: varint,
}

// RO_DATA and RW_DATA sections just have the raw byte payloads

struct ImportsBody { // kind == 4
    import_count: varint,
    import_offsets: [u32; Self::import_count],
    import_symbols: [u8; ...until end of the section...]
}

struct ExportsBody { // kind == 5
    exports: [Export; ...until end of the section...]
}

struct Export {
    program_counter: varint,
    name_length: varint,
    name: [u8; Self::name_length]
}
```

id:
1647
timestamp:
2025-04-24T11:01:38.402Z
sender:
@jan:parity.io
content:
It's somewhat similar to how `.wasm` blobs look internally. Basically it's a preset header + list of sections where each section has an ID and a length, the sections must always be given in order (so you don't have to write a loop to parse this format because e.g. you always know that memory config section must *always* come before the ro data section, etc.), there is a list of imports and exports (so unlike the JAM blob the imports and exports are *not* hardcoded), and it also supports debug info (which the JAM blobs don't).

id:
3225
timestamp:
2025-04-24T13:47:55.407Z
sender:
@haikoschol:matrix.org
content:
sourabhniyogi: was that reaction a "yes" or a "i'm wondering about that too" or a "yay, equations!!1!"? 🤔 

id:
3226
timestamp:
2025-04-24T15:12:42.773Z
sender:
@haikoschol:matrix.org
content:
KwickBit - Charles-Edouard LADARI: it's not the hash though, it's the (32 octet) encoding of the service index, right?

id:
3227
timestamp:
2025-04-24T15:14:28.592Z
sender:
@celadari:matrix.org
content:
By 32 octet i meaning we add 0s before the last 4 octets of the service index ? 🤔

id:
3228
timestamp:
2025-04-24T15:44:30.174Z
sender:
@haikoschol:matrix.org
content:
i think so

id:
3229
timestamp:
2025-04-24T15:45:37.996Z
sender:
@haikoschol:matrix.org
content:
it won't incidentally turn out to be the hash of some code, so the code hash field needs to have been set with `upgrade` to this value before i reckon

id:
3230
timestamp:
2025-04-24T16:52:53.011Z
sender:
@jaymansfield:matrix.org
content:
No you only need to know what was returned from CE137 originally. You are providing a path to a root for a list of shards with an index, not for all shards o a given segment. Refer back to s_clubs in your availability specifier.

id:
3231
timestamp:
2025-04-24T16:53:08.847Z
sender:
@jaymansfield:matrix.org
content:
* No you only need to know what was returned from CE137 originally. You are providing a path to a root for a list of shards with an index, not for all shards of a given segment. Refer back to s\_clubs in your availability specifier.

id:
3232
timestamp:
2025-04-24T17:28:10.048Z
sender:
@ascriv:matrix.org
content:
How is the serialization of the storage dictionary of a service account in (D.2) not lossy? The key (I thought) is not the hash of the value unlike the preimage lookup  

id:
3233
timestamp:
2025-04-24T17:28:50.293Z
sender:
@ascriv:matrix.org
content:
* How is the serialization of the storage dictionary (a_s) of a service account in (D.2) not lossy? The key (I thought) is not the hash of the value unlike the preimage lookup  

id:
3270
timestamp:
2025-04-24T17:35:07.458Z
sender:
@ascriv:matrix.org
content:
* How is the serialization of the storage dictionary (a_s) of a service account in (D.2) not lossy? The key (I thought) is not the hash of the value unlike the preimage lookup. I’m definitely missing something or we are losing the last 4 bytes 

id:
3234
timestamp:
2025-04-24T17:35:13.770Z
sender:
@ascriv:matrix.org
content:
* How is the serialization of the storage dictionary (a_s) of a service account in (D.2) not lossy? The key (I thought) is not the hash of the value unlike the preimage lookup. I’m definitely missing something or we are losing the last 4 bytes of the keys

id:
3235
timestamp:
2025-04-24T17:53:42.240Z
sender:
@prasad-kumkar:matrix.org
content:
I think the point is we don’t need to store the full a_s keys, they’re only used to look up values when the key is already known

id:
3236
timestamp:
2025-04-24T18:24:29.019Z
sender:
@erin:parity.io
content:
hello all, I've created hosted archives of the JAM and graypaper chats with plaintext versions also available at https://paritytech.github.io/matrix-archiver/

there are a few quality of life improvements still to be done but any feedback or comments are welcome if you find this useful. This is also now linked on the jamcha.in site.

id:
3237
timestamp:
2025-04-24T18:25:36.795Z
sender:
@erin:parity.io
content:
These are updated daily at ~3am UTC.

id:
3238
timestamp:
2025-04-24T18:29:17.096Z
sender:
@erin:parity.io
content:
Other JAM-related channels are welcome to be archived - they need to be unencrypted and world-readable (history available to "Anybody"). Please open an issue with the internal room ID if you wish to archive a channel.

id:
3239
timestamp:
2025-04-24T18:30:55.054Z
sender:
@erin:parity.io
content:
hello all, I've created hosted archives of the JAM and graypaper chats with plaintext versions also available at https://paritytech.github.io/matrix-archiver/

there are a few quality of life improvements still to be done but any feedback or comments are welcome if you find this useful. This is also now linked on the jamcha.in site.

These are updated daily at ~3am UTC.
Other JAM-related channels are welcome to be archived - they need to be unencrypted and world-readable (history available to "Anybody"). Please open an issue with the internal room ID if you wish to archive a channel.

id:
3240
timestamp:
2025-04-24T18:37:43.353Z
sender:
@erin:parity.io
content:
* hello all, I've created hosted archives of the JAM and graypaper chats with plaintext versions also available at https://paritytech.github.io/matrix-archiver/

there are a few quality of life improvements still to be done but any feedback or comments are welcome if you find this useful. This is also now linked on the jamcha.in site.

These are updated daily at ~3am UTC.
Other JAM-related channels are welcome to be archived - they need to be unencrypted and world-readable (history available to "Anyone"). Please open an issue with the internal room ID if you wish to archive a channel.

id:
3241
timestamp:
2025-04-24T18:37:57.368Z
sender:
@erin:parity.io
content:
* Other JAM-related channels are welcome to be archived - they need to be unencrypted and world-readable (history available to "Anyone"). Please open an issue with the internal room ID if you wish to archive a channel.

id:
3242
timestamp:
2025-04-24T18:41:11.127Z
sender:
@ascriv:matrix.org
content:
> <@prasad-kumkar:matrix.org> I think the point is we don’t need to store the full a_s keys, they’re only used to look up values when the key is already known

I don’t completely follow, should we just be comparing first 28 bytes when doing lookups? What about when two keys only differ in the last 4 bytes?

id:
3243
timestamp:
2025-04-24T18:43:23.659Z
sender:
@dakkk:matrix.org
content:
keys are hash, a collision is very unlikely

id:
3244
timestamp:
2025-04-24T18:46:07.351Z
sender:
@tomusdrw:matrix.org
content:
erin: would you consider adding `<a name="<element-msg-id>"></a>` to the `<time>` in html mode? That would allow linking to a specific message, which I think would be pretty cool.

id:
3245
timestamp:
2025-04-24T18:46:20.862Z
sender:
@tomusdrw:matrix.org
content:
* erin: would you consider adding `<a name="{element-msg-id}"></a>` to the `<time>` in html mode? That would allow linking to a specific message, which I think would be pretty cool.

id:
3246
timestamp:
2025-04-24T18:46:32.277Z
sender:
@erin:parity.io
content:
yeah, that's one of the QoL features in the pipeline :)

id:
3247
timestamp:
2025-04-24T18:47:54.717Z
sender:
@erin:parity.io
content:
others may include "jump to bottom" and perhaps pagination, though i kinda like the big raw log style (feedback again welcome here).

id:
3259
timestamp:
2025-04-24T18:54:33.185Z
sender:
@ascriv:matrix.org
content:
> <@dakkk:matrix.org> keys are hash, a collision is very unlikely

So state serialization being lossless isn’t important, it just needs to be extremely unlikely to collide 

id:
3248
timestamp:
2025-04-24T18:54:35.201Z
sender:
@tomusdrw:matrix.org
content:
afaict 220kB gzipped for 1y+ worth of content. My guess is that adding JS pagination to this with a modern framework would start to pay off only after 2 more years :D

id:
3249
timestamp:
2025-04-24T18:54:50.547Z
sender:
@tomusdrw:matrix.org
content:
* afaict it's 220kB gzipped for 1y+ worth of content. My guess is that adding JS pagination to this with a modern framework would start to pay off only after 2 more years :D

id:
3250
timestamp:
2025-04-24T18:55:09.701Z
sender:
@erin:parity.io
content:
i'll worry about it later then 😂

id:
3251
timestamp:
2025-04-24T18:55:33.619Z
sender:
@erin:parity.io
content:
it's just static html generated by a python script at the moment

id:
3252
timestamp:
2025-04-24T21:32:25.092Z
sender:
@erin:parity.io
content:
* hello all, I've created hosted archives of the JAM and graypaper chats with plaintext versions also available at https://paritytech.github.io/matrix-archiver/

there are a few quality of life improvements still to be done but any feedback or comments are welcome if you find this useful. This is also now linked on the jamcha.in site.

These are updated daily at ~3am UTC.
Other JAM-related channels are welcome to be archived - they need to be unencrypted and world-readable (history available to "Anyone"). Please open an issue [here](https://github.com/paritytech/matrix-archiver) with the internal room ID if you wish to archive a channel.

id:
3253
timestamp:
2025-04-24T21:32:34.703Z
sender:
@erin:parity.io
content:
* Other JAM-related channels are welcome to be archived - they need to be unencrypted and world-readable (history available to "Anyone"). Please open an issue [here](https://github.com/paritytech/matrix-archiver) with the internal room ID if you wish to archive a channel.

id:
3295
timestamp:
2025-04-25T12:21:26.269Z
sender:
@erin:parity.io
content:
tomusdrw: the timestamps are now links to the messages

id:
3296
timestamp:
2025-04-25T13:02:27.042Z
sender:
@charliewinston14:matrix.org
content:
Hi I have a timing question. Segments are kept in the DA for 28 day, but it seems SegmentRootLookupItem's in work package bundles must be recent (exist in beta or extrinsic). Does that mean we store segments for 28 days, but they can only be used as import segments for 8 days? 

id:
3297
timestamp:
2025-04-25T13:03:15.685Z
sender:
@dave:parity.io
content:
The segment root lookup stuff is for when you import using a work-package hash

id:
3298
timestamp:
2025-04-25T13:03:31.958Z
sender:
@dave:parity.io
content:
This is only supported for recent work-packages, for older packages you should use the segment-root directly

id:
3299
timestamp:
2025-04-25T13:08:05.603Z
sender:
@charliewinston14:matrix.org
content:
Thank you. Didn't realize an ImportSpec could be a export root OR a work package hash.

id:
3300
timestamp:
2025-04-25T13:12:53.337Z
sender:
@knight1205:matrix.org
content:
David Emett: Do you have any updates on this?

id:
3301
timestamp:
2025-04-25T13:15:37.544Z
sender:
@dave:parity.io
content:
Almost certainly there will be a push protocol in the final network protocol. Not intending to change SNP though.

id:
3302
timestamp:
2025-04-25T13:16:32.573Z
sender:
@knight1205:matrix.org
content:
Alright. Thanks for the confirmation.

id:
3303
timestamp:
2025-04-25T13:49:38.342Z
sender:
@sourabhniyogi:matrix.org
content:
Cool  if we had discord rooms with content, should we put them in this repo so its not a matrix-archiver but a gp-archive?

id:
3304
timestamp:
2025-04-25T15:51:17.384Z
sender:
@knight1205:matrix.org
content:
David Emett: Could you please elaborate on how is shard index mapped to a particular validator at time of distribution? Like which particular validator will recive which particular shard?

id:
3305
timestamp:
2025-04-25T15:54:27.608Z
sender:
@dave:parity.io
content:
Hmm good question, not sure if this is documented anywhere!

id:
3306
timestamp:
2025-04-25T16:02:31.984Z
sender:
@knight1205:matrix.org
content:
Does anyone know how is shard index determined from validator index? Like which particular validator requests for which particular shard index as in CE 137?

Here in [GP](https://graypaper.fluffylabs.dev/#/cc517d7/1c32011c3801?v=0.6.5): 
 It is just mentioned that 
>chunks are distributed to each validator whose keys are together with similarly corresponding chunks for imported, extrinsic and exported segments data, such that
> each validator can justify completeness according to the
> work-report’s erasure-root.


I am not sure what it is trying to imply.

id:
3307
timestamp:
2025-04-25T16:04:04.267Z
sender:
@erin:parity.io
content:
there should be an automated script to grab the content daily unless they're dead/archived channels. the script/rendering right now is very matrix-specific. it would be best to have a matrix bridge or something regardless

id:
3308
timestamp:
2025-04-25T16:07:44.390Z
sender:
@dave:parity.io
content:
So shard index = ((core index * systematic threshold) + val index) % num vals, will add this to the SNP doc

id:
3309
timestamp:
2025-04-25T16:08:25.127Z
sender:
@dave:parity.io
content:
core index meaning the core the package was refined on

id:
3310
timestamp:
2025-04-25T16:09:12.195Z
sender:
@dave:parity.io
content:
systematic threshold meaning the minimum number of shards required to reconstruct, this is 342 with the "full" params, 2 with the "tiny" params

id:
3311
timestamp:
2025-04-25T16:13:53.723Z
sender:
@dave:parity.io
content:
See my reply in the "Let's JAM" channel.

id:
3312
timestamp:
2025-04-25T16:34:00.500Z
sender:
@dave:parity.io
content:
I've added a section to the SNP doc covering this, see https://github.com/zdave-parity/jam-np/blob/main/simple.md

id:
3313
timestamp:
2025-04-25T16:34:06.753Z
sender:
@dave:parity.io
content:
("Shard assignment" section)

id:
3314
timestamp:
2025-04-25T17:36:13.094Z
sender:
@jaymansfield:matrix.org
content:
Oh good to know! I previously assumed shard index = validator index.

id:
3315
timestamp:
2025-04-25T17:37:06.172Z
sender:
@knight1205:matrix.org
content:
Thanks a ton!!

id:
3316
timestamp:
2025-04-25T17:53:41.282Z
sender:
@knight1205:matrix.org
content:
>shard index = validator index

in that case few initial nodes will receive too many requests to handle, and they will form most traffic for an epoch for responding to them.

id:
3317
timestamp:
2025-04-26T14:17:03.825Z
sender:
@clearloop:matrix.org
content:
just found that the archiver is even easier than matrix to search chat history lmao, hope we can integrate more sources into the static site asap

id:
3318
timestamp:
2025-04-26T14:18:23.639Z
sender:
@clearloop:matrix.org
content:
* just found that the archiver is even easier than matrix to search chat history lmao, hope we can integrate more sources into the static site asap

it even supports links to messages, great work, direct and efficient

id:
3319
timestamp:
2025-04-26T14:23:05.135Z
sender:
@erin:parity.io
content:
thank you!

id:
3320
timestamp:
2025-04-26T14:25:22.958Z
sender:
@clearloop:matrix.org
content:
* just found that the archiver is even easier than matrix to search chat history lmao, hope we can integrate more sources into the static site asap

it even supports links to messages, threads with paddings, great work, direct and efficient

id:
3321
timestamp:
2025-04-26T17:40:23.056Z
sender:
@ascriv:matrix.org
content:
For the function definition (D.4), it seems like k could  have (much) fewer than 248 bits, e.g. in the case where there are many elements of the preimage lookup for any service account. Since in this case the keys for all of these in the serialized state will have the same first 8 bytes, so by the time we are computing the leaf value for one of these keys we will have cleaved off at least 64 bits, leaving fewer than 192 bits for k, if I understand correctly  

id:
3322
timestamp:
2025-04-26T17:40:51.596Z
sender:
@ascriv:matrix.org
content:
Should we be padding with zeroes in such cases to compute bits(k)…248?

id:
3323
timestamp:
2025-04-27T21:59:14.625Z
sender:
@decentration:matrix.org
content:
Just working on the pvm implementation i just want to confirm with what is the requirement for M1... in the article (https://hackmd.io/@polkadot/jamprize) it references PVM for M3 but not M1. what is the minimum requirement for M1? 

1. Is it enough to expose the JAM host-function interface while stubbing the RISC-V runtime?
2. Or do we already need full RISC-V sandbox execution (load blob -> run -> trap -> return ephemerals) in M1?
3. If execution is required, may we rely on an existing impl such as PolkaVM as a temporary backend and postpone a custom PVM to M3?
4. Alternatively, is all PVM work evaluated only from M3 onward?
5. And, is there a difference between M1 and MN1?

id:
3324
timestamp:
2025-04-27T22:34:35.420Z
sender:
@decentration:matrix.org
content:
* Just working on the pvm implementation i just want to confirm with what is the minimum requirement for M1... in the article (https://hackmd.io/@polkadot/jamprize) it references PVM for M3 but not M1. 

1. Is it enough to expose the JAM host-function interface while stubbing the RISC-V runtime?
2. Or do we already need full RISC-V sandbox execution (load blob -> run -> trap -> return ephemerals) in M1?
3. If execution is required, may we rely on an existing impl such as PolkaVM as a temporary backend and postpone a custom PVM to M3?
4. Alternatively, is all PVM work evaluated only from M3 onward?
5. And, is there a difference between M1 and MN1?

id:
3325
timestamp:
2025-04-28T06:58:03.239Z
sender:
@subotic:matrix.org
content:
You need a working PVM for M1 (accumulate needs the PVM). For M1 and M2 it can be an interpreter, while from M3 on it needs to be more performant and thus recompiler based. There are different paths described in the linked HackMD, and from what I understand, an original PVM is needed for the first milestone for all paths, as you cannot import a block without.

id:
3326
timestamp:
2025-04-28T07:58:53.217Z
sender:
@kianenigma:parity.io
content:
I noticed from (latex) comments in https://github.com/gavofyork/graypaper/pull/285 that the encoding of the new statistics state components vary between u16, u32, and u64. Yet, in the state serialization section, they are not covered: https://graypaper.fluffylabs.dev/#/cc517d7/39dd0139e901?v=0.6.5 

Is this something that is not finalized yet, or are they mentioned in another section and I am missing it? 

For example, I could guess that gas always serialized as u64 but didn't have a prior reference to know e.g. extrinsic count is encoded as u16.

id:
3327
timestamp:
2025-04-28T09:31:03.677Z
sender:
@gav:polkadot.io
content:
The state serialisation is the general (variable size integer) encoding. 

id:
3328
timestamp:
2025-04-28T09:31:29.559Z
sender:
@gav:polkadot.io
content:
* The state serialisation for the latter two components is the general (variable size integer) encoding. 

id:
3329
timestamp:
2025-04-28T13:36:32.523Z
sender:
@wabkebab:matrix.org
content:
**JAM *action* the following week **

The following week comes with a packed JAM schedule. JAM Experience, Toaster visit and ETHLisbon JAM weekend (booth and keynote during ETHLisbon) will keep you engaged for the whole week. 

For those who have not confirmed their presence on the visit, please sign up on the following link: https://lu.ma/jamtoastervisit

Limited spots!

id:
3330
timestamp:
2025-04-28T13:36:46.814Z
sender:
@wabkebab:matrix.org
content:
* **JAM _action_ the following week**

The following week comes with a packed JAM schedule. JAM Experience, Toaster visit and ETHLisbon JAM weekend (booth and keynote during ETHLisbon) will keep you engaged for the whole week.

For those who have not confirmed their presence on the visit, please sign up on the following link: https://lu.ma/jamtoastervisit

Limited spots!

id:
3331
timestamp:
2025-04-28T13:37:41.868Z
sender:
@wabkebab:matrix.org
content:
* **JAM _action_ the following week**

The following week comes with a packed JAM schedule. JAM Experience, Toaster visit and ETHLisbon JAM weekend (booth and keynote during ETHLisbon) will keep you engaged for the whole week.

For those who have not confirmed their presence on the Toaster visit, please sign up on the following link: https://lu.ma/jamtoastervisit

Limited spots!

id:
3332
timestamp:
2025-04-28T13:38:06.765Z
sender:
@wabkebab:matrix.org
content:
* **JAM _action_ next week**

The following week comes with a packed JAM schedule. JAM Experience, Toaster visit and ETHLisbon JAM weekend (booth and keynote during ETHLisbon) will keep you engaged for the whole week.

For those who have not confirmed their presence on the Toaster visit, please sign up on the following link: https://lu.ma/jamtoastervisit

Limited spots!

id:
3333
timestamp:
2025-04-28T14:01:34.221Z
sender:
@jay_ztc:matrix.org
content:
Hi Mila | Pala Labs , hope you're doing well. Are you able to share if registration approvals for the toaster tour have started to be processed yet? My teammate & I still see our registration as pending.

id:
3334
timestamp:
2025-04-28T14:49:15.736Z
sender:
@milawords:matrix.org
content:
Hi Jay | ZTC thanks for reaching out. We will review all outstanding registrations in the next two days.

id:
3335
timestamp:
2025-04-28T14:50:04.992Z
sender:
@milawords:matrix.org
content:
* Hi Jay | ZTC thanks for reaching out. We will review all outstanding registrations today and tomorrow.

id:
3336
timestamp:
2025-04-28T15:07:03.542Z
sender:
@milawords:matrix.org
content:
Your support needed: JAM’s first official sponsorship; JAM Booth at ETHLisbon

Dear Implementers,

We are excited to organise JAM’s first official sponsorship of an Ethereum conference - the JAM booth at ETHLisbon.

We would love your support in representing JAM at the booth as you are the faces behind JAM. You won’t need to be there the entire time but rather flexible time slots based on your availability.

As a thank you, we are organising a dinner for everyone who helps with the booth. It will be a great opportunity to meet the Pala Labs team and discuss how we can better support JAM implementers moving forward.

Please feel free to reach out to me directly and looking forward to hearing from you!

Mila


id:
3337
timestamp:
2025-04-28T15:39:15.288Z
sender:
@oliverb:parity.io
content:
Mila | Pala Labs: Hi - hope you're well. Have you invited Parity people to this separately? I know you probably want lots of non-Parity people as well but happy to help spread the message internally...

id:
3338
timestamp:
2025-04-28T15:46:08.891Z
sender:
@dave:parity.io
content:
Sorry, missed these messages earlier. s is the full sequence of segments shards with a given index yes. These are all provided in CE 137 (the `[Segment Shard]`), you should not need to request data from another node to handle a CE 140 request.

id:
3339
timestamp:
2025-04-28T15:47:53.266Z
sender:
@dave:parity.io
content:
FWIW the segments root is _not_ involved here, the justifications returned by CE 140 are justifications from the returned segment shards to the erasure root

id:
3340
timestamp:
2025-04-28T15:48:35.078Z
sender:
@dave:parity.io
content:
* Sorry, missed these messages earlier. s is the full sequence of segments shards with the given index yes. These are all provided in CE 137 (the `[Segment Shard]`), you should not need to request data from another node to handle a CE 140 request.

id:
3341
timestamp:
2025-04-28T15:48:40.335Z
sender:
@dave:parity.io
content:
* Sorry, missed these messages earlier. s is the full sequence of segments shards with the given shard index yes. These are all provided in CE 137 (the `[Segment Shard]`), you should not need to request data from another node to handle a CE 140 request.

id:
3342
timestamp:
2025-04-28T15:52:34.939Z
sender:
@dave:parity.io
content:
As Jason says look at s_clubs

id:
3343
timestamp:
2025-04-28T15:53:30.992Z
sender:
@dave:parity.io
content:
I've opened a PR to (hopefully) improve the wording: https://github.com/gavofyork/graypaper/pull/345

id:
3344
timestamp:
2025-04-28T16:07:13.159Z
sender:
@prasad-kumkar:matrix.org
content:
Should argument `a` from [argument invocation function](https://graypaper.fluffylabs.dev/#/cc517d7/2c05022c0702?v=0.6.5) be passed directly into the [program initialization function](https://graypaper.fluffylabs.dev/#/cc517d7/2b64022b6402?v=0.6.5), rather than decoding it from the program code `p` as described in [A.37](https://graypaper.fluffylabs.dev/#/cc517d7/2bb1022bc102?v=0.6.5) ? As noted:

> Given some p which is appropriately encoded together with some argument data a, we can define program code c, registers ω and ram μ through the standard initialization decoder function Y



id:
3345
timestamp:
2025-04-28T18:19:28.209Z
sender:
@prasad-kumkar:matrix.org
content:
* Should argument `a` from [argument invocation function](https://graypaper.fluffylabs.dev/#/cc517d7/2c05022c0702?v=0.6.5) be encoded with p before passing in [program initialization function](https://graypaper.fluffylabs.dev/#/cc517d7/2b64022b6402?v=0.6.5), as its being decoding from `p` as described in [A.37](https://graypaper.fluffylabs.dev/#/cc517d7/2bb1022bc102?v=0.6.5) ? As noted:

> Given some p which is appropriately encoded together with some argument data a, we can define program code c, registers ω and ram μ through the standard initialization decoder function Y

id:
3346
timestamp:
2025-04-28T19:14:51.099Z
sender:
@ascriv:matrix.org
content:
Should we expect to be able to handle bad blocks for M1? Or are all blocks presumed valid?

id:
3347
timestamp:
2025-04-28T19:25:01.277Z
sender:
@jay_ztc:matrix.org
content:
If I may, add "supporting forks" to this question

id:
3348
timestamp:
2025-04-28T19:27:00.787Z
sender:
@jay_ztc:matrix.org
content:
* If I may, add "handle forks" to this question

id:
3349
timestamp:
2025-04-28T19:27:15.028Z
sender:
@jay_ztc:matrix.org
content:
* If I may, tag on "handle forks" to this question

id:
3350
timestamp:
2025-04-28T19:27:27.902Z
sender:
@jay_ztc:matrix.org
content:
* +"handle forks" to this questionGraypaper document:

1.1. Nomenclature
In this paper, we introduce a decentralized, crypto-economic protocol to which the Polkadot Network will transition itself in a major revision on the basis of approval by its governance apparatus. An early, unrefined, version of this protocol was first proposed in Polkadot Fellowship rfc 31, known as CoreJam. CoreJam takes its name after the collect/refine/join/accumulate model of computation at the heart of its service proposition. While the CoreJam rfc suggested an incomplete, scope-limited alteration to the Polkadot protocol, Jam refers to a complete and coherent overall blockchain protocol.

1.2. Driving Factors
Within the realm of blockchain and the wider Web3, we are driven by the need first and foremost to deliver resilience. A proper Web3 digital system should honor a declared service profile—and ideally meet even perceived expectations—regardless of the desires, wealth or power of any economic actors including individuals, organizations and, indeed, other Web3 systems. Inevitably this is aspirational, and we must be pragmatic over how perfectly this may really be delivered. Nonetheless, a Web3 system should aim to provide such radically strong guarantees that, for practical purposes, the system may be described as unstoppable. While Bitcoin is, perhaps, the first example of such a system within the economic domain, it was not general purpose in terms of the nature of the service it offered. A rules-based service is only as useful as the generality of the rules which may be conceived and placed within it. Bitcoin’s rules allowed for an initial use-case, namely a fixedissuance token, ownership of which is well-approximated and autonomously enforced through knowledge of a secret, as well as some further elaborations on this theme. Later, Ethereum would provide a categorically more general-purpose rule set, one which was practically Turing complete. 1 In the context of Web3 where we are aiming to deliver a massively multiuser application platform, generality is crucial, and thus we take this as a given. Beyond resilience and generality, things get more interesting, and we must look a little deeper to understand what our driving factors are. For the present purposes, we identify three additional goals: (1) Resilience: highly resistant from being stopped, corrupted and censored. (2) Generality: able to perform Turing-complete computation. 1 The gas mechanism did restrict what programs can execute on it by placing an upper bound on the number of steps which may be executed, but some restriction to avoid infinite-computation must surely be introduced in a permissionless setting. 1 JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 2 (3) Performance: able to perform computation quickly and at low cost. (4) Coherency: the causal relationship possible between different elements of state and thus how well individual applications may be composed. (5) Accessibility: negligible barriers to innovation; easy, fast, cheap and permissionless. As a declared Web3 technology, we make an implicit assumption of the first two items. Interestingly, items 3 and 4 are antagonistic according to an information theoretic principle which we are sure must already exist in some form but are nonetheless unaware of a name for it. For argument’s sake we shall name it size-coherency antagonism.

1.3. Scaling under Size-Coherency Antagonism
Size-coherency antagonism is a simple principle implying that as the state-space of information systems grow, then the system necessarily becomes less coherent. It is a direct implication of principle that causality is limited by speed. The maximum speed allowed by physics is C the speed of light in a vacuum, however other information systems may have lower bounds: In biological system this is largely determined by various chemical processes whereas in electronic systems is it determined by the speed of electrons in various substances. Distributed software systems will tend to have much lower bounds still, being dependent on a substrate of software, hardware and packet-switched networks of varying reliability. The argument goes: (1) The more state a system utilizes for its dataprocessing, the greater the amount of space this state must occupy. (2) The more space used, then the greater the mean and variance of distances between statecomponents. (3) As the mean and variance increase, then time for causal resolution (i.e. all correct implications of an event to be felt) becomes divergent across the system, causing incoherence. Setting the question of overall security aside for a moment, we can manage incoherence by fragmenting the system into causally-independent subsystems, each of which is small enough to be coherent. In a resource-rich environment, a bacterium may split into two rather than growing to double its size. This pattern is rather a crude means of dealing with incoherency under growth: intrasystem processing has low size and total coherence, intersystem processing supports higher overall sizes but without coherence. It is the principle behind meta-networks such as Polkadot, Cosmos and the predominant vision of a scaled Ethereum (all to be discussed in depth shortly). Such systems typically rely on asynchronous and simplistic communication with “settlement areas” which provide a small-scoped coherent state-space to manage specific interactions such as a token transfer. The present work explores a middle-ground in the antagonism, avoiding the persistent fragmentation of statespace of the system as with existing approaches. We do this by introducing a new model of computation which pipelines a highly scalable, mostly coherent element to a synchronous, fully coherent element. Asynchrony is not avoided, but we bound it to the length of the pipeline and substitute the crude partitioning we see in scalable systems so far with a form of “cache affinity” as it typically seen in multicpu systems with a shared ram. Unlike with snark-based L2-blockchain techniques for scaling, this model draws upon crypto-economic mechanisms and inherits their low-cost and high-performance profiles and averts a bias toward centralization.

1.4. Document Structure
We begin with a brief overview of present scaling approaches in blockchain technology in section 2. In section 3 we define and clarify the notation from which we will draw for our formalisms. We follow with a broad overview of the protocol in section 4 outlining the major areas including the Polka Virtual Machine (pvm), the consensus protocols Safrole and Grandpa, the common clock and build the foundations of the formalism. We then continue with the full protocol definition split into two parts: firstly the correct on-chain state-transition formula helpful for all nodes wishing to validate the chain state, and secondly, in sections 14 and 19 the honest strategy for the off-chain actions of any actors who wield a validator key. The main body ends with a discussion over the performance characteristics of the protocol in section 20 and finally conclude in section 21. The appendix contains various additional material important for the protocol definition including the pvm in appendices A & B, serialization and Merklization in appendices C & D and cryptography in appendices E, G & H. We finish with an index of terms which includes the values of all simple constant terms used in the work in appendix I, and close with the bibliography.

2.1. Polkadot
In order to deliver its service, Jam coopts much of the same game-theoretic and cryptographic machinery as Polkadot known as Elves and described by Jeff Burdges, Cevallos, et al. 2024. However, major differences exist in the actual service offered with Jam, providing an abstraction much closer to the actual computation model generated by the validator nodes its economy incentivizes. It was a major point of the original Polkadot proposal, a scalable heterogeneous multichain, to deliver highperformance through partition and distribution of the workload over multiple host machines. In doing so it took an explicit position that composability would be lowered. Polkadot’s constituent components, parachains are, practically speaking, highly isolated in their nature. Though a message passing system (xcmp) exists it is asynchronous, coarse-grained and practically limited by its reliance on a high-level slowly evolving interaction language xcm. As such, the composability offered by Polkadot between its constituent chains is lower than that of JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 3 Ethereum-like smart-contract systems offering a single and universal object environment and allowing for the kind of agile and innovative integration which underpins their success. Polkadot, as it stands, is a collection of independent ecosystems with only limited opportunity for collaboration, very similar in ergonomics to bridged blockchains though with a categorically different security profile. A technical proposal known as spree would utilize Polkadot’s unique shared-security and improve composability, though blockchains would still remain isolated. Implementing and launching a blockchain is hard, timeconsuming and costly. By its original design, Polkadot limits the clients able to utilize its service to those who are both able to do this and raise a sufficient deposit to win an auction for a long-term slot, one of around 50 at the present time. While not permissioned per se, accessibility is categorically and substantially lower than for smart-contract systems similar to Ethereum. Enabling as many innovators to participate and interact, both with each other and each other’s user-base, appears to be an important component of success for a Web3 application platform. Accessibility is therefore crucial.

2.2. Ethereum
The Ethereum protocol was formally defined in this paper’s spiritual predecessor, the Yellow Paper, by Wood 2014. This was derived in large part from the initial concept paper by Buterin 2013. In the decade since the YP was published, the de facto Ethereum protocol and public network instance have gone through a number of evolutions, primarily structured around introducing flexibility via the transaction format and the instruction set and “precompiles” (niche, sophisticated bonus instructions) of its scripting core, the Ethereum virtual machine (evm). Almost one million crypto-economic actors take part in the validation for Ethereum. 2 Block extension is done through a randomized leader-rotation method where the physical address of the leader is public in advance of their block production. 3 Ethereum uses Casper-FFG introduced by Buterin and Griffith 2019 to determine finality, which with the large validator base finalizes the chain extension around every 13 minutes. Ethereum’s direct computational performance remains broadly similar to that with which it launched in 2015, with a notable exception that an additional service now allows 1 mb of commitment data to be hosted per block (all nodes to store it for a limited period). The data cannot be directly utilized by the main state-transition function, but special functions provide proof that the data (or some subsection thereof) is available. According to Ethereum Foundation 2024b, the present design direction is to improve on this over the coming years by splitting responsibility for its storage amongst the validator base in a protocol known as Dank-sharding. According to Ethereum Foundation 2024a, the scaling strategy of Ethereum would be to couple this data availability with a private market of roll-ups, sideband computation facilities of various design, with zk-snark-based roll-ups being a stated preference. Each vendor’s roll-up design, execution and operation comes with its own implications. One might reasonably assume that a diversified marketbased approach for scaling via multivendor roll-ups will allow well-designed solutions to thrive. However, there are potential issues facing the strategy. A research report by Sharma 2023 on the level of decentralization in the various roll-ups found a broad pattern of centralization, but notes that work is underway to attempt to mitigate this. It remains to be seen how decentralized they can yet be made. Heterogeneous communication properties (such as datagram latency and semantic range), security properties (such as the costs for reversion, corruption, stalling and censorship) and economic properties (the cost of accepting and processing some incoming message or transaction) may differ, potentially quite dramatically, between major areas of some grand patchwork of roll-ups by various competing vendors. While the overall Ethereum network may eventually provide some or even most of the underlying machinery needed to do the sideband computation it is far from clear that there would be a “grand consolidation” of the various properties should such a thing happen. We have not found any good discussion of the negative ramifications of such a fragmented approach. 4 2.2.1. Snark Roll-ups. While the protocol’s foundation makes no great presuppositions on the nature of roll-ups, Ethereum’s strategy for sideband computation does centre around snark-based rollups and as such the protocol is being evolved into a design that makes sense for this. Snarks are the product of an area of exotic cryptography which allow proofs to be constructed to demonstrate to a neutral observer that the purported result of performing some predefined computation is correct. The complexity of the verification of these proofs tends to be sub-linear in their size of computation to be proven and will not give away any of the internals of said computation, nor any dependent witness data on which it may rely. Zk-snarks come with constraints. There is a trade-off between the proof’s size, verification complexity and the computational complexity of generating it. Non-trivial computation, and especially the sort of general-purpose computation laden with binary manipulation which makes smart-contracts so appealing, is hard to fit into the model of snarks. To give a practical example, risc-zero (as assessed by Bögli 2024) is a leading project and provides a platform for producing snarks of computation done by a risc-v virtual machine, an open-source and succinct risc machine architecture well-supported by tooling. A recent benchmarking report by Polkavm Project 2024 showed that compared to risc-zero’s own benchmark, proof generation alone takes over 61,000 times as long as simply recompiling and executing even when executing on 32 times as many cores, using 20,000 times as much ram and an additional state-of-the-art gpu. According to hardware 2 Practical matters do limit the level of real decentralization. Validator software expressly provides functionality to allow a single instance to be configured with multiple key sets, systematically facilitating a much lower level of actual decentralization than the apparent number of actors, both in terms of individual operators and hardware. Using data collated by Dune and hildobby 2024 on Ethereum 2, one can see one major node operator, Lido, has steadily accounted for almost one-third of the almost one million crypto-economic participants. 3 Ethereum’s developers hope to change this to something more secure, but no timeline is fixed. 4 Some initial thoughts on the matter resulted in a proposal by Sadana 2024 to utilize Polkadot technology as a means of helping create a modicum of compatibility between roll-up ecosystems! JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 4 rental agents https://cloud-gpus.com/, the cost multiplier of proving using risc-zero is 66,000,000x of the cost 5 to execute using the Polkavm recompiler. Many cryptographic primitives become too expensive to be practical to use and specialized algorithms and structures must be substituted. Often times they are otherwise suboptimal. In expectation of the use of snarks (such as plonk as proposed by Gabizon, Williamson, and Ciobotaru 2019), the prevailing design of the Ethereum project’s Dank-sharding availability system uses a form of erasure coding centered around polynomial commitments over a large prime field in order to allow snarks to get acceptably performant access to subsections of data. Compared to alternatives, such as a binary field and Merklization in the present work, it leads to a load on the validator nodes orders of magnitude higher in terms of cpu usage. In addition to their basic cost, snarks present no great escape from decentralization and the need for redundancy, leading to further cost multiples. While the need for some benefits of staked decentralization is averted through their verifiable nature, the need to incentivize multiple parties to do much the same work is a requirement to ensure that a single party not form a monopoly (or several not form a cartel). Proving an incorrect state-transition should be impossible, however service integrity may be compromised in other ways; a temporary suspension of proof-generation, even if only for minutes, could amount to major economic ramifications for real-time financial applications. Real-world examples exist of the pit of centralization giving rise to monopolies. One would be the aforementioned snark-based exchange framework; while notionally serving decentralized exchanges, it is in fact centralized with Starkware itself wielding a monopoly over enacting trades through the generation and submission of proofs, leading to a single point of failure—should Starkware’s service become compromised, then the liveness of the system would suffer. It has yet to be demonstrated that snark-based strategies for eliminating the trust from computation will ever be able to compete on a cost-basis with a multi-party crypto-economic platform. All as-yet proposed snarkbased solutions are heavily reliant on crypto-economic systems to frame them and work around their issues. Data availability and sequencing are two areas well understood as requiring a crypto-economic solution. We would note that snark technology is improving and the cryptographers and engineers behind them do expect improvements in the coming years. In a recent article by Thaler 2023 we see some credible speculation that with some recent advancements in cryptographic techniques, slowdowns for proof generation could be as little as 50,000x from regular native execution and much of this could be parallelized. This is substantially better than the present situation, but still several orders of magnitude greater than would be required to compete on a cost-basis with established crypto-economic techniques such as Elves.

2.3. Fragmented Meta-Networks
Directions for general-purpose computation scalability taken by other projects broadly centre around one of two approaches; either what might be termed a fragmentation approach or alternatively a centralization approach. We argue that neither approach offers a compelling solution. The fragmentation approach is heralded by projects such as Cosmos (proposed by Kwon and Buchman 2019) and Avalanche (by Tanana 2019). It involves a system fragmented by networks of a homogenous consensus mechanic, yet staffed by separately motivated sets of validators. This is in contrast to Polkadot’s single validator set and Ethereum’s declared strategy of heterogeneous rollups secured partially by the same validator set operating under a coherent incentive framework. The homogeneity of said fragmentation approach allows for reasonably consistent messaging mechanics, helping to present a fairly unified interface to the multitude of connected networks. However, the apparent consistency is superficial. The networks are trustless only by assuming correct operation of their validators, who operate under a crypto-economic security framework ultimately conjured and enforced by economic incentives and punishments. To do twice as much work with the same levels of security and no special coordination between validator sets, then such systems essentially prescribe forming a new network with the same overall levels of incentivization. Several problems arise. Firstly, there is a similar downside as with Polkadot’s isolated parachains and Ethereum’s isolated roll-up chains: a lack of coherency due to a persistently sharded state preventing synchronous composability. More problematically, the scaling-by-fragmentation approach, proposed specifically by Cosmos, provides no homogenous security—and therefore trustlessness— guarantees. Validator sets between networks must be assumed to be independently selected and incentivized with no relationship, causal or probabilistic, between the Byzantine actions of a party on one network and potential for appropriate repercussions on another. Essentially, this means that should validators conspire to corrupt or revert the state of one network, the effects may be felt across other networks of the ecosystem. That this is an issue is broadly accepted, and projects propose for it to be addressed in one of two ways. Firstly, to fix the expected cost-of-attack (and thus level of security) across networks by drawing from the same validator set. The massively redundant way of doing this, as proposed by Cosmos Project 2023 under the name replicated security, would be to require each validator to validate on all networks and for the same incentives and punishments. This is economically inefficient in the cost of security provision as each network would need to independently provide the same level of incentives and punishment-requirements as the most secure with which it wanted to interoperate. This is to ensure the economic proposition remain unchanged for validators and the security proposition remained equivalent for all networks. At the present time, replicated security is not a readily available permissionless service. We might speculate that these punishing economics have something to do with it. The more efficient approach, proposed by the OmniLedger team, Kokoris-Kogias et al. 2017, would be to 5 In all likelihood actually substantially more as this was using low-tier “spare” hardware in consumer units, and our recompiler was unoptimized. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 5 make the validators non-redundant, partitioning them between different networks and periodically, securely and randomly repartitioning them. A reduction in the cost to attack over having them all validate on a single network is implied since there is a chance of having a single network accidentally have a compromising number of malicious validators even with less than this proportion overall. This aside it presents an effective means of scaling under a basis of weak-coherency. Alternatively, as in Elves by Jeff Burdges, Cevallos, et al. 2024, we may utilize non-redundant partitioning, combine this with a proposal-and-auditing game which validators play to weed out and punish invalid computations, and then require that the finality of one network be contingent on all causally-entangled networks. This is the most secure and economically efficient solution of the three, since there is a mechanism for being highly confident that invalid transitions will be recognized and corrected before their effect is finalized across the ecosystem of networks. However, it requires substantially more sophisticated logic and their causal-entanglement implies some upper limit on the number of networks which may be added.

2.4. High-Performance Fully Synchronous Networks
Another trend in the recent years of blockchain development has been to make “tactical” optimizations over data throughput by limiting the validator set size or diversity, focusing on software optimizations, requiring a higher degree of coherency between validators, onerous requirements on the hardware which validators must have, or limiting data availability. The Solana blockchain is underpinned by technology introduced by Yakovenko 2018 and boasts theoretical figures of over 700,000 transactions per second, though according to Ng 2024 the network is only seen processing a small fraction of this. The underlying throughput is still substantially more than most blockchain networks and is owed to various engineering optimizations in favor of maximizing synchronous performance. The result is a highlycoherent smart-contract environment with an api not unlike that of YP Ethereum (albeit using a different underlying vm), but with a near-instant time to inclusion and finality which is taken to be immediate upon inclusion. Two issues arise with such an approach: firstly, defining the protocol as the outcome of a heavily optimized codebase creates structural centralization and can undermine resilience. Jha 2024 writes “since January 2022, 11 significant outages gave rise to 15 days in which major or partial outages were experienced”. This is an outlier within the major blockchains as the vast majority of major chains have no downtime. There are various causes to this downtime, but they are generally due to bugs found in various subsystems. Ethereum, at least until recently, provided the most contrasting alternative with its well-reviewed specification, clear research over its crypto-economic foundations and multiple clean-room implementations. It is perhaps no surprise that the network very notably continued largely unabated when a flaw in its most deployed implementation was found and maliciously exploited, as described by Hertig 2016. The second issue is concerning ultimate scalability of the protocol when it provides no means of distributing workload beyond the hardware of a single machine. In major usage, both historical transaction data and state would grow impractically. Solana illustrates how much of a problem this can be. Unlike classical blockchains, the Solana protocol offers no solution for the archival and subsequent review of historical data, crucial if the present state is to be proven correct from first principle by a third party. There is little information on how Solana manages this in the literature, but according to Solana Foundation 2023, nodes simply place the data onto a centralized database hosted by Google. 6 Solana validators are encouraged to install large amounts of ram to help hold its large state in memory (512 gb is the current recommendation according to Solana Labs 2024). Without a divide-and-conquer approach, Solana shows that the level of hardware which validators can reasonably be expected to provide dictates the upper limit on the performance of a totally synchronous, coherent execution model. Hardware requirements represent barriers to entry for the validator set and cannot grow without sacrificing decentralization and, ultimately, transparency.

3.1. Typography
We use a number of different typefaces to denote different kinds of terms. Where a term is used to refer to a value only relevant within some localized section of the document, we use a lower-case roman letter e.g. x, y (typically used for an item of a set or sequence) or e.g. i, j (typically used for numerical indices). Where we refer to a Boolean term or a function in a local context, we tend to use a capitalized roman alphabet letter such as A, F. If particular emphasis is needed on the fact a term is sophisticated or multidimensional, then we may use a bold typeface, especially in the case of sequences and sets. For items which retain their definition throughout the present work, we use other typographic conventions. Sets are usually referred to with a blackboard typeface, e.g. N refers to all natural numbers including zero. Sets which may be parameterized may be subscripted or be followed by parenthesized arguments. Imported functions, used by the present work but not specifically introduced by it, are written in calligraphic typeface, e.g. H the Blake2 cryptographic hashing function. For other non-context dependent functions introduced in the present work, we use upper case Greek letters, e.g. Υ denotes the state transition function. Values which are not fixed but nonetheless hold some consistent meaning throughout the present work are denoted with lower case Greek letters such as σ, the state 6 Earlier node versions utilized Arweave network, a decentralized data store, but this was found to be unreliable for the data throughput which Solana required. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 6 identifier. These may be placed in bold typeface to denote that they refer to an abnormally complex value.

3.2. Functions and Operators
We define the precedes relation to indicate that one term is defined in terms of another. E.g. y ≺ x indicates that y may be defined purely in terms of x : y ≺ x ⇐⇒ ∃ f ∶ y = f (x) (3.1) The substitute-if-nothing function U is equivalent to the first argument which is not ∅, or ∅ if no such argument exists: U (a 0 ,. .. a n) ≡ a x ∶ (a x ≠ ∅ ∨ x = n), x − 1 ⋀ i = 0 a i = ∅ (3.2) Thus, e.g. U (∅, 1, ∅, 2) = 1 and U (∅, ∅) = ∅.

3.3. Sets
Given some set s, its power set and cardinality are denoted as the usual ℘ ⟨ s ⟩ and S s S. When forming a power set, we may use a numeric subscript in order to restrict the resultant expansion to a particular cardinality. E.g. ℘ ⟨{ 1, 2, 3 }⟩ 2 = {{ 1, 2 }, { 1, 3 }, { 2, 3 }}. Sets may be operated on with scalars, in which case the result is a set with the operation applied to each element, e.g. { 1, 2, 3 } + 3 = { 4, 5, 6 }. Functions may also be applied to all members of a set to yield a new set, but for clarity we denote this with a # superscript, e.g. f # ({ 1, 2 }) ≡ { f (1), f (2)}. We denote set-disjointness with the relation ⫰. Formally: A ∩ B = ∅ ⇐⇒ A ⫰ B We commonly use ∅ to indicate that some term is validly left without a specific value. Its cardinality is defined as zero. We define the operation ? such that A ? ≡ A ∪ { ∅ } indicating the same set but with the addition of the ∅ element. The term ∇ is utilized to indicate the unexpected failure of an operation or that a value is invalid or unexpected. (We try to avoid the use of the more conventional  here to avoid confusion with Boolean false, which may be interpreted as some successful result in some contexts.)

3.4. Numbers
N denotes the set of naturals including zero whereas N n implies a restriction on that set to values less than n. Formally, N = { 0, 1 ,. .. } and N n = { x S x ∈ N, x < n }. Z denotes the set of integers. We denote Z a...b to be the set of integers within the interval [ a, b). Formally, Z a...b = { x S x ∈ Z, a ≤ x < b }. E.g. Z 2 ... 5 = { 2, 3, 4 }. We denote the offset/length form of this set as Z a ⋅⋅⋅+ b, a short form of Z a...a + b. It can sometimes be useful to represent lengths of sequences and yet limit their size, especially when dealing with sequences of octets which must be stored practically. Typically, these lengths can be defined as the set N 2 32. To improve clarity, we denote N L as the set of lengths of octet sequences and is equivalent to N 2 32. We denote the % operator as the modulo operator, e.g. 5 % 3 = 2. Furthermore, we may occasionally express a division result as a quotient and remainder with the separator R, e.g. 5 ÷ 3 = 1 R 2.

3.5. Dictionaries
A dictionary is a possibly partial mapping from some domain into some co-domain in much the same manner as a regular function. Unlike functions however, with dictionaries the total set of pairings are necessarily enumerable, and we represent them in some data structure as the set of all (key ↦ value) pairs. (In such data-defined mappings, it is common to name the values within the domain a key and the values within the co-domain a value, hence the naming.) Thus, we define the formalism D ⟨ K → V ⟩ to denote a dictionary which maps from the domain K to the range V. We define a dictionary as a member of the set of all dictionaries D and a set of pairs p = (k ↦ v) : D ⊂ {(k ↦ v)} (3.3) A dictionary’s members must associate at most one unique value for any key k : ∀ d ∈ D ∶ ∀ (k ↦ v) ∈ d ∶ ∃ ! v ′ ∶ (k ↦ v ′) ∈ d (3.4) This assertion allows us to unambiguously define the subscript and subtraction operator for a dictionary d : ∀ d ∈ D ∶ d [ k ] ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ v if ∃ k ∶ (k ↦ v) ∈ d ∅ otherwise (3.5) ∀ d ∈ D, s ⊆ K ∶ d ∖ s ≡ {(k ↦ v) ∶ (k ↦ v) ∈ d, k ~ ∈ s } (3.6) Note that when using a subscript, it is an implicit assertion that the key exists in the dictionary. Should the key not exist, the result is undefined and any block which relies on it must be considered invalid. It is typically useful to limit the sets from which the keys and values may be drawn. Formally, we define a typed dictionary D ⟨ K → V ⟩ as a set of pairs p of the form (k ↦ v) : D ⟨ K → V ⟩ ⊂ D (3.7) D ⟨ K → V ⟩ ≡ {(k ↦ v) S k ∈ K ∧ v ∈ V } (3.8) To denote the active domain (i.e. set of keys) of a dictionary d ∈ D ⟨ K → V ⟩, we use K (d) ⊆ K and for the range (i.e. set of values), V (d) ⊆ V. Formally: K (d ∈ D) ≡ { k S ∃ v ∶ (k ↦ v) ∈ d } (3.9) V (d ∈ D) ≡ { v S ∃ k ∶ (k ↦ v) ∈ d } (3.10) Note that since the co-domain of V is a set, should different keys with equal values appear in the dictionary, the set will only contain one such value. Dictionaries may be combined through the union operator ∪, which priorities the right-side operand in the case of a key-collision: (3.11) ∀ d ∈ D, e ∈ D ∶ d ∪ e ≡ (d ∖ K (e)) ∪ e

3.6. Tuples
Tuples are groups of values where each item may belong to a different set. They are denoted with parentheses, e.g. the tuple t of the naturals 3 and 5 is denoted t = (3, 5), and it exists in the set of natural pairs sometimes denoted N × N, but denoted in the present work as (N, N). We have frequent need to refer to a specific item within a tuple value and as such find it convenient to declare a name for each item. E.g. we may denote a tuple with two named natural components a and b as T = ⎧ ⎩ a ∈ N, b ∈ N ⎫ ⎭. We would denote an item t ∈ T through subscripting its name, thus for some t = ⎧ ⎩ a ▸ ▸ 3, b ▸ ▸ 5 ⎫ ⎭, t a = 3 and t b = 5. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 7

3.7. Sequences
A sequence is a series of elements with particular ordering not dependent on their values. The set of sequences of elements all of which are drawn from some set T is denoted ⟦ T ⟧, and it defines a partial mapping N → T. The set of sequences containing exactly n elements each a member of the set T may be denoted ⟦ T ⟧ n and accordingly defines a complete mapping N n → T. Similarly, sets of sequences of at most n elements and at least n elements may be denoted ⟦ T ⟧ ∶ n and ⟦ T ⟧ n ∶ respectively. Sequences are subscriptable, thus a specific item at index i within a sequence s may be denoted s [ i ], or where unambiguous, s i. A range may be denoted using an ellipsis for example: [ 0, 1, 2, 3 ] ... 2 = [ 0, 1 ] and [ 0, 1, 2, 3 ] 1 ⋅⋅⋅+ 2 = [ 1, 2 ]. The length of such a sequence may be denoted S s S. We denote modulo subscription as s [ i ] ↺ ≡ s [ i % S s S ]. We denote the final element x of a sequence s = [ ..., x ] through the function last (s) ≡ x. 3.7.1. Construction. We may wish to define a sequence in terms of incremental subscripts of other values: [ x 0, x 1 ,. .. ] n denotes a sequence of n values beginning x 0 continuing up to x n − 1. Furthermore, we may also wish to define a sequence as elements each of which are a function of their index i ; in this case we denote [ f (i) S i < − N n ] ≡ [ f (0), f (1) ,. .., f (n − 1)]. Thus, when the ordering of elements matters we use < − rather than the unordered notation ∈. The latter may also be written in short form [ f (i < − N n)]. This applies to any set which has an unambiguous ordering, particularly sequences, thus [ i 2 S i < − [ 1, 2, 3 ] ] = [ 1, 4, 9 ]. Multiple sequences may be combined, thus [ i ⋅ j S i < − [ 1, 2, 3 ], j < − [ 2, 3, 4 ] ] = [ 2, 6, 12 ]. As with sets, we use explicit notation f # to denote a function mapping over all items of a sequence. Sequences may be constructed from sets or other sequences whose order should be ignored through sequence ordering notation [ i k ^ ^ i ∈ X ], which is defined to result in the set or sequence of its argument except that all elements i are placed in ascending order of the corresponding value i k. The key component may be elided in which case it is assumed to be ordered by the elements directly; i.e. [ i ∈ X ] ≡ [ i ^ ^ i ∈ X ]. [ i k _ _ i ∈ X ] does the same, but excludes any duplicate values of i. E.g. assuming s = [ 1, 3, 2, 3 ], then [ i _ _ i ∈ s ] = [ 1, 2, 3 ] and [ − i ^ ^ i ∈ s ] = [ 3, 3, 2, 1 ]. Sets may be constructed from sequences with the regular set construction syntax, e.g. assuming s = [ 1, 2, 3, 1 ], then { a S a ∈ s } would be equivalent to { 1, 2, 3 }. Sequences of values which themselves have a defined ordering have an implied ordering akin to a regular dictionary, thus [ 1, 2, 3 ] < [ 1, 2, 4 ] and [ 1, 2, 3 ] < [ 1, 2, 3, 1 ]. 3.7.2. Editing. We define the sequence concatenation operator ⌢ such that [ x 0, x 1 ,. .., y 0, y 1 ,. .. ] ≡ x ⌢ y. For sequences of sequences, we define a unary concatenate-all operator: Ì x ≡ x 0 ⌢ x 1 ⌢. .. . Further, we denote element concatenation as x i ≡ x ⌢ [ i ]. We denote the sequence made up of the first n elements of sequence s to be Ð → s n ≡ [ s 0, s 1 ,. .., s n − 1 ], and only the final elements as ← Ð s n. We define T x as the transposition of the sequence-ofsequences x, fully defined in equation H.5. We may also apply this to sequences-of-tuples to yield a tuple of sequences. We denote sequence subtraction with a slight modification of the set subtraction operator; specifically, some sequence s excepting the left-most element equal to v would be denoted s m { v }. 3.7.3. Boolean values. B s denotes the set of Boolean strings of length s, thus B s = ⟦{ , ⊺ }⟧ s. When dealing with Boolean values we may assume an implicit equivalence mapping to a bit whereby ⊺ = 1 and  = 0, thus B ◻ = ⟦ N 2 ⟧ ◻. We use the function bits (Y) ∈ B to denote the sequence of bits, ordered with the most significant first, which represent the octet sequence Y, thus bits ([ 160, 0 ]) = [ 1, 0, 1, 0, 0 ,. .. ]. The unary-not operator applies to both boolean values and sequences of boolean values, thus ¬⊺ =  and ¬ [ ⊺,  ] = [ , ⊺ ]. 3.7.4. Octets and Blobs. Y denotes the set of octet strings (“blobs”) of arbitrary length. As might be expected, Y x denotes the set of such sequences of length x. Y $ denotes the subset of Y which are ASCII-encoded strings. Note that while an octet has an implicit and obvious bijective relationship with natural numbers less than 256, and we may implicitly coerce between octet form and natural number form, we do not treat them as exactly equivalent entities. In particular for the purpose of serialization, an octet is always serialized to itself, whereas a natural number may be serialized as a sequence of potentially several octets, depending on its magnitude and the encoding variant. 3.7.5. Shuffling. We define the sequence-shuffle function F, originally introduced by Fisher and Yates 1938, with an efficient in-place algorithm described by Wikipedia 2024. This accepts a sequence and some entropy and returns a sequence of the same length with the same elements but in an order determined by the entropy. The entropy may be provided as either an indefinite sequence of naturals or a hash. For a full definition see appendix F.

3.8. Cryptography
3.8.1. Hashing. H denotes the set of 256-bit values typically expected to be arrived at through a cryptographic function, equivalent to Y 32, with H 0 being equal to [ 0 ] 32. We assume a function H (m ∈ Y) ∈ H denoting the Blake2b 256-bit hash introduced by Saarinen and Aumasson 2015 and a function H K (m ∈ Y) ∈ H denoting the Keccak 256bit hash as proposed by Bertoni et al. 2013 and utilized by Wood 2014. We may sometimes wish to take only the first x octets of a hash, in which case we denote H x (m) ∈ Y x to be the first x octets of H (m). The inputs of a hash function should be expected to be passed through our serialization codec E to yield an octet sequence to which the cryptography may be applied. (Note that an octet sequence conveniently yields an identity transform.) We may wish to interpret a sequence of octets as some other kind of value with the assumed decoder function E − 1 (x ∈ Y). In both cases, we may subscript the transformation function with the number of octets we expect the octet sequence term to have. Thus, r = E 4 (x ∈ N) would assert x ∈ N 2 32 and r ∈ Y 4, whereas s = E − 1 8 (y) would assert y ∈ Y 8 and s ∈ N 2 64. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 8 3.8.2. Signing Schemes. E k ⟨ m ⟩ ⊂ Y 64 is the set of valid Ed25519 signatures, defined by Josefsson and Liusvaara 2017, made through knowledge of a secret key whose public key counterpart is k ∈ Y 32 and whose message is m. To aid readability, we denote the set of valid public keys H E. We use Y BLS ⊂ Y 144 to denote the set of public keys for the bls signature scheme, described by Boneh, Lynn, and Shacham 2004, on curve bls 12 - 381 defined by Hopwood et al. 2020. We denote the set of valid Bandersnatch public keys as H B, defined in appendix G. F m ∈ Y k ∈ H B ⟨ x ∈ Y ⟩ ⊂ Y 96 is the set of valid singly-contextualized signatures of utilizing the secret counterpart to the public key k, some context x and message m. ¯ F m ∈ Y r ∈ Y R ⟨ x ∈ Y ⟩ ⊂ Y 784, meanwhile, is the set of valid Bandersnatch Ring vrf deterministic singly-contextualized proofs of knowledge of a secret within some set of secrets identified by some root in the set of valid roots Y R ⊂ Y 144. We denote O (s ∈ ⟦ H B ⟧) ∈ Y R to be the root specific to the set of public key counterparts s. A root implies a specific set of Bandersnatch key pairs, knowledge of one of the secrets would imply being capable of making a unique, valid—and anonymous—proof of knowledge of a unique secret within the set. Both the Bandersnatch signature and Ring vrf proof strictly imply that a member utilized their secret key in combination with both the context x and the message m ; the difference is that the member is identified in the former and is anonymous in the latter. Furthermore, both define a vrf output, a high entropy hash influenced by x but not by m, formally denoted Y (¯ F m r ⟨ x ⟩) ⊂ H and Y (F m k ⟨ x ⟩) ⊂ H. We define the function S as the signature function, such that S k (m) ∈ F m k ⟨[]⟩ ∪ E k ⟨ m ⟩. We assert that the ability to compute a result for this function relies on knowledge of a secret key.

10.1. The State
The disputes state includes four items, three of which concern verdicts: a good-set (ψ g), a badset (ψ b) and a wonky-set (ψ w) containing the hashes of all work-reports which were respectively judged to be correct, incorrect or that it appears impossible to judge. The fourth item, the punish-set (ψ o), is a set of Ed25519 keys representing validators which were found to have misjudged a work-report. (10.1) ψ ≡ ⎧ ⎩ ψ g, ψ b, ψ w, ψ o ⎫ ⎭

10.2. Extrinsic
The disputes extrinsic, E D, may contain one or more verdicts v as a compilation of judgments coming from exactly two-thirds plus one of either the active validator set or the previous epoch’s validator set, i.e. the Ed25519 keys of κ or λ. Additionally, it may contain proofs of the misbehavior of one or more validators, either by guaranteeing a work-report found to be invalid (culprits, c), or by signing a judgment found to be contradiction to a work-report’s validity (faults, f). Both are considered a kind of offense. Formally: (10.2) E D ≡ (v, c, f) where v ∈ E ⎧ ⎪ ⎪ ⎪ ⎪ ⎩ H, τ E  − N 2, ⟦ ⎧ ⎩ { ⊺,  }, N V, E ⎫ ⎭ ⟧ ⌊ 2 ~ 3 V ⌋ + 1 ⎫ ⎪ ⎪ ⎪ ⎪ ⎭ J and c ∈ ⟦ H, H E, E ⟧, f ∈ ⟦ H, { ⊺,  }, H E, E ⟧ The signatures of all judgments must be valid in terms of one of the two allowed validator key-sets, identified by the verdict’s second term which must be either the epoch index of the prior state or one less. Formally: ∀ (r, a, j) ∈ v, ∀ (v, i, s) ∈ j ∶ s ∈ E k [ i ] e ⟨ X v ⌢ r ⟩ where k = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ κ if a = τ E  λ otherwise (10.3) X ⊺ ≡ $jam_valid, X  ≡ $jam_invalid (10.4) Offender signatures must be similarly valid and reference work-reports with judgments and may not report keys which are already in the punish-set: ∀ (r, k, s) ∈ c ∶ ⋀ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ r ∈ ψ ′ b, k ∈ k, s ∈ E k ⟨ X G ⌢ r ⟩ (10.5) ∀ (r, v, k, s) ∈ f ∶ ⋀ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ r ∈ ψ ′ b ⇔ r ~ ∈ ψ ′ g ⇔ v, k ∈ k, s ∈ E k ⟨ X v ⌢ r ⟩ (10.6) where k = { k e S k ∈ λ ∪ κ } ∖ ψ o Verdicts v must be ordered by report hash. Offender signatures c and f must each be ordered by the validator’s Ed25519 key. There may be no duplicate report hashes within the extrinsic, nor amongst any past reported hashes. Formally: v = [ r _ _ ⎧ ⎩ r, a, j ⎫ ⎭ ∈ v ] (10.7) c = [ k _ _ ⎧ ⎩ r, k, s ⎫ ⎭ ∈ c ], f = [ k _ _ ⎧ ⎩ r, v, k, s ⎫ ⎭ ∈ f ] (10.8) { r S ⎧ ⎩ r, a, j ⎫ ⎭ ∈ v } ⫰ ψ g ∪ ψ b ∪ ψ w (10.9) The judgments of all verdicts must be ordered by validator index and there may be no duplicates: (10.10) ∀ (r, a, j) ∈ v ∶ j = [ i _ _ ⎧ ⎩ v, i, s ⎫ ⎭ ∈ j ] We define V to derive from the sequence of verdicts introduced in the block’s extrinsic, containing only the report hash and the sum of positive judgments. We require this total to be either exactly two-thirds-plus-one, zero or one-third of the validator set indicating, respectively, that the report is good, that it’s bad, or that it’s wonky. 11 Formally: V ∈ ⟦ ⎧ ⎩ H, { 0, ⌊ 1 ~ 3 V ⌋, ⌊ 2 ~ 3 V ⌋ + 1 } ⎫ ⎭ ⟧ (10.11) V = ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ r, ∑ ⎧ ⎩ v,i,s ⎫ ⎭ ∈ j v ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ R R R R R R R R R R R R ⎧ ⎩ r, a, j ⎫ ⎭ < − v ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ (10.12) There are some constraints placed on the composition of this extrinsic: any verdict containing solely valid judgments implies the same report having at least one valid entry in the faults sequence f. Any verdict containing solely invalid judgments implies the same report having at least two valid entries in the culprits sequence c. Formally: ∀ (r, ⌊ 2 ~ 3 V ⌋ + 1) ∈ V ∶ ∃ (r,. ..) ∈ f (10.13) ∀ (r, 0) ∈ V ∶ S{(r,. ..) ∈ c }S ≥ 2 (10.14) 11 This requirement may seem somewhat arbitrary, but these happen to be the decision thresholds for our three possible actions and are acceptable since the security assumptions include the requirement that at least two-thirds-plus-one validators are live (Jeff Burdges, Cevallos, et al. 2024 discusses the security implications in depth). JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 19 We clear any work-reports which we judged as uncertain or invalid from their core: (10.15) ∀ c ∈ N C ∶ ρ † [ c ] = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ∅ if {(H (ρ [ c ] w), t) ∈ V, t < ⌊ 2 ~ 3 V ⌋} ρ [ c ] otherwise The state’s good-set, bad-set and wonky-set assimilate the hashes of the reports from each verdict. Finally, the punish-set accumulates the keys of any validators who have been found guilty of offending. Formally: ψ ′ g ≡ ψ g ∪ { r S ⎧ ⎩ r, ⌊ 2 ~ 3 V ⌋ + 1 ⎫ ⎭ ∈ V } (10.16) ψ ′ b ≡ ψ b ∪ { r S ⎧ ⎩ r, 0 ⎫ ⎭ ∈ V } (10.17) ψ ′ w ≡ ψ w ∪ { r S ⎧ ⎩ r, ⌊ 1 ~ 3 V ⌋ ⎫ ⎭ ∈ V } (10.18) ψ ′ o ≡ ψ o ∪ { k S (r, k, s) ∈ c } ∪ { k S (r, v, k, s) ∈ f } (10.19)

10.3. Header
The offenders markers must contain exactly the keys of all new offenders, respectively. Formally: H o ≡ [ k S (r, k, s) ∈ c ] ⌢ [ k S (r, v, k, s) ∈ f ] (10.20)

11.1. State
The state of the reporting and availability portion of the protocol is largely contained within ρ, which tracks the work-reports which have been reported but are not yet known to be available to a super-majority of validators, together with the time at which each was reported. As mentioned earlier, only one report may be assigned to a core at any given time. Formally: (11.1) ρ ∈ ⟦ ⎧ ⎩ w ∈ W, t ∈ N T ⎫ ⎭ ? ⟧ C As usual, intermediate and posterior values (ρ †, ρ ‡, ρ ′) are held under the same constraints as the prior value. 11.1.1. Work Report. A work-report, of the set W, is defined as a tuple of the work-package specification, s ; the refinement context, x ; the core-index (i.e. on which the work is done), c ; as well as the authorizer hash a and trace o ; a segment-root lookup dictionary l ; the gas consumed during the Is-Authorized invocation, g ; and finally the work-digests r which comprise the results of the evaluation of each of the items in the package together with some associated data. Formally: (11.2) W ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ s ∈ S, x ∈ X, c ∈ N C, a ∈ H, o ∈ Y, l ∈ D ⟨ H → H ⟩, r ∈ ⟦ L ⟧ 1 ∶ I, g ∈ N G, ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ We limit the sum of the number of items in the segment-root lookup dictionary and the number of prerequisites to J = 8 : (11.3) ∀ w ∈ W ∶ S w l S + S(w x) p S ≤ J 11.1.2. Refinement Context. A refinement context, denoted by the set X, describes the context of the chain at the point that the report’s corresponding work-package was evaluated. It identifies two historical blocks, the anchor, header hash a along with its associated posterior state-root s and posterior Beefy root b ; and the lookupanchor, header hash l and of timeslot t. Finally, it identifies the hash of any prerequisite work-packages p. Formally: (11.4) X ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ a ∈ H, s ∈ H, b ∈ H, l ∈ H, t ∈ N T, p ∈ { H } ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ 11.1.3. Availability. We define the set of availability specifications, S, as the tuple of the work-package’s hash h, an auditable work bundle length l (see section 14.4.1 for more clarity on what this is), together with an erasure-root u, a segment-root e and segment-count n. Work-results include this availability specification in order to ensure they are able to correctly reconstruct and audit the purported ramifications of any reported work-package. Formally: S ≡ ⎧ ⎩ h ∈ H, l ∈ N L, u ∈ H, e ∈ H, n ∈ N ⎫ ⎭ (11.5) The erasure-root (u) is the root of a binary Merkle tree which functions as a commitment to all data required for the auditing of the report and for use by later workpackages should they need to retrieve any data yielded. It is thus used by assurers to verify the correctness of data they have been sent by guarantors, and it is later verified as correct by auditors. It is discussed fully in section 14. The segment-root (e) is the root of a constant-depth, left-biased and zero-hash-padded binary Merkle tree committing to the hashes of each of the exported segments of each work-item. These are used by guarantors to verify the correctness of any reconstructed segments they are called upon to import for evaluation of some later workpackage. It is also discussed in section 14. 11.1.4. Work Digest. We finally come to define a workdigest, L, which is the data conduit by which services’ states may be altered through the computation done within a work-package. (11.6) L ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ s ∈ N S, h ∈ H, y ∈ H, g ∈ N G, d ∈ Y ∪ J, u ∈ N G, i ∈ N, x ∈ N, z ∈ N, e ∈ N ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ Work-digests are a tuple comprising several items. Firstly s, the index of the service whose state is to be altered and thus whose refine code was already executed. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 20 We include the hash of the code of the service at the time of being reported h, which must be accurately predicted within the work-report according to equation 11.42. Next, the hash of the payload (y) within the work item which was executed in the refine stage to give this result. This has no immediate relevance, but is something provided to the accumulation logic of the service. We follow with the gas limit g for executing this item’s accumulate. There is the work result, the output blob or error of the execution of the code, d, which may be either an octet sequence in case it was successful, or a member of the set J, if not. This latter set is defined as the set of possible errors, formally: (11.7) J ∈ { ∞, ☇, ⊚, BAD, BIG } The first two are special values concerning execution of the virtual machine, ∞ denoting an out-of-gas error and ☇ denoting an unexpected program termination. Of the remaining three, the first indicates that the number of exports made was invalidly reported, the second indicates that the service’s code was not available for lookup in state at the posterior state of the lookup-anchor block. The third indicates that the code was available but was beyond the maximum size allowed W C. Finally, we have five fields describing the level of activity which this workload imposed on the core in bringing the result to bear. We include u the actual amount of gas used during refinement; i and e the number of segments imported from, and exported into, the D 3 L respectively; and x and z the number of, and total size in octets of, the extrinsics used in computing the workload. See section 14 for more information on the meaning of these values. In order to ensure fair use of a block’s extrinsic space, work-reports are limited in the maximum total size of the successful refinement output blobs together with the authorizer trace, effectively limiting their overall size: ∀ w ∈ W ∶ S w o S + ∑ r ∈ w r ∩ Y S r d S ≤ W R (11.8) W R ≡ 48 ⋅ 2 10 (11.9)

4.1. The Block
To aid comprehension and definition of our protocol, we partition as many of our terms as possible into their functional components. We begin with the block B which may be restated as the header H and some input data external to the system and thus said to be extrinsic, E : B ≡ (H, E) (4.2) E ≡ (E T, E D, E P, E A, E G) (4.3) The header is a collection of metadata primarily concerned with cryptographic references to the blockchain ancestors and the operands and result of the present transition. As an immutable known a priori, it is assumed to be available throughout the functional components of block transition. The extrinsic data is split into its several portions: tickets: Tickets, used for the mechanism which manages the selection of validators for the permissioning of block authoring. This component is denoted E T. preimages: Static data which is presently being requested to be available for workloads to be able to fetch on demand. This is denoted E P. reports: Reports of newly completed workloads whose accuracy is guaranteed by specific validators. This is denoted E G. availability: Assurances by each validator concerning which of the input data of workloads they have correctly received and are storing locally. This is denoted E A. disputes: Information relating to disputes between validators over the validity of reports. This is denoted E D.

4.2. The State
Our state may be logically partitioned into several largely independent segments which can both help avoid visual clutter within our protocol description and provide formality over elements of computation which may be simultaneously calculated (i.e. parallelized). We therefore pronounce an equivalence between σ (some complete state) and a tuple of partitioned segments of that state: σ ≡ (α, β, γ, δ, η, ι, κ, λ, ρ, τ, φ, χ, ψ, π, ϑ, ξ) (4.4) In summary, δ is the portion of state dealing with services, analogous in Jam to the Yellow Paper’s (smart contract) accounts, the only state of the YP ’s Ethereum. The identities of services which hold some privileged status are tracked in χ. Validators, who are the set of economic actors uniquely privileged to help build and maintain the Jam chain, are identified within κ, archived in λ and enqueued from ι. All other state concerning the determination of these keys is held within γ. Note this is a departure from the YP proofof-work definitions which were mostly stateless, and this set was not enumerated but rather limited to those with sufficient compute power to find a partial hash-collision in the sha 2 - 256 cryptographic hash function. An on-chain entropy pool is retained in η. Our state also tracks two aspects of each core: α, the authorization requirement which work done on that core must satisfy at the time of being reported on-chain, together with the queue which fills this, φ ; and ρ, each of the cores’ currently assigned report, the availability of whose 7 Practically speaking, blockchains sometimes make assumptions of some fraction of participants whose behavior is simply honest, and not provably incorrect nor otherwise economically disincentivized. While the assumption may be reasonable, it must nevertheless be stated apart from the rules of state-transition. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 9 work-package must yet be assured by a super-majority of validators. Finally, details of the most recent blocks and timeslot index are tracked in β and τ respectively, work-reports which are ready to be accumulated and work-packages which were recently accumulated are tracked in ϑ and ξ respectively and, judgments are tracked in ψ and validator statistics are tracked in π. 4.2.1. State Transition Dependency Graph. Much as in the YP, we specify Υ as the implication of formulating all items of posterior state in terms of the prior state and block. To aid the architecting of implementations which parallelize this computation, we minimize the depth of the dependency graph where possible. The overall dependency graph is specified here: τ ′ ≺ H (4.5) β † ≺ (H, β) (4.6) γ ′ ≺ (H, τ, E T, γ, ι, η ′, κ ′, ψ ′) (4.7) η ′ ≺ (H, τ, η) (4.8) κ ′ ≺ (H, τ, κ, γ) (4.9) λ ′ ≺ (H, τ, λ, κ) (4.10) ψ ′ ≺ (E D, ψ) (4.11) ρ † ≺ (E D, ρ) (4.12) ρ ‡ ≺ (E A, ρ †) (4.13) ρ ′ ≺ (E G, ρ ‡, κ, τ ′) (4.14) W ∗ ≺ (E A, ρ ′) (4.15) (ϑ ′, ξ ′, δ ‡, χ ′, ι ′, φ ′, C) ≺ (W ∗, ϑ, ξ, δ, χ, ι, φ, τ, τ ′) (4.16) β ′ ≺ (H, E G, β †, C) (4.17) δ ′ ≺ (E P, δ ‡, τ ′) (4.18) α ′ ≺ (H, E G, φ ′, α) (4.19) π ′ ≺ (E G, E P, E A, E T, τ, κ ′, π, H) (4.20) The only synchronous entanglements are visible through the intermediate components superscripted with a dagger and defined in equations 4.6, 4.12, 4.13, 4.14, 4.16, 4.17 and 4.18. The latter two mark a merge and join in the dependency graph and, concretely, imply that the availability extrinsic may be fully processed and accumulation of work happen before the preimage lookup extrinsic is folded into state.

4.3. Which History?
A blockchain is a sequence of blocks, each cryptographically referencing some prior block by including a hash of its header, all the way back to some first block which references the genesis header. We already presume consensus over this genesis header H 0 and the state it represents already defined as σ 0. By defining a deterministic function for deriving a single posterior state for any (valid) combination of prior state and block, we are able to define a unique canonical state for any given block. We generally call the block with the most ancestors the head and its state the head state. It is generally possible for two blocks to be valid and yet reference the same prior block in what is known as a fork. This implies the possibility of two different heads, each with their own state. While we know of no way to strictly preclude this possibility, for the system to be useful we must nonetheless attempt to minimize it. We therefore strive to ensure that: (1) It be generally unlikely for two heads to form. (2) When two heads do form they be quickly resolved into a single head. (3) It be possible to identify a block not much older than the head which we can be extremely confident will form part of the blockchain’s history in perpetuity. When a block becomes identified as such we call it finalized and this property naturally extends to all of its ancestor blocks. These goals are achieved through a combination of two consensus mechanisms: Safrole, which governs the (not-necessarily forkless) extension of the blockchain; and Grandpa, which governs the finalization of some extension into canonical history. Thus, the former delivers point 1, the latter delivers point 3 and both are important for delivering point 2. We describe these portions of the protocol in detail in sections 6 and 19 respectively. While Safrole limits forks to a large extent (through cryptography, economics and common-time, below), there may be times when we wish to intentionally fork since we have come to know that a particular chain extension must be reverted. In regular operation this should never happen, however we cannot discount the possibility of malicious or malfunctioning nodes. We therefore define such an extension as any which contains a block in which data is reported which any other block’s state has tagged as invalid (see section 10 on how this is done). We further require that Grandpa not finalize any extension which contains such a block. See section 19 for more information here.

4.4. Time
We presume a pre-existing consensus over time specifically for block production and import. While this was not an assumption of Polkadot, pragmatic and resilient solutions exist including the ntp protocol and network. We utilize this assumption in only one way: we require that blocks be considered temporarily invalid if their timeslot is in the future. This is specified in detail in section 6. Formally, we define the time in terms of seconds passed since the beginning of the Jam Common Era, 1200 UTC on January 1, 2025. 8 Midday UTC is selected to ensure that all major timezones are on the same date at any exact 24-hour multiple from the beginning of the common era. Formally, this value is denoted T.

4.5. Best block
Given the recognition of a number of valid blocks, it is necessary to determine which should be treated as the “best” block, by which we mean the most recent block we believe will ultimately be within of all future Jam chains. The simplest and least risky means of doing this would be to inspect the Grandpa finality mechanism which is able to provide a block for which there is a very high degree of confidence it will remain an ancestor to any future chain head. However, in reducing the risk of the resulting block ultimately not being within the canonical chain, Grandpa will typically return a block some small period older than the most recently authored block. (Existing deployments 8 1,735,732,800 seconds after the Unix Epoch. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 10 suggest around 1-2 blocks in the past under regular operation.) There are often circumstances when we may wish to have less latency at the risk of the returned block not ultimately forming a part of the future canonical chain. E.g. we may be in a position of being able to author a block, and we need to decide what its parent should be. Alternatively, we may care to speculate about the most recent state for the purpose of providing information to a downstream application reliant on the state of Jam. In these cases, we define the best block as the head of the best chain, itself defined in section 19.

4.6. Economics
The present work describes a cryptoeconomic system, i.e. one combining elements of both cryptography and economics and game theory to deliver a self-sovereign digital service. In order to codify and manipulate economic incentives we define a token which is native to the system, which we will simply call tokens in the present work. A value of tokens is generally referred to as a balance, and such a value is said to be a member of the set of balances, N B, which is exactly equivalent to the set of naturals less than 2 64 (i.e. 64-bit unsigned integers in coding parlance). Formally: N B ≡ N 2 64 (4.21) Though unimportant for the present work, we presume that there be a standard named denomination for 10 9 tokens. This is different to both Ethereum (which uses a denomination of 10 18), Polkadot (which uses a denomination of 10 10) and Polkadot’s experimental cousin Kusama (which uses 10 12). The fact that balances are constrained to being less than 2 64 implies that there may never be more than around 18 × 10 9 tokens (each divisible into portions of 10 − 9) within Jam. We would expect that the total number of tokens ever issued will be a substantially smaller amount than this. We further presume that a number of constant prices stated in terms of tokens are known. However we leave the specific values to be determined in following work: B I : the additional minimum balance implied for a single item within a mapping. B L : the additional minimum balance implied for a single octet of data within a mapping. B S : the minimum balance implied for a service.

4.7. The Virtual Machine and Gas
In the present work, we presume the definition of a Polka Virtual Machine (pvm). This virtual machine is based around the risc-v instruction set architecture, specifically the rv 64 em variant, and is the basis for introducing permissionless logic into our state-transition function. The pvm is comparable to the evm defined in the Yellow Paper, but somewhat simpler: the complex instructions for cryptographic operations are missing as are those which deal with environmental interactions. Overall it is far less opinionated since it alters a pre-existing general purpose design, risc-v, and optimizes it for our needs. This gives us excellent pre-existing tooling, since pvm remains essentially compatible with risc-v, including support from the compiler toolkit llvm and languages such as Rust and C++. Furthermore, the instruction set simplicity which risc-v and pvm share, together with the register size (64-bit), active number (13) and endianness (little) make it especially well-suited for creating efficient recompilers on to common hardware architectures. The pvm is fully defined in appendix A, but for contextualization we will briefly summarize the basic invocation function Ψ which computes the resultant state of a pvm instance initialized with some registers (⟦ N R ⟧ 13) and ram (M) and has executed for up to some amount of gas (N G), a number of approximately time-proportional computational steps: (4.22) Ψ ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ Y, N R, N G, ⟦ N R ⟧ 13, M ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ → ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ { ∎, ☇, ∞ } ∪ { F, ̵ h } × N R, N R, Z G, ⟦ N R ⟧ 13, M ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ We refer to the time-proportional computational steps as gas (much like in the YP) and limit it to a 64-bit quantity. We may use either N G or Z G to bound it, the first as a prior argument since it is known to be positive, the latter as a result where a negative value indicates an attempt to execute beyond the gas limit. Within the context of the pvm, ϱ ∈ N G is typically used to denote gas. (4.23) Z G ≡ Z − 2 63 ... 2 63, N G ≡ N 2 64, N R ≡ N 2 64 It is left as a rather important implementation detail to ensure that the amount of time taken while computing the function Ψ (. .., ϱ,. ..) has a maximum computation time approximately proportional to the value of ϱ regardless of other operands. The pvm is a very simple risc register machine and as such has 13 registers, each of which is a 64-bit quantity, denoted as N R, a natural less than 2 64. 9 Within the context of the pvm, ω ∈ ⟦ N R ⟧ 13 is typically used to denote the registers. M ≡ ⎧ ⎪ ⎩ V ∈ Y 2 32, A ∈ ⟦{ W, R, ∅ }⟧ p ⎫ ⎪ ⎭, p = 2 32 Z P (4.24) Z P = 2 12 (4.25) The pvm assumes a simple pageable ram of 32-bit addressable octets situated in pages of Z P = 4096 octets where each page may be either immutable, mutable or inaccessible. The ram definition M includes two components: a value V and access A. If the component is unspecified while being subscripted then the value component may be assumed. Within the context of the virtual machine, μ ∈ M is typically used to denote ram. V μ ≡ { i S μ A [⌊ i ~ Z P ⌋] ≠ ∅ } (4.26) V ∗ μ ≡ { i S μ A [⌊ i ~ Z P ⌋] = W } (4.27) We define two sets of indices for the ram μ : V μ is the set of indices which may be read from; and V ∗ μ is the set of indices which may be written to. Invocation of the pvm has an exit-reason as the first item in the resultant tuple. It is either: ● Regular program termination caused by an explicit halt instruction, ∎. ● Irregular program termination caused by some exceptional circumstance, ☇. ● Exhaustion of gas, ∞. 9 This is three fewer than risc-v ’s 16, however the amount that program code output by compilers uses is 13 since two are reserved for operating system use and the third is fixed as zero JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 11 ● A page fault (attempt to access some address in ram which is not accessible), F. This includes the address of the page at fault. ● An attempt at progressing a host-call, ̵ h. This allows for the progression and integration of a context-dependent state-machine beyond the regular pvm. The full definition follows in appendix A.

9.3. Account Footprint and Threshold Balance
We define the dependent values i and o as the storage footprint of the service, specifically the number of items in storage and the total number of octets used in storage. They are defined purely in terms of the storage map of a service, and it must be assumed that whenever a service’s storage is changed, these change also. Furthermore, as we will see in the account serialization function in section C, these are expected to be found explicitly within the Merklized state data. Because of this we make explicit their set. We may then define a second dependent term t, the minimum, or threshold, balance needed for any given service account in terms of its storage footprint. ∀ a ∈ V (δ) ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ a i ∈ N 2 32 ≡ 2 ⋅ S a l S + S a s S a o ∈ N 2 64 ≡ ∑ (h,z) ∈ K (a l) 81 + z + ∑ x ∈ V (a s) 32 + S x S a t ∈ N B ≡ B S + B I ⋅ a i + B L ⋅ a o (9.8)

4.8. Epochs and Slots
Unlike the YP Ethereum with its proof-of-work consensus system, Jam defines a proof-ofauthority consensus mechanism, with the authorized validators presumed to be identified by a set of public keys and decided by a staking mechanism residing within some system hosted by Jam. The staking system is out of scope for the present work; instead there is an api which may be utilized to update these keys, and we presume that whatever logic is needed for the staking system will be introduced and utilize this api as needed. The Safrole mechanism subdivides time following genesis into fixed length epoch s with each epoch divided into E = 600 time slot s each of uniform length P = 6 seconds, given an epoch period of E ⋅ P = 3600 seconds or one hour. This six-second slot period represents the minimum time between Jam blocks, and through Safrole we aim to strictly minimize forks arising both due to contention within a slot (where two valid blocks may be produced within the same six-second period) and due to contention over multiple slots (where two valid blocks are produced in different time slots but with the same parent). Formally when identifying a timeslot index, we use a natural less than 2 32 (in compute parlance, a 32-bit unsigned integer) indicating the number of six-second timeslots from the Jam Common Era. For use in this context we introduce the set N T : N T ≡ N 2 32 (4.28) This implies that the lifespan of the proposed protocol takes us to mid-August of the year 2840, which with the current course that humanity is on should be ample.

4.9. The Core Model and Services
Whereas in the Ethereum Yellow Paper when defining the state machine which is held in consensus amongst all network participants, we presume that all machines maintaining the full network state and contributing to its enlargement—or, at least, hoping to—evaluate all computation. This “everybody does everything” approach might be called the onchain consensus model. It is unfortunately not scalable, since the network can only process as much logic in consensus that it could hope any individual node is capable of doing itself within any given period of time. 4.9.1. In-core Consensus. In the present work, we achieve scalability of the work done through introducing a second model for such computation which we call the in-core consensus model. In this model, and under normal circumstances, only a subset of the network is responsible for actually executing any given computation and assuring the availability of any input data it relies upon to others. By doing this and assuming a certain amount of computational parallelism within the validator nodes of the network, we are able to scale the amount of computation done in consensus commensurate with the size of the network, and not with the computational power of any single machine. In the present work we expect the network to be able to do upwards of 300 times the amount of computation in-core as that which could be performed by a single machine running the virtual machine at full speed. Since in-core consensus is not evaluated or verified by all nodes on the network, we must find other ways to become adequately confident that the results of the computation are correct, and any data used in determining this is available for a practical period of time. We do this through a crypto-economic game of three stages called guaranteeing, assuring, auditing and, potentially, judging. Respectively, these attach a substantial economic cost to the invalidity of some proposed computation; then a sufficient degree of confidence that the inputs of the computation will be available for some period of time; and finally, a sufficient degree of confidence that the validity of the computation (and thus enforcement of the first guarantee) will be checked by some party who we can expect to be honest. All execution done in-core must be reproducible by any node synchronized to the portion of the chain which has been finalized. Execution done in-core is therefore designed to be as stateless as possible. The requirements for doing it include only the refinement code of the service, the code of the authorizer and any preimage lookups it carried out during its execution. When a work-report is presented on-chain, a specific block known as the lookup-anchor is identified. Correct behavior requires that this must be in the finalized chain and reasonably recent, both properties which may be proven and thus are acceptable for use within a consensus protocol. We describe this pipeline in detail in the relevant sections later. 4.9.2. On Services and Accounts. In YP Ethereum, we have two kinds of accounts: contract accounts (whose actions are defined deterministically based on the account’s associated code and state) and simple accounts which act as gateways for data to arrive into the world state and are controlled by knowledge of some secret key. In Jam, all accounts are service accounts. Like Ethereum’s contract accounts, they have an associated balance, some code and state. Since they are not controlled by a secret key, they do not need a nonce. The question then arises: how can external data be fed into the world state of Jam ? And, by extension, how does overall payment happen if not by deducting the account balances of those who sign transactions? The answer to the first lies in the fact that our service definition actually includes multiple code entry-points, one concerning refinement and the other concerning accumulation. The former acts as a sort of high-performance stateless processor, able to accept arbitrary input data and distill it into some much smaller amount of output data, which together with some metadata is known as a digest. The latter code is more stateful, providing access to certain on-chain functionality including the possibility of transferring balance and invoking the execution of code in other services. Being stateful this might be said to more closely correspond to the code of an Ethereum contract account. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 12 To understand how Jam breaks up its service code is to understand Jam ’s fundamental proposition of generality and scalability. All data extrinsic to Jam is fed into the refinement code of some service. This code is not executed on-chain but rather is said to be executed incore. Thus, whereas the accumulator code is subject to the same scalability constraints as Ethereum’s contract accounts, refinement code is executed off-chain and subject to no such constraints, enabling Jam services to scale dramatically both in the size of their inputs and in the complexity of their computation. While refinement and accumulation take place in consensus environments of a different nature, both are executed by the members of the same validator set. The Jam protocol through its rewards and penalties ensures that code executed in-core has a comparable level of cryptoeconomic security to that executed on-chain, leaving the primary difference between them one of scalability versus synchroneity. As for managing payment, Jam introduces a new abstraction mechanism based around Polkadot’s Agile Coretime. Within the Ethereum transactive model, the mechanism of account authorization is somewhat combined with the mechanism of purchasing blockspace, both relying on a cryptographic signature to identify a single “transactor” account. In Jam, these are separated and there is no such concept of a “transactor”. In place of Ethereum’s gas model for purchasing and measuring blockspace, Jam has the concept of coretime, which is prepurchased and assigned to an authorization agent. Coretime is analogous to gas insofar as it is the underlying resource which is being consumed when utilizing Jam. Its procurement is out of scope in the present work and is expected to be managed by a system parachain operating within a parachains service itself blessed with a number of cores for running such system services. The authorization agent allows external actors to provide input to a service without necessarily needing to identify themselves as with Ethereum’s transaction signatures. They are discussed in detail in section 8.

5.1. The Markers
If not ∅, then the epoch marker specifies key and entropy relevant to the following epoch in case the ticket contest does not complete adequately (a very much unexpected eventuality). Similarly, the winning-tickets marker, if not ∅, provides the series of 600 slot sealing “tickets” for the next epoch (see the next section). Finally, the offenders marker is the sequence of Ed25519 keys of newly misbehaving validators, to be fully explained in section 10. Formally: (5.10) H e ∈ ⎧ ⎩ H, H, ⟦ ⎧ ⎩ H B, H E ⎫ ⎭ ⟧ V ⎫ ⎭ ?, H w ∈ ⟦ C ⟧ E ?, H o ∈ ⟦ H E ⟧ The terms are fully defined in sections 6.6 and 10. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 13

6.1. Timekeeping
Here, τ defines the most recent block’s slot index, which we transition to the slot index as defined in the block’s header: (6.1) τ ∈ N T, τ ′ ≡ H t We track the slot index in state as τ in order that we are able to easily both identify a new epoch and determine the slot at which the prior block was authored. We denote e as the prior’s epoch index and m as the prior’s slot phase index within that epoch and e ′ and m ′ are the corresponding values for the present block: let e R m = τ E, e ′ R m ′ = τ ′ E (6.2)

6.2. Safrole Basic State
We restate γ into a number of components: γ ≡ ⎧ ⎩ γ k, γ z, γ s, γ a ⎫ ⎭ (6.3) γ z is the epoch’s root, a Bandersnatch ring root composed with the one Bandersnatch key of each of the next epoch’s validators, defined in γ k (itself defined in the next section). γ z ∈ Y R (6.4) Finally, γ a is the ticket accumulator, a series of highestscoring ticket identifiers to be used for the next epoch. γ s is the current epoch’s slot-sealer series, which is either a full complement of E tickets or, in the case of a fallback mode, a series of E Bandersnatch keys: γ a ∈ ⟦ C ⟧ ∶ E, γ s ∈ ⟦ C ⟧ E ∪ ⟦ H B ⟧ E (6.5) Here, C is used to denote the set of tickets, a combination of a verifiably random ticket identifier y and the ticket’s entry-index r : C ≡ ⎧ ⎩ y ∈ H, r ∈ N N ⎫ ⎭ (6.6) As we state in section 6.4, Safrole requires that every block header H contain a valid seal H s, which is a Bandersnatch signature for a public key at the appropriate index m of the current epoch’s seal-key series, present in state as γ s.

6.3. Key Rotation
In addition to the active set of validator keys κ and staging set ι, internal to the Safrole state we retain a pending set γ k. The active set is the set of keys identifying the nodes which are currently privileged to author blocks and carry out the validation processes, whereas the pending set γ k, which is reset to ι at the beginning of each epoch, is the set of keys which will be active in the next epoch and which determine the Bandersnatch ring root which authorizes tickets into the sealing-key contest for the next epoch. ι ∈ ⟦ K ⟧ V, γ k ∈ ⟦ K ⟧ V, κ ∈ ⟦ K ⟧ V, λ ∈ ⟦ K ⟧ V (6.7) We must introduce K, the set of validator key tuples. This is a combination of a set of cryptographic public keys and metadata which is an opaque octet sequence, but utilized to specify practical identifiers for the validator, not least a hardware address. The set of validator keys itself is equivalent to the set of 336-octet sequences. However, for clarity, we divide the sequence into four easily denoted components. For any validator key k, the Bandersnatch key is denoted k b, and is equivalent to the first 32-octets; the Ed25519 key, k e, is the second 32 octets; the bls key denoted k BLS is equivalent to the following 144 octets, and finally the metadata k m is the last 128 octets. Formally: K ≡ Y 336 (6.8) ∀ k ∈ K ∶ k b ∈ H B ≡ k 0 ⋅⋅⋅+ 32 (6.9) ∀ k ∈ K ∶ k e ∈ H E ≡ k 32 ⋅⋅⋅+ 32 (6.10) ∀ k ∈ K ∶ k BLS ∈ Y BLS ≡ k 64 ⋅⋅⋅+ 144 (6.11) ∀ k ∈ K ∶ k m ∈ Y 128 ≡ k 208 ⋅⋅⋅+ 128 (6.12) JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 14 With a new epoch under regular conditions, validator keys get rotated and the epoch’s Bandersnatch key root is updated into γ ′ z : (γ ′ k, κ ′, λ ′, γ ′ z) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (Φ (ι), γ k, κ, z) if e ′ > e (γ k, κ, λ, γ z) otherwise (6.13) where z = O ([ k b S k < − γ ′ k ]) Φ (k) ≡  [ 0, 0 ,. .. ] if k e ∈ ψ ′ o k otherwise ¡ W k < − k (6.14) Note that on epoch changes the posterior queued validator key set γ ′ k is defined such that incoming keys belonging to the offenders ψ ′ o are replaced with a null key containing only zeroes. The origin of the offenders is explained in section 10.

6.4. Sealing and Entropy Accumulation
The header must contain a valid seal and valid vrf output. These are two signatures both using the current slot’s seal key; the message data of the former is the header’s serialization omitting the seal component H s, whereas the latter is used as a bias-resistant entropy source and thus its message must already have been fixed: we use the entropy stemming from the vrf of the seal signature. Formally: let i = γ ′ s [ H t ] ↺ ∶ γ ′ s ∈ ⟦ C ⟧ Ô⇒ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ i y = Y (H s), H s ∈ F E U (H) H a ⟨ X T ⌢ η ′ 3 i r ⟩, T = 1 (6.15) γ ′ s ∈ ⟦ H B ⟧ Ô⇒ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ i = H a, H s ∈ F E U (H) H a ⟨ X F ⌢ η ′ 3 ⟩, T = 0 (6.16) H v ∈ F [] H a ⟨ X E ⌢ Y (H s)⟩ (6.17) X E = $jam_entropy (6.18) X F = $jam_fallback_seal (6.19) X T = $jam_ticket_seal (6.20) Sealing using the ticket is of greater security, and we utilize this knowledge when determining a candidate block on which to extend the chain, detailed in section 19. We thus note that the block was sealed under the regular security with the boolean marker T. We define this only for the purpose of ease of later specification. In addition to the entropy accumulator η 0, we retain three additional historical values of the accumulator at the point of each of the three most recently ended epochs, η 1, η 2 and η 3. The second-oldest of these η 2 is utilized to help ensure future entropy is unbiased (see equation 6.29) and seed the fallback seal-key generation function with randomness (see equation 6.24). The oldest is used to regenerate this randomness when verifying the seal above (see equations 6.16 and 6.15). η ∈ ⟦ H ⟧ 4 (6.21) η 0 defines the state of the randomness accumulator to which the provably random output of the vrf, the signature over some unbiasable input, is combined each block. η 1, η 2 and η 3 meanwhile retain the state of this accumulator at the end of the three most recently ended epochs in order. η ′ 0 ≡ H (η 0 ⌢ Y (H v)) (6.22) On an epoch transition (identified as the condition e ′ > e), we therefore rotate the accumulator value into the history η 1, η 2 and η 3 : (η ′ 1, η ′ 2, η ′ 3) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (η 0, η 1, η 2) if e ′ > e (η 1, η 2, η 3) otherwise (6.23)

9.4. Service Privileges
Up to three services may be recognized as privileged. The portion of state in which this is held is denoted χ and has three service index components together with a gas limit. The first, χ m, is the index of the manager service which is the service able to effect an alteration of χ from block to block. The following two, χ a and χ v, are each the indices of services able to alter φ and ι from block to block. Finally, χ g is a small dictionary containing the indices of services which automatically accumulate in each block together with a basic amount of gas with which each accumulates. Formally: χ ≡ ⎧ ⎩ χ m ∈ N S, χ a ∈ N S, χ v ∈ N S, χ g ∈ D ⟨ N S → N G ⟩ ⎫ ⎭ (9.9)

6.5. The Slot Key Sequence
The posterior slot key sequence γ ′ s is one of three expressions depending on the circumstance of the block. If the block is not the first in an epoch, then it remains unchanged from the prior γ s. If the block signals the next epoch (by epoch index) and the previous block’s slot was within the closing period of the previous epoch, then it takes the value of the prior ticket accumulator γ a. Otherwise, it takes the value of the fallback key sequence. Formally: γ ′ s ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ Z (γ a) if e ′ = e + 1 ∧ m ≥ Y ∧ S γ a S = E γ s if e ′ = e F (η ′ 2, κ ′) otherwise (6.24) Here, we use Z as the outside-in sequencer function, defined as follows: (6.25) Z ∶  ⟦ C ⟧ E → ⟦ C ⟧ E s ↦ [ s 0, s S s S − 1, s 1, s S s S − 2 ,. .. ] Finally, F is the fallback key sequence function which selects an epoch’s worth of validator Bandersnatch keys (⟦ H B ⟧ E) from the validator key set k using the entropy collected on-chain r : (6.26) F ∶ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ⎧ ⎩ H, ⟦ K ⟧ ⎫ ⎭ → ⟦ H B ⟧ E ⎧ ⎩ r, k ⎫ ⎭ ↦  k [ E − 1 (H 4 (r ⌢ E 4 (i)))] ↺ b U i ∈ N E 

6.6. The Markers
The epoch and winning-tickets markers are information placed in the header in order to minimize data transfer necessary to determine the validator keys associated with any given epoch. They are particularly useful to nodes which do not synchronize the entire state for any given block since they facilitate the secure tracking of changes to the validator key sets using only the chain of headers. As mentioned earlier, the header’s epoch marker H e is either empty or, if the block is the first in a new epoch, then a tuple of the next and current epoch randomness, along with a sequence of tuples containing both Bandersnatch keys and Ed25519 keys for each validator defining the validator keys beginning in the next epoch. Formally: H e ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (η 0, η 1, [ ⎧ ⎩ k b, k e ⎫ ⎭ S k < − γ ′ k ]) if e ′ > e ∅ otherwise (6.27) The winning-tickets marker H w is either empty or, if the block is the first after the end of the submission period for tickets and if the ticket accumulator is saturated, then the final sequence of ticket identifiers. Formally: H w ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ Z (γ a) if e ′ = e ∧ m < Y ≤ m ′ ∧ S γ a S = E ∅ otherwise (6.28) JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 15

6.7. The Extrinsic and Tickets
The extrinsic E T is a sequence of proofs of valid tickets; a ticket implies an entry in our epochal “contest” to determine which validators are privileged to author a block for each timeslot in the following epoch. Tickets specify an entry index together with a proof of ticket’s validity. The proof implies a ticket identifier, a high-entropy unbiasable 32-octet sequence, which is used both as a score in the aforementioned contest and as input to the on-chain vrf. Towards the end of the epoch (i.e. Y slots from the start) this contest is closed implying successive blocks within the same epoch must have an empty tickets extrinsic. At this point, the following epoch’s seal key sequence becomes fixed. We define the extrinsic as a sequence of proofs of valid tickets, each of which is a tuple of an entry index (a natural number less than N) and a proof of ticket validity. Formally: E T ∈ D ⎧ ⎪ ⎪ ⎩ r ∈ N N, p ∈ ¯ F [] γ ′ z ⟨ X T ⌢ η ′ 2 r ⟩ ⎫ ⎪ ⎪ ⎭ I (6.29) S E T S ≤ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ K if m ′ < Y 0 otherwise (6.30) We define n as the set of new tickets, with the ticket identifier, a hash, defined as the output component of the Bandersnatch Ring vrf proof: n ≡ [ ⎧ ⎩ y ▸ ▸ Y (i p), r ▸ ▸ i r ⎫ ⎭ S i < − E T ] (6.31) The tickets submitted via the extrinsic must already have been placed in order of their implied identifier. Duplicate identifiers are never allowed lest a validator submit the same ticket multiple times: n = [ x y _ _ x ∈ n ] (6.32) { x y S x ∈ n } ⫰ { x y S x ∈ γ a } (6.33) The new ticket accumulator γ ′ a is constructed by merging new tickets into the previous accumulator value (or the empty sequence if it is a new epoch): (6.34) γ ′ a ≡ Ð Ð Ð Ð Ð Ð Ð Ð Ð Ð Ð Ð Ð Ð Ð Ð Ð Ð Ð Ð → ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ x y ^ ^ ^ ^ ^ ^ x ∈ n ∪ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ∅ if e ′ > e γ a otherwise ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ E The maximum size of the ticket accumulator is E. On each block, the accumulator becomes the lowest items of the sorted union of tickets from prior accumulator γ a and the submitted tickets. It is invalid to include useless tickets in the extrinsic, so all submitted tickets must exist in their posterior ticket accumulator. Formally: n ⊆ γ ′ a (6.35) Note that it can be shown that in the case of an empty extrinsic E T = [], as implied by m ′ ≥ Y, and unchanged epoch (e ′ = e), then γ ′ a = γ a.

8.1. Authorizers and Authorizations
The authorization system involves three key concepts: Authorizers, Tokens and Traces. A Token is simply a piece of opaque data to be included with a work-package to help make an argument that the work-package should be authorized. Similarly, a Trace is a piece of opaque data which helps characterize or describe some successful authorization. An Authorizer meanwhile, is a piece of logic which executes within some pre-specified and well-known computational JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 16 limits and determines whether a work-package—including its Token—is authorized for execution on some particular core and yields a Trace on success. Authorizers are identified as the hash of their pvm code concatenated with their Configuration blob, the latter being, like Tokens and Traces, opaque data meaningful to the pvm code. The process by which work-packages are determined to be authorized (or not) is not the competence of on-chain logic and happens entirely in-core and as such is discussed in section 14.3. However, on-chain logic must identify each set of authorizers assigned to each core in order to verify that a work-package is legitimately able to utilize that resource. It is this subsystem we will now define.

8.2. Pool and Queue
We define the set of authorizers allowable for a particular core c as the authorizer pool α [ c ]. To maintain this value, a further portion of state is tracked for each core: the core’s current authorizer queue φ [ c ], from which we draw values to fill the pool. Formally: (8.1) α ∈ C⟦ H ⟧ ∶ O H C, φ ∈ C⟦ H ⟧ Q H C Note: The portion of state φ may be altered only through an exogenous call made from the accumulate logic of an appropriately privileged service. The state transition of a block involves placing a new authorization into the pool from the queue: ∀ c ∈ N C ∶ α ′ [ c ] ≡ ←Ð Ð Ð Ð Ð Ð Ð Ð Ð Ð ÐÐ F (c) φ ′ [ c ][ H t ] ↺ O (8.2) F (c) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ α [ c ] m {(g w) a } if ∃ g ∈ E G ∶ (g w) c = c α [ c ] otherwise (8.3) Since α ′ is dependent on φ ′, practically speaking, this step must be computed after accumulation, the stage in which φ ′ is defined. Note that we utilize the guarantees extrinsic E G to remove the oldest authorizer which has been used to justify a guaranteed work-package in the current block. This is further defined in equation 11.23.

9.1. Code and Gas
The code and associated metadata of a service account is identified by a hash which, if the service is to be functional, must be present within its preimage lookup (see section 9.2). We thus define the actual code c and metadata m : ∀ a ∈ A ∶ E (↕ a m, a c) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ a p [ a c ] if a c ∈ a p ∅ otherwise (9.4) There are three entry-points in the code: 0 refine : Refinement, executed in-core and stateless. 10 1 accumulate : Accumulation, executed on-chain and stateful. 2 on_transfer : Transfer handler, executed onchain and stateful. Whereas the first, executing in-core, is described in more detail in section 14.3, the latter two are defined in the present section. As stated in appendix A, execution time in the Jam virtual machine is measured deterministically in units of gas, represented as a natural number less than 2 64 and formally denoted N G. We may also use Z G to denote the set Z − 2 63 ... 2 63 if the quantity may be negative. There are two limits specified in the account, g, the minimum gas required in order to execute the Accumulate entry-point of the service’s code, and m, the minimum required for the On Transfer entry-point.

9.2. Preimage Lookups
In addition to storing data in arbitrary key/value pairs available only on-chain, an account may also solicit data to be made available also incore, and thus available to the Refine logic of the service’s code. State concerning this facility is held under the service’s p and l components. There are several differences between preimage-lookups and storage. Firstly, preimage-lookups act as a mapping from a hash to its preimage, whereas general storage maps arbitrary keys to values. Secondly, preimage data is supplied extrinsically, whereas storage data originates as part of the service’s accumulation. Thirdly preimage data, once supplied, may not be removed freely; instead it goes through a process of being marked as unavailable, and only after a period of time may it be removed from state. This ensures that historical information on its existence is retained. The final point especially is important since preimage data is designed to be queried in-core, under the Refine logic of the service’s code, and thus it is important that the historical availability of the preimage is known. We begin by reformulating the portion of state concerning our data-lookup system. The purpose of this system is to provide a means of storing static data on-chain such that it may later be made available within the execution 10 Technically there is some small assumption of state, namely that some modestly recent instance of each service’s preimages. The specifics of this are discussed in section 14.3. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 17 of any service code as a function accepting only the hash of the data and its length in octets. During the on-chain execution of the Accumulate function, this is trivial to achieve since there is inherently a state which all validators verifying the block necessarily have complete knowledge of, i.e. σ. However, for the incore execution of Refine, there is no such state inherently available to all validators; we thus name a historical state, the lookup anchor which must be considered recently finalized before the work’s implications may be accumulated hence providing this guarantee. By retaining historical information on its availability, we become confident that any validator with a recently finalized view of the chain is able to determine whether any given preimage was available at any time within the period where auditing may occur. This ensures confidence that judgments will be deterministic even without consensus on chain state. Restated, we must be able to define some historical lookup function Λ which determines whether the preimage of some hash h was available for lookup by some service account a at some timeslot t, and if so, provide its preimage: (9.5) Λ ∶  (A, N H t − D ... H t, H) → Y ? (a, t, H (p)) ↦ v ∶ v ∈ { p, ∅ } This function is defined shortly below in equation 9.7. The preimage lookup for some service of index s is denoted δ [ s ] p is a dictionary mapping a hash to its corresponding preimage. Additionally, there is metadata associated with the lookup denoted δ [ s ] l which is a dictionary mapping some hash and presupposed length into historical information. 9.2.1. Invariants. The state of the lookup system naturally satisfies a number of invariants. Firstly, any preimage value must correspond to its hash, equation 9.6. Secondly, a preimage value being in state implies that its hash and length pair has some associated status, also in equation 9.6. Formally: (9.6) ∀ a ∈ A, (h ↦ p) ∈ a p ⇒ h = H (p) ∧ ⎧ ⎩ h, S p S ⎫ ⎭ ∈ K (a l) 9.2.2. Semantics. The historical status component h ∈ ⟦ N T ⟧ ∶ 3 is a sequence of up to three time slots and the cardinality of this sequence implies one of four modes: ● h = [] : The preimage is requested, but has not yet been supplied. ● h ∈ ⟦ N T ⟧ 1 : The preimage is available and has been from time h 0. ● h ∈ ⟦ N T ⟧ 2 : The previously available preimage is now unavailable since time h 1. It had been available from time h 0. ● h ∈ ⟦ N T ⟧ 3 : The preimage is available and has been from time h 2. It had previously been available from time h 0 until time h 1. The historical lookup function Λ may now be defined as: (9.7) Λ ∶ (A, N T, H) → Y ? Λ (a, t, h) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ a p [ h ] if h ∈ K (a p) ∧ I (a l [ h, S a p [ h ]S], t) ∅ otherwise where I (l, t) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩  if [] = l x ≤ t if [ x ] = l x ≤ t < y if [ x, y ] = l x ≤ t < y ∨ z ≤ t if [ x, y, z ] = l

11.2. Package Availability Assurances
We first define ρ ‡, the intermediate state to be utilized next in section 11.4 as well as W, the set of available work-reports, which will we utilize later in section 12. Both require the integration of information from the assurances extrinsic E A. 11.2.1. The Assurances Extrinsic. The assurances extrinsic is a sequence of assurance values, at most one per validator. Each assurance is a sequence of binary values (i.e. a bitstring), one per core, together with a signature and the index of the validator who is assuring. A value of 1 (or ⊺, if interpreted as a Boolean) at any given index implies that the validator assures they are contributing to its availability. 12 Formally: E A ∈ ⟦ ⎧ ⎩ a ∈ H, f ∈ B C, v ∈ N V, s ∈ E ⎫ ⎭ ⟧ ∶ V (11.10) The assurances must all be anchored on the parent and ordered by validator index: ∀ a ∈ E A ∶ a a = H p (11.11) ∀ i ∈ { 1. .. S E A S} ∶ E A [ i − 1 ] v < E A [ i ] v (11.12) The signature must be one whose public key is that of the validator assuring and whose message is the serialization of the parent hash H p and the aforementioned bitstring: ∀ a ∈ E A ∶ a s ∈ E κ [ a v ] e ⟨ X A ⌢ H (E (H p, a f))⟩ (11.13) X A ≡ $jam_available (11.14) A bit may only be set if the corresponding core has a report pending availability on it: (11.15) ∀ a ∈ E A, c ∈ N C ∶ a f [ c ] ⇒ ρ † [ c ] ≠ ∅ 11.2.2. Available Reports. A work-report is said to become available if and only if there are a clear 2 / 3 supermajority of validators who have marked its core as set within the block’s assurance extrinsic. Formally, we define the sequence of newly available work-reports W as: W ≡ ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ ρ † [ c ] w R R R R R R R R R R R c < − N C, ∑ a ∈ E A a f [ c ] > 2 ~ 3 V ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ (11.16) This value is utilized in the definition of both δ ′ and ρ ‡ which we will define presently as equivalent to ρ † except for the removal of items which are either now available or have timed out: ∀ c ∈ N C ∶ ρ ‡ [ c ] ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ∅ if ρ [ c ] w ∈ W ∨ H t ≥ ρ † [ c ] t + U ρ † [ c ] otherwise (11.17)

11.3. Guarantor Assignments
Every block, each core has three validators uniquely assigned to guarantee workreports for it. This is borne out with V = 1, 023 validators and C = 341 cores, since V ~ C = 3. The core index assigned to each of the validators, as well as the validators’ Ed25519 keys are denoted by G : (11.18) G ∈ (⟦ N C ⟧ N V, ⟦ H K ⟧ N V) We determine the core to which any given validator is assigned through a shuffle using epochal entropy and a periodic rotation to help guard the security and liveness of the network. We use η 2 for the epochal entropy rather than η 1 to avoid the possibility of fork-magnification where uncertainty about chain state at the end of an epoch could give rise to two established forks before it naturally resolves. We define the permute function P, the rotation function R and finally the guarantor assignments G as follows: R (c, n) ≡ [(x + n) mod C S x < − c ] (11.19) P (e, t) ≡ R  F  C ⋅ i V  V i < − N V , e , t mod E R  (11.20) G ≡ (P (η ′ 2, τ ′), Φ (κ ′)) (11.21) We also define G ∗, which is equivalent to the value G as it would have been under the previous rotation: (11.22) let (e, k) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (η ′ 2, κ ′) if  τ ′ − R E  =  τ ′ E  (η ′ 3, λ ′) otherwise G ∗ ≡ (P (e, τ ′ − R), Φ (k)) 12 This is a “soft” implication since there is no consequence on-chain if dishonestly reported. For more information on this implication see section 16. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 21

11.4. Work Report Guarantees
We begin by defining the guarantees extrinsic, E G, a series of guarantees, at most one for each core, each of which is a tuple of a work-report, a credential a and its corresponding timeslot t. The core index of each guarantee must be unique and guarantees must be in ascending order of this. Formally: E G ∈ C ⎧ ⎩ w ∈ W, t ∈ N T, a ∈ ⟦ ⎧ ⎩ N V, E ⎫ ⎭ ⟧ 2 ∶ 3 ⎫ ⎭ H ∶ C (11.23) E G = [(g w) c ^ ^ g ∈ E G ] (11.24) The credential is a sequence of two or three tuples of a unique validator index and a signature. Credentials must be ordered by their validator index: ∀ g ∈ E G ∶ g a = [ v _ _ ⎧ ⎩ v, s ⎫ ⎭ ∈ g a ] (11.25) The signature must be one whose public key is that of the validator identified in the credential, and whose message is the serialization of the hash of the work-report. The signing validators must be assigned to the core in question in either this block G if the timeslot for the guarantee is in the same rotation as this block’s timeslot, or in the most recent previous set of assignments, G ∗ : ∀ (w, t, a) ∈ E G, ∀ (v, s) ∈ a ∶ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ s ∈ E (k v) e ⟨ X G ⌢ H (w)⟩ c v = w c ∧ R (τ ′ ~ R  − 1) ≤ t ≤ τ ′ k ∈ R ⇔ ∃ (w, t, a) ∈ E G, ∃ (v, s) ∈ a ∶ k = (k v) e where (c, k) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ G if  τ ′ R  = t R  G ∗ otherwise (11.26) X G ≡ $jam_guarantee (11.27) We note that the Ed25519 key of each validator whose signature is in a credential is placed in the reporters set R. This is utilized by the validator activity statistics bookkeeping system section 13. We denote w to be the set of work-reports in the present extrinsic E : let w = { g w S g ∈ E G } (11.28) No reports may be placed on cores with a report pending availability on it. A report is valid only if the authorizer hash is present in the authorizer pool of the core on which the work is reported. Formally: (11.29) ∀ w ∈ w ∶ ρ ‡ [ w c ] = ∅ ∧ w a ∈ α [ w c ] We require that the gas allotted for accumulation of each work-digest in each work-report respects its service’s minimum gas requirements. We also require that all workreports’ total allotted accumulation gas is no greater than the overall gas limit G A : (11.30) ∀ w ∈ w ∶ ∑ r ∈ w r (r g) ≤ G A ∧ ∀ r ∈ w r ∶ r g ≥ δ [ r s ] g 11.4.1. Contextual Validity of Reports. For convenience, we define two equivalences x and p to be, respectively, the set of all contexts and work-package hashes within the extrinsic: (11.31) let x ≡ { w x S w ∈ w }, p ≡ {(w s) h S w ∈ w } There must be no duplicate work-package hashes (i.e. two work-reports of the same package). Therefore, we require the cardinality of p to be the length of the workreport sequence w : (11.32) S p S = S w S We require that the anchor block be within the last H blocks and that its details be correct by ensuring that it appears within our most recent blocks β † : ∀ x ∈ x ∶ ∃ y ∈ β † ∶ x a = y h ∧ x s = y s ∧ x b = M R (y b) (11.33) We require that each lookup-anchor block be within the last L timeslots: ∀ x ∈ x ∶ x t ≥ H t − L (11.34) We also require that we have a record of it; this is one of the few conditions which cannot be checked purely with on-chain state and must be checked by virtue of retaining the series of the last L headers as the ancestor set A. Since it is determined through the header chain, it is still deterministic and calculable. Formally: ∀ x ∈ x ∶ ∃ h ∈ A ∶ h t = x t ∧ H (h) = x l (11.35) We require that the work-package of the report not be the work-package of some other report made in the past. We ensure that the work-package not appear anywhere within our pipeline. Formally: let q = {(w x) p S q ∈ ϑ, (w, d) ∈ q } (11.36) let a = {((i w) x) p S i ∈ ρ, i ≠ ∅ } (11.37) ∀ p ∈ p, p ~ ∈ ⋃ x ∈ β K (x p) ∪ ⋃ x ∈ ξ x ∪ q ∪ a (11.38) We require that the prerequisite work-packages, if present, and any work-packages mentioned in the segment-root lookup, be either in the extrinsic or in our recent history. ∀ w ∈ w, ∀ p ∈ (w x) p ∪ K (w l) ∶ p ∈ p ∪ { x S x ∈ K (b p), b ∈ β } (11.39) We require that any segment roots mentioned in the segment-root lookup be verified as correct based on our recent work-package history and the present block: let p = {((g w) s) h ↦ ((g w) s) e S g ∈ E G } (11.40) ∀ w ∈ w ∶ w l ⊆ p ∪ ⋃ b ∈ β b p (11.41) (Note that these checks leave open the possibility of accepting work-reports in apparent dependency loops. We do not consider this a problem: the pre-accumulation stage effectively guarantees that accumulation never happens in these cases and the reports are simply ignored.) Finally, we require that all work-digests within the extrinsic predicted the correct code hash for their corresponding service: ∀ w ∈ w, ∀ r ∈ w r ∶ r c = δ [ r s ] c (11.42)

11.5. Transitioning for Reports
We define ρ ′ as being equivalent to ρ ‡, except where the extrinsic replaced an entry. In the case an entry is replaced, the new value includes the present time τ ′ allowing for the value to be replaced without respect to its availability once sufficient time has elapsed (see equation 11.29). (11.43) ∀ c ∈ N C ∶ ρ ′ [ c ] ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ⎧ ⎩ w, t ▸ ▸ τ ′ ⎫ ⎭ if ∃ ⎧ ⎩ c, w, a ⎫ ⎭ ∈ E G ρ ‡ [ c ] otherwise This concludes the section on reporting and assurance. We now have a complete definition of ρ ′ together with W to be utilized in section 12, describing the portion of the state transition happening once a work-report is guaranteed and made available. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 22

12.1. History and Queuing
Accumulation of a workpackage/work-report is deferred in the case that it has a not-yet-fulfilled dependency and is cancelled entirely in the case of an invalid dependency. Dependencies are specified as work-package hashes and in order to know which work-packages have been accumulated already, we maintain a history of what has been accumulated. This history, ξ, is sufficiently large for an epoch worth of work-reports. Formally: ξ ∈ ⟦{ H }⟧ E (12.1) © ξ ≡ ⋃ x ∈ ξ (x) (12.2) We also maintain knowledge of ready (i.e. available and/or audited) but not-yet-accumulated work-reports in the state item ϑ. Each of these were made available at most one epoch ago but have or had unfulfilled dependencies. Alongside the work-report itself, we retain its unaccumulated dependencies, a set of work-package hashes. Formally: ϑ ∈ ⟦⟦(W, { H })⟧⟧ E (12.3) The newly available work-reports, W, are partitioned into two sequences based on the condition of having zero prerequisite work-reports. Those meeting the condition, W !, are accumulated immediately. Those not, W Q, are for queued execution. Formally: W ! ≡ [ w S w < − W, S(w x) p S = 0 ∧ w l = {}] (12.4) W Q ≡ E ([ D (w) S w < − W, S(w x) p S > 0 ∨ w l ≠ {}], © ξ) (12.5) D (w) ≡ (w, {(w x) p } ∪ K (w l)) (12.6) We define the queue-editing function E, which is essentially a mutator function for items such as those of ϑ, parameterized by sets of now-accumulated work-package hashes (those in ξ). It is used to update queues of workreports when some of them are accumulated. Functionally, it removes all entries whose work-report’s hash is in the set provided as a parameter, and removes any dependencies which appear in said set. Formally: (12.7) E ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (⟦(W, { H })⟧, { H }) → ⟦(W, { H })⟧ (r, x) ↦ (w, d ∖ x) W  (w, d) < − r, (w s) h ~ ∈ x We further define the accumulation priority queue function Q, which provides the sequence of work-reports which are accumulatable given a set of not-yet-accumulated work-reports and their dependencies. (12.8) Q ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ⟦(W, { H })⟧ → ⟦ W ⟧ r ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ [] if g = [] g ⌢ Q (E (r, P (g))) otherwise where g = [ w S (w, {}) < − r ] Finally, we define the mapping function P which extracts the corresponding work-package hashes from a set of work-reports: (12.9) P ∶  { W } → { H } w ↦ {(w s) h S w ∈ w } We may now define the sequence of accumulatable work-reports in this block as W ∗ : let m = H t mod E (12.10) W ∗ ≡ W ! ⌢ Q (q) (12.11) where q = E (Ï ϑ m... ⌢ Ï ϑ ...m ⌢ W Q, P (W !)) (12.12)

12.2. Execution
We work with a limited amount of gas per block and therefore may not be able to process all items in W ∗ in a single block. There are two slightly antagonistic factors allowing us to optimize the amount of work-items, and thus work-reports, accumulated in a single block: Firstly, while we have a well-known gas-limit for each work-item to be accumulated, accumulation may still result in a lower amount of gas used. Only after a work-item is accumulated can it be known if it uses less gas than the advertised limit. This implies a sequential execution pattern. Secondly, since pvm setup cannot be expected to be zero-cost, we wish to amortize this cost over as many work-items as possible. This can be done by aggregating work-items associated with the same service into the same pvm invocation. This implies a non-sequential execution pattern. We resolve this by defining a function ∆ + which accumulates work-reports sequentially, and which itself utilizes a function ∆ ∗ which accumulates work-reports in a nonsequential, service-aggregated manner. Only once all such accumulation is executed do we integrate the results and thus define the relevant posterior state items. In doing so we also integrate the consequences of any deferred-transfers implied by accumulation. Our formalisms begin by defining U as a characterization of (i.e. values capable of representing) state components which are both needed and mutable by the accumulation process. This comprises the service accounts state (as in δ), the upcoming validator keys ι, the queue of authorizers φ and the privileges state χ. Formally: (12.13) U ≡ ⎛ ⎝ d ∈ D ⟨ N S → A ⟩, i ∈ ⟦ K ⟧ V, q ∈ C⟦ H ⟧ Q H C, x ∈ (N S, N S, N S, D ⟨ N S → N G ⟩) ⎞ ⎠ JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 23 We denote the set characterizing a deferred transfer as T, noting that a transfer includes a memo component m of W T = 128 octets, together with the service index of the sender s, the service index of the receiver d, the balance to be transferred a and the gas limit g for the transfer. Formally: T ≡ ⎧ ⎩ s ∈ N S, d ∈ N S, a ∈ N B, m ∈ Y W T, g ∈ N G ⎫ ⎭ (12.14) Finally, we denote the set of service/hash pairs, utilized as a service-indexed commitment to the accumulation output, as B : (12.15) B ≡ {(N S, H)} U ≡ ⟦ ⎧ ⎩ N S, N G ⎫ ⎭ ⟧ We define the outer accumulation function ∆ + which transforms a gas-limit, a sequence of work-reports, an initial partial-state and a dictionary of services enjoying free accumulation, into a tuple of the number of workresults accumulated, a posterior state-context, the resultant deferred-transfers and accumulation-output pairings: (12.16) ∆ + ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (N G, ⟦ W ⟧, U, D ⟨ N S → N G ⟩) → (N, U, ⟦ T ⟧, B, U) (g, w, o, f) ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (0, o, [], {}, []) if i = 0 (i + j, o ′, t ∗ ⌢ t, b ∗ ∪ b, u ∗ ⌢ u) o/w where i = max (N S w S + 1) ∶ ∑ w ∈ w ...i ∑ r ∈ w r (r g) ≤ g and (o ∗, t ∗, b ∗, u ∗) = ∆ ∗ (o, w ...i, f) and (j, o ′, t, b, u) = ∆ + (g − ∑ (s,u) ∈ u ∗ u, w i..., o ∗, {}) We come to define the parallelized accumulation function ∆ ∗ which, with the help of the single-service accumulation function ∆ 1, transforms an initial state-context, together with a sequence of work-reports and a dictionary of privileged always-accumulate services, into a tuple of the total gas utilized in pvm execution u, a posterior statecontext (x ′, d ′, i ′, q ′) and the resultant accumulationoutput pairings b and deferred-transfers Ì t : (12.17) ∆ ∗ ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (U, ⟦ W ⟧, D ⟨ N S → N G ⟩) → (U, ⟦ T ⟧, B, U) (o, w, f) ↦ ((d ′, i ′, q ′, x ′), Ì t, b, u, p ∗) where: s = { r s S w ∈ w, r ∈ w r } ∪ K (f) u = [(s, ∆ 1 (o, w, f, s) u) S s < − s ] b = {(s, b) S s ∈ s, b = ∆ 1 (o, w, f, s) b, b ≠ ∅ } t = [ ∆ 1 (o, w, f, s) t S s < − s ] p = ⋃ s < − s ∆ 1 (o, w, f, s) p d ′ = P ((d ∪ n) ∖ m, p) (d, i, q, (m, a, v, z)) = o x ′ = (∆ 1 (o, w, f, m) o) x i ′ = (∆ 1 (o, w, f, v) o) i q ′ = (∆ 1 (o, w, f, a) o) q n = ⋃ s ∈ s ({(∆ 1 (o, w, f, s) o) d ∖ K (d ∖ { s })}) m = ⋃ s ∈ s (K (d) ∖ K ((∆ 1 (o, w, f, s) o) d)) And P is the preimage integration function, which transforms a dictionary of service states and a set of service/hash pairs into a new dictionary of service states. Preimage provisions into services which no longer exist or whose relevant request is dropped are disregarded: (12.18) P ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (D ⟨ N S → A ⟩, { ⎧ ⎩ N S, Y ⎫ ⎭ }) → D ⟨ N S → A ⟩ (d, p) ↦ d ′ where d ′ = d except: ∀ ⎧ ⎩ s, i ⎫ ⎭ ∈ p, s ∈ K (d), d [ s ] l [ H (i), S i S] = [] ∶ d ′ [ s ] l [ H (i), S i S] = τ ′ d ′ [ s ] p [ H i ] = i We note that while forming the union of all altered, newly added service and newly removed indices, defined in the above context as K (n) ∪ m, different services may not each contribute the same index for a new, altered or removed service. This cannot happen for the set of removed and altered services since the code hash of removable services has no known preimage and thus cannot execute itself to make an alteration. For new services this should also never happen since new indices are explicitly selected to avoid such conflicts. In the unlikely event it does happen, the block must be considered invalid. The single-service accumulation function, ∆ 1, transforms an initial state-context, sequence of work-reports and a service index into an alterations state-context, a sequence of transfers, a possible accumulation-output and the actual pvm gas used. This function wrangles the workitems of a particular service from a set of work-reports and invokes pvm execution with said data: (12.19) O ≡ ⎧ ⎩ h ∈ H, e ∈ H, a ∈ H, o ∈ Y, y ∈ H, g ∈ N G, d ∈ Y ∪ J ⎫ ⎭ ∆ 1 ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ U, ⟦ W ⟧, D ⟨ N S → N G ⟩, N S ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ → ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ o ∈ U, t ∈ ⟦ T ⟧, b ∈ H ?, u ∈ N G, p ∈ { ⎧ ⎩ N S, Y ⎫ ⎭ } ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ (o, w, f, s) ↦ Ψ A (o, τ ′, s, g, i) where: g = U (f s, 0) + ∑ w ∈ w, r ∈ w r, r s = s (r g) i =  ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ d ▸ ▸ r d, g ▸ ▸ r g, y ▸ ▸ r y, o ▸ ▸ w o, e ▸ ▸ (w s) e, h ▸ ▸ (w s) h, a ▸ ▸ w a ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ W w < − w, r < − w r, r s = s (12.20) This introduces O, the set of wrangled operand tuples, used as an operand to the pvm Accumulation function Ψ A : work-items (together with associated data in their workpackages) are rephrased into a sequence of such operand tuples p. It also draws upon g, the gas limit implied by the work-reports and gas-privileges.

12.3. Deferred Transfers and State Integration
Given the result of the top-level ∆ +, we may define the posterior state χ ′, φ ′ and ι ′ as well as the first intermediate state of the service-accounts δ † and the Beefy commitment map C : let g = max  G T, G A ⋅ C + ∑ x ∈ V (χ g) (x) (12.21) let (n, o, t, C, u) = ∆ + (g, W ∗, (χ, δ, ι, φ), χ g) (12.22) (χ ′, δ †, ι ′, φ ′) ≡ o (12.23) We compose I, our accumulation statistics, which is a mapping from the service indices which were accumulated to the amount of gas used throughout accumulation and the number of work-items accumulated. Formally: I ∈ D ⟨ N S → ⎧ ⎩ N G, N ⎫ ⎭ ⟩ (12.24) I ≡  s ↦ ⎧ ⎪ ⎩ ∑ (s,u) ∈ u (u), S N (s)S ⎫ ⎪ ⎭ T N (s) ≠ [] (12.25) JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 24 where N (s) ≡  r S w < − W ∗ ...n, r < − w r, r s = s  (12.26) Note that the accumulation commitment map C is a set of pairs of indices of the output-yielding accumulated services to their accumulation result. This is utilized in equation 7.3, when determining the accumulation-result tree root for the present block, useful for the Beefy protocol. We have denoted the sequence of implied transfers as t, ordered internally according to the source service’s execution. We define a selection function R, which maps a sequence of deferred transfers and a desired destination service index into the sequence of transfers targeting said service, ordered primarily according to the source service index and secondarily their order within t. Formally: (12.27) R ∶  (⟦ T ⟧, N S) → ⟦ T ⟧ (t, d) ↦ [ t S s < − N S, t < − t, t s = s, t d = d ] The second intermediate state δ ‡ may then be defined with all the deferred effects of the transfers applied: x = { s ↦ Ψ T (δ †, τ ′, s, R (t, s)) S (s ↦ a) ∈ δ † } (12.28) δ ‡ ≡ { s ↦ a S (s ↦ ⎧ ⎩ a, u ⎫ ⎭) ∈ x } (12.29) Furthermore we build the deferred transfers statistics value X as the number of transfers and the total gas used in transfer processing for each destination service index. Formally: X ∈ D ⟨ N S → ⎧ ⎩ N, N G ⎫ ⎭ ⟩ (12.30) X ≡  d ↦ ⎧ ⎩ S R (t, d)S, u ⎫ ⎭ W R (t, d) ≠ [], ∃ a ∶ x [ d ] = (a, u) ¡ (12.31) Note that Ψ T is defined in appendix B.5 such that it results in δ † [ d ], i.e. no difference to the account’s intermediate state, if R (d) = [], i.e. said account received no transfers. We define the final state of the ready queue and the accumulated map by integrating those work-reports which were accumulated in this block and shifting any from the prior state with the oldest such items being dropped entirely: ξ ′ E − 1 = P (W ∗ ...n) (12.32) ∀ i ∈ N E − 1 ∶ ξ ′ i ≡ ξ i + 1 (12.33) ∀ i ∈ N E ∶ ϑ ′ ↺ m − i ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ E (W Q, ξ ′ E − 1) if i = 0 [] if 1 ≤ i < τ ′ − τ E (ϑ ↺ m − i, ξ ′ E − 1) if i ≥ τ ′ − τ (12.34)

12.4. Preimage Integration
After accumulation, we must integrate all preimages provided in the lookup extrinsic to arrive at the posterior account state. The lookup extrinsic is a sequence of pairs of service indices and data. These pairs must be ordered and without duplicates (equation 12.36 requires this). The data must have been solicited by a service but not yet provided in the prior state. Formally: E P ∈ ⟦ ⎧ ⎩ N S, Y ⎫ ⎭ ⟧ (12.35) E P = [ i _ _ i ∈ E P ] (12.36) R (d, s, h, l) ⇔ h ~ ∈ d [ s ] p ∧ d [ s ] l [ ⎧ ⎩ h, l ⎫ ⎭ ] = [] (12.37) ∀ ⎧ ⎩ s, p ⎫ ⎭ ∈ E P ∶ R (δ, s, H (p), S p S) (12.38) We disregard, without prejudice, any preimages which due to the effects of accumulation are no longer useful. We define δ ′ as the state after the integration of the stillrelevant preimages: let P = {(s, p) S ⎧ ⎩ s, p ⎫ ⎭ ∈ E P, R (δ ‡, s, H (p), S p S)} (12.39) δ ′ = δ ‡ ex. ∀ ⎧ ⎩ s, p ⎫ ⎭ ∈ P ∶ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ δ ′ [ s ] p [ H (p)] = p δ ′ [ s ] l [ H (p), S p S] = [ τ ′ ] (12.40)

14.1. Honest Behavior
We have so far specified how to recognize blocks for a correctly transitioning Jam blockchain. Through defining the state transition function and a state Merklization function, we have also defined how to recognize a valid header. While it is not especially difficult to understand how a new block may be authored for any node which controls a key which would allow the creation of the two signatures in the header, nor indeed to fill in the other header fields, readers will note that the contents of the extrinsic remain unclear. We define not only correct behavior through the creation of correct blocks but also honest behavior, which involves the node taking part in several off-chain activities. This does have analogous aspects within YP Ethereum, though it is not mentioned so explicitly in said document: the creation of blocks along with the gossiping and inclusion of transactions within those blocks would all count as off-chain activities for which honest behavior is helpful. In Jam ’s case, honest behavior is well-defined and expected of at least 2 ~ 3 of validators. Beyond the production of blocks, incentivized honest behavior includes: ● the guaranteeing and reporting of work-packages, along with chunking and distribution of both the chunks and the work-package itself, discussed in section 15; ● assuring the availability of work-packages after being in receipt of their data; ● determining which work-reports to audit, fetching and auditing them, and creating and distributing judgments appropriately based on the outcome of the audit; ● submitting the correct amount of auditing work seen being done by other validators, discussed in section 13.

14.2. Segments and the Manifest
Our basic erasurecoding segment size is W E = 684 octets, derived from the fact we wish to be able to reconstruct even should almost two-thirds of our 1023 participants be malicious or incapacitated, the 16-bit Galois field on which the erasure-code is based and the desire to efficiently support encoding data of close to, but no less than, 4 kb. Work-packages are generally small to ensure guarantors need not invest a lot of bandwidth in order to discover whether they can get paid for their evaluation into a workreport. Rather than having much data inline, they instead reference data through commitments. The simplest commitments are extrinsic data. Extrinsic data are blobs which are being introduced into the system alongside the work-package itself generally by the work-package builder. They are exposed to the Refine logic as an argument. We commit to them through including each of their hashes in the work-package. Work-packages have two other types of external data associated with them: A cryptographic commitment to each imported segment and finally the number of segments which are exported. 14.2.1. Segments, Imports and Exports. The ability to communicate large amounts of data from one workpackage to some subsequent work-package is a key feature of the Jam availability system. An export segment, defined as the set G, is an octet sequence of fixed length W G = 4104. It is the smallest datum which may individually be imported from—or exported to—the long-term D 3 L during the Refine function of a work-package. Being an exact multiple of the erasure-coding piece size ensures that the data segments of work-package can be efficiently JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 26 placed in the D 3 L system. (14.1) G ≡ Y W G Exported segments are data which are generated through the execution of the Refine logic and thus are a side effect of transforming the work-package into a workreport. Since their data is deterministic based on the execution of the Refine logic, we do not require any particular commitment to them in the work-package beyond knowing how many are associated with each Refine invocation in order that we can supply an exact index. On the other hand, imported segments are segments which were exported by previous work-packages. In order for them to be easily fetched and verified they are referenced not by hash but rather the root of a Merkle tree which includes any other segments introduced at the time, together with an index into this sequence. This allows for justifications of correctness to be generated, stored, included alongside the fetched data and verified. This is described in depth in the next section. 14.2.2. Data Collection and Justification. It is the task of a guarantor to reconstitute all imported segments through fetching said segments’ erasure-coded chunks from enough unique validators. Reconstitution alone is not enough since corruption of the data would occur if one or more validators provided an incorrect chunk. For this reason we ensure that the import segment specification (a Merkle root and an index into the tree) be a kind of cryptographic commitment capable of having a justification applied to demonstrate that any particular segment is indeed correct. Justification data must be available to any node over the course of its segment’s potential requirement. At around 350 bytes to justify a single segment, justification data is too voluminous to have all validators store all data. We therefore use the same overall availability framework for hosting justification metadata as the data itself. The guarantor is able to use this proof to justify to themselves that they are not wasting their time on incorrect behavior. We do not force auditors to go through the same process. Instead, guarantors build an Auditable Work Package, and place this in the Audit da system. This is the original work-package, its extrinsic data, its imported data and a concise proof of correctness of that imported data. This tactic routinely duplicates data between the D 3 L and the Audit da, however it is acceptable in order to reduce the bandwidth cost for auditors who must justify the correctness as cheaply as possible as auditing happens on average 30 times for each work-package whereas guaranteeing happens only twice or thrice.

14.3. Packages and Items
We begin by defining a work-package, of set P, and its constituent work-item s, of set I. A work-package includes a simple blob acting as an authorization token j, the index of the service which hosts the authorization code h, an authorization code hash u and a configuration blob p, a context x and a sequence of work items w : (14.2) P ≡ ⎧ ⎩ j ∈ Y, h ∈ N S, u ∈ H, p ∈ Y, x ∈ X, w ∈ ⟦ I ⟧ 1 ∶ I ⎫ ⎭ A work item includes: s the identifier of the service to which it relates, the code hash of the service at the time of reporting h (whose preimage must be available from the perspective of the lookup anchor block), a payload blob y, gas limits for Refinement and Accumulation g & a, and the three elements of its manifest, a sequence of imported data segments i which identify a prior exported segment through an index and the identity of an exporting workpackage, x, a sequence of blob hashes and lengths to be introduced in this block (and which we assume the validator knows) and e the number of data segments exported by this work item. (14.3) I ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ s ∈ N S, h ∈ H, y ∈ Y, g ∈ N G, a ∈ N G, e ∈ N, i ∈ C ⎧ ⎪ ⎩ H ∪ (H ⊞), N ⎫ ⎪ ⎭ H, x ∈ ⟦(H, N)⟧ ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ Note that an imported data segment’s work-package is identified through the union of sets H and a tagged variant H ⊞. A value drawn from the regular H implies the hash value is of the segment-root containing the export, whereas a value drawn from H ⊞ implies the hash value is the hash of the exporting work-package. In the latter case it must be converted into a segment-root by the guarantor and this conversion reported in the work-report for on-chain validation. We limit the total number of exported items to W X = 3072, the total number of imported items to W M = 3072, and the total number of extrinsics to T = 128 : (14.4) ∀ p ∈ P ∶ ∑ w ∈ p w w e ≤ W X ∧ ∑ w ∈ p w S w i S ≤ W M ∧ ∑ w ∈ p w S w x S ≤ T We make an assumption that the preimage to each extrinsic hash in each work-item is known by the guarantor. In general this data will be passed to the guarantor alongside the work-package. We limit the total size of the implied import and extrinsic items, together with all payloads, the authorizer configuration and the authorization token to 12 mb in order to allow for around 2 mb /s/core data throughput: ∀ p ∈ P ∶ S p j S + S p S + ∑ w ∈ p w S (w) ≤ W B where S (w ∈ I) ≡ S w y S + S w i S ⋅ W G + ∑ (h,l) ∈ w x l (14.5) W B ≡ 12 ⋅ 2 20 (14.6) We limit the sums of each of the two gas limits to be at most the maximum gas allocated to a core for the corresponding operation: (14.7) ∀ p ∈ P ∶ ∑ w ∈ p w (w a) < G A ∧ ∑ w ∈ p w (w g) < G R Given the result d and gas used u of some work-item, we define the item-to-digest function C as: (14.8) C ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (I, Y ∪ J, N G) → L ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ s, h, y, a, e, i, x ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭, d, u ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ s, h, y ▸ ▸ H (y), g ▸ ▸ a, d, u, i ▸ ▸ S i S, e, x ▸ ▸ S x S, z ▸ ▸ ∑ (h,z) ∈ x z ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ We define the work-package’s implied authorizer as p a, the hash of the authorization code hash concatenated with the configuration. We define the authorization code as p c and require that it be available at the time of the lookup anchor block from the historical lookup of service p h. Formally: (14.9) ∀ p ∈ P ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ p a ≡ H (p u ⌢ p p) E (↕ p m, p c) ≡ Λ (δ [ p h ], (p x) t, p u) (p m, p c) ∈ (Y, Y) JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 27 (The historical lookup function, Λ, is defined in equation 9.7.) 14.3.1. Exporting. Any of a work-package’s work-items may export segments and a segments-root is placed in the work-report committing to these, ordered according to the work-item which is exporting. It is formed as the root of a constant-depth binary Merkle tree as defined in equation E.4. Guarantors are required to erasure-code and distribute two data sets: one blob, the auditable bundle containing the encoded work-package, extrinsic data and self-justifying imported segments which is placed in the short-term Audit da store; and a second set of exportedsegments data together with the Paged-Proofs metadata. Items in the first store are short-lived; assurers are expected to keep them only until finality of the block in which the availability of the work-result’s work-package is assured. Items in the second, meanwhile, are longlived and expected to be kept for a minimum of 28 days (672 complete epochs) following the reporting of the workreport. This latter store is referred to as the Distributed, Decentralized, Data Lake or D 3 L owing to its large size. We define the paged-proofs function P which accepts a series of exported segments s and defines some series of additional segments placed into the D 3 L via erasurecoding and distribution. The function evaluates to pages of hashes, together with subtree proofs, such that justifications of correctness based on a segments-root may be made from it: (14.10) P ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ ⟦ G ⟧ → ⟦ G ⟧ s ↦  P l (E (↕ J 6 (s, i), ↕ L 6 (s, i))) S i < − N ⌈ S s S ~ 64 ⌉  where l = W G

14.4. Computation of Work-Report
We now come to the work-report computation function Ξ. This forms the basis for all utilization of cores on Jam. It accepts some work-package p for some nominated core c and results in either an error ∇ or the work-report and series of exported segments. This function is deterministic and requires only that it be evaluated within eight epochs of a recently finalized block thanks to the historical lookup functionality. It can thus comfortably be evaluated by any node within the auditing period, even allowing for practicalities of imperfect synchronization. Formally: (14.11) Ξ ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (P, N C) → W (p, c) ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ∇ if o ~ ∈ Y ⎧ ⎩ s, x ▸ ▸ p x, c, a ▸ ▸ p a, o, l, r, g ⎫ ⎭ otherwise Where: K (l) ≡ { h S w ∈ p w, (h ⊞, n) ∈ w i }, S l S ≤ 8 (o, g) = Ψ I (p, c) (r, e) = T [(C (p w [ j ], r, u), e) S (r, u, e) = I (p, j), j < − N S p w S ] I (p, j) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (r, u, e) if S e S = w e (r, u, [ G 0, G 0 ,. .. ] ...w e) otherwise if r ~ ∈ Y (⊚, u, [ G 0, G 0 ,. .. ] ...w e) otherwise where (r, e, u) = Ψ R (j, p, o, S (w), ℓ) and h = H (p), w = p w [ j ], ℓ = ∑ k < j p w [ k ] e Note that we gracefully handle the case where number of segments actually exported by a work-item’s Refine execution is incorrectly reported in the work-item’s export segment count. In this case, the work-package continues to be valid as a whole, but the work item’s exported segments are replaced by a sequence of zero-segments equal in size to the export segment count. Initially we constrain the segment-root dictionary l : It should contain entries for all unique work-package hashes of imported segments not identified directly via a segmentroot but rather through a work-package hash. We immediately define the segment-root lookup function L, dependent on this dictionary, which collapses a union of segment-roots and work-package hashes into segment-roots using the dictionary: (14.12) L (r ∈ H ∪ H ⊞) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ r if r ∈ H l [ h ] if ∃ h ∈ H ∶ r = h ⊞ In order to expect to be compensated for a work-report they are building, guarantors must compose a value for l to ensure not only the above but also a further constraint that all pairs of work-package hashes and segment-roots do properly correspond: (14.13) ∀ (h ↦ e) ∈ l ∶ ∃ p, c ∈ P, N C ∶ H (p) = h ∧ (Ξ (p, c) s) e = e As long as the guarantor is unable to satisfy the above constraints, then it should consider the work-package unable to be guaranteed. Auditors are not expected to populate this but rather to reuse the value in the work-report they are auditing. The next term to be introduced, ⎧ ⎩ o, g ⎫ ⎭, is the authorization trace, the result of the Is-Authorized function together with the amount of gas it used. The second term, ⎧ ⎩ r, e ⎫ ⎭ is the sequence of results for each of the work-items in the work-package together with all segments exported by each work-item. The third definition I performs an ordered accumulation (i.e. counter) in order to ensure that the Refine function has access to the total number of exports made from the work-package up to the current workitem. The above relies on two functions, S and X which, respectively, define the import segment data and the extrinsic data for some work-item argument w. We also define J, which compiles justifications of segment data: (14.14) X (w ∈ I) ≡ [ d S (H (d), S d S) < − w x ] S (w ∈ I) ≡ [ b [ n ] S M (b) = L (r), (r, n) < − w i ] J (w ∈ I) ≡ [ ↕ J 0 (b, n) S M (b) = L (r), (r, n) < − w i ] We may then define s as the data availability specification of the package using these two functions together with the yet to be defined Availability Specifier function A (see section 14.4.1): (14.15) s = A (H (p), E (p, X # (p w), S # (p w), J # (p w)), Ì e) Note that while S and J are both formulated using the inner term b (all segments exported by all work-packages exporting a segment to be imported) such a vast amount of data is not generally needed as the justification can be derived through a single paged-proof. This reduces the worst case data fetching for a guarantor to two segments for every one to be imported. In the case that contiguously exported segments are imported (which we might assume JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 28 is a fairly common situation), then a single proof-page should be sufficient to justify many imported segments. Also of note is the lack of length prefixes: only the Merkle paths for the justifications have a length prefix. All other sequence lengths are determinable through the work package itself. The Is-Authorized logic it references must be executed first in order to ensure that the work-package warrants the needed core-time. Next, the guarantor should ensure that all segment-tree roots which form imported segment commitments are known and have not expired. Finally, the guarantor should ensure that they can fetch all preimage data referenced as the commitments of extrinsic segments. Once done, then imported segments must be reconstructed. This process may in fact be lazy as the Refine function makes no usage of the data until the fetch hostcall is made. Fetching generally implies that, for each imported segment, erasure-coded chunks are retrieved from enough unique validators (342, including the guarantor) and is described in more depth in appendix H. (Since we specify systematic erasure-coding, its reconstruction is trivial in the case that the correct 342 validators are responsive.) Chunks must be fetched for both the data itself and for justification metadata which allows us to ensure that the data is correct. Validators, in their role as availability assurers, should index such chunks according to the index of the segmentstree whose reconstruction they facilitate. Since the data for segment chunks is so small at 12 octets, fixed communications costs should be kept to a bare minimum. A good network protocol (out of scope at present) will allow guarantors to specify only the segments-tree root and index together with a Boolean to indicate whether the proof chunk need be supplied. Since we assume at least 341 other validators are online and benevolent, we can assume that the guarantor can compute S and J above with confidence, based on the general availability of data committed to with s ♣, which is specified below. 14.4.1. Availability Specifier. We define the availability specifier function A, which creates an availability specifier from the package hash, an octet sequence of the audit-friendly work-package bundle (comprising the workpackage itself, the extrinsic data and the concatenated import segments along with their proofs of correctness), and the sequence of exported segments: (14.16) A ∶  ⎧ ⎩ H, Y, ⟦ G ⟧ ⎫ ⎭ → S ⎧ ⎩ h, b, s ⎫ ⎭ ↦ ⎧ ⎩ h, l ▸ ▸ S b S, u, e ▸ ▸ M (s), n ▸ ▸ S s S ⎫ ⎭ where u = M B ([Ì x S x < − T [ b ♣, s ♣ ]]) and b ♣ = H # (C ⌈ S b S ~ W E ⌉ (P W E (b))) and s ♣ = M # B (T C # 6 (s ⌢ P (s))) The paged-proofs function P, defined earlier in equation 14.10, accepts a sequence of segments and returns a sequence of paged-proofs sufficient to justify the correctness of every segment. There are exactly ⌈ 1 ~ 64 ⌉ pagedproof segments as the number of yielded segments, each composed of a page of 64 hashes of segments, together with a Merkle proof from the root to the subtree-root which includes those 64 segments. The functions M and M B are the fixed-depth and simple binary Merkle root functions, defined in equations E.4 and E.3. The function C is the erasure-coding function, defined in appendix H. And P is the zero-padding function to take an octet array to some multiple of n in length: (14.17) P n ∈ N 1 ... ∶  Y → Y k ⋅ n x ↦ x ⌢ [ 0, 0, ... ] ((S x S + n − 1) mod n) + 1 ...n Validators are incentivized to distribute each newly erasure-coded data chunk to the relevant validator, since they are not paid for guaranteeing unless a work-report is considered to be available by a super-majority of validators. Given our work-package p, we should therefore send the corresponding work-package bundle chunk and exported segments chunks to each validator whose keys are together with similarly corresponding chunks for imported, extrinsic and exported segments data, such that each validator can justify completeness according to the work-report’s erasure-root. In the case of a coming epoch change, they may also maximize expected reward by distributing to the new validator set. We will see this function utilized in the next sections, for guaranteeing, auditing and judging.

17.1. Overview
The auditing process involves each node requiring themselves to fetch, evaluate and issue judgment on a random but deterministic set of workreports from each Jam chain block in which the workreport becomes available (i.e. from W). Prior to any evaluation, a node declares and proves its requirement. At specific common junctures in time thereafter, the set of work-reports which a node requires itself to evaluate from each block’s W may be enlarged if any declared intentions are not matched by a positive judgment in a reasonable time or in the event of a negative judgment being seen. These enlargement events are called tranches. If all declared intentions for a work-report are matched by a positive judgment at any given juncture, then the work-report is considered audited. Once all of any given block’s newly available work-reports are audited, then we consider the block to be audited. One prerequisite of a node finalizing a block is for it to view the block as audited. Note that while there will be eventual consensus on whether a block is audited, there may not be consensus at the time that the block gets finalized. This does not affect the crypto-economic guarantees of this system. In regular operation, no negative judgments will ultimately be found for a work-report, and there will be no direct consequences of the auditing stage. In the unlikely event that a negative judgment is found, then one of several things happens; if there are still more than 2 ~ 3 V positive judgments, then validators issuing negative judgments may receive a punishment for time-wasting. If there are greater than 1 ~ 3 V negative judgments, then the block which includes the work-report is ban-listed. It and all its descendants are disregarded and may not be built on. In all cases, once there are enough votes, a judgment extrinsic can be constructed by a block author and placed on-chain to denote the outcome. See section 10 for details on this. All announcements and judgments are published to all validators along with metadata describing the signed material. On receipt of sure data, validators are expected to update their perspective accordingly (later defined as J and A).

17.2. Data Fetching
For each work-report to be audited, we use its erasure-root to request erasure-coded chunks from enough assurers. From each assurer we fetch three items (which with a good network protocol should be done under a single request) corresponding to the workpackage super-chunks, the self-justifying imports superchunks and the extrinsic segments super-chunks. We may validate the work-package reconstruction by ensuring its hash is equivalent to the hash includes as part of the work-package specification in the work-report. We may validate the extrinsic segments through ensuring their hashes are each equivalent to those found in the relevant work-item. Finally, we may validate each imported segment as a justification must follow the concatenated segments which allows verification that each segment’s hash is included in the referencing Merkle root and index of the corresponding work-item. Exported segments need not be reconstructed in the same way, but rather should be determined in the same JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 30 manner as with guaranteeing, i.e. through the execution of the Refine logic. All items in the work-package specification field of the work-report should be recalculated from this now knowngood data and verified, essentially retracing the guarantors steps and ensuring correctness.

17.3. Selection of Reports
Each validator shall perform auditing duties on each valid block received. Since we are entering off-chain logic, and we cannot assume consensus, we henceforth consider ourselves a specific validator of index v and assume ourselves focused on some recent block B with other terms corresponding to the statetransition implied by that block, so ρ is said block’s prior core-allocation, κ is its prior validator set, H is its header &c. Practically, all considerations must be replicated for all blocks and multiple blocks’ considerations may be underway simultaneously. We define the sequence of work-reports which we may be required to audit as Q, a sequence of length equal to the number of cores, which functions as a mapping of core index to a work-report pending which has just become available, or ∅ if no report became available on the core. Formally: Q ∈ ⟦ W ? ⟧ C (17.1) Q ≡  ρ [ c ] w if ρ [ c ] w ∈ W ∅ otherwise ¡ W c < − N C (17.2) We define our initial audit tranche in terms of a verifiable random quantity s 0 created specifically for it: s 0 ∈ F [] κ [ v ] b ⟨ X U ⌢ Y (H v)⟩ (17.3) X U = $jam_audit (17.4) We may then define a 0 as the non-empty items to audit through a verifiably random selection of ten cores: a 0 = { ⎧ ⎩ c, w ⎫ ⎭ S ⎧ ⎩ c, w ⎫ ⎭ ∈ p ⋅⋅⋅+ 10, w ≠ ∅ } (17.5) where p = F ([ ⎧ ⎩ c, Q c ⎫ ⎭ S c < − N C ], r) (17.6) and r = Y (s 0) (17.7) Every A = 8 seconds following a new time slot, a new tranche begins, and we may determine that additional cores warrant an audit from us. Such items are defined as a n where n is the current tranche. Formally: (17.8) let n = T − P ⋅ H t A  New tranches may contain items from Q stemming from one of two reasons: either a negative judgment has been received; or the number of judgments from the previous tranche is less than the number of announcements from said tranche. In the first case, the validator is always required to issue a judgment on the work-report. In the second case, a new special-purpose vrf must be constructed to determine if an audit and judgment is warranted from us. In all cases, we publish a signed statement of which of the cores we believe we are required to audit (an announcement) together with evidence of the vrf signature to select them and the other validators’ announcements from the previous tranche unmatched with a judgment in order that all other validators are capable of verifying the announcement. Publication of an announcement should be taken as a contract to complete the audit regardless of any future information. Formally, for each tranche n we ensure the announcement statement is published and distributed to all other validators along with our validator index v, evidence s n and all signed data. Validator’s announcement statements must be in the set S : S ≡ E κ [ v ] e ⟨ X I n ⌢ x n ⌢ H (H)⟩ (17.9) where x n = E ([ E 2 (c) ⌢ H (w) S ⎧ ⎩ c, w ⎫ ⎭ ∈ a n ]) (17.10) X I = $jam_announce (17.11) We define A n as our perception of which validator is required to audit each of the work-reports (identified by their associated core) at tranche n. This comes from each other validators’ announcements (defined above). It cannot be correctly evaluated until n is current. We have absolute knowledge about our own audit requirements. A n ∶ W → ℘ ⟨ N V ⟩ (17.12) ∀ (c, w) ∈ a 0 ∶ v ∈ q 0 (w) (17.13) We further define J ⊺ and J  to be the validator indices who we know to have made respectively, positive and negative, judgments mapped from each work-report’s core. We don’t care from which tranche a judgment is made. J { , ⊺ } ∶ W → ℘ ⟨ N V ⟩ (17.14) We are able to define a n for tranches beyond the first on the basis of the number of validators who we know are required to conduct an audit yet from whom we have not yet seen a judgment. It is possible that the late arrival of information alters a n and nodes should reevaluate and act accordingly should this happen. We can thus define a n beyond the initial tranche through a new vrf which acts upon the set of no-show validators. ∀ n > 0 ∶ s n (w) ∈ F [] κ [ v ] b ⟨ X U ⌢ Y (H v) ⌢ H (w) n ⟩ (17.15) a n ≡ { V 256 F Y (s n (w)) 0 < m n S w ∈ Q, w ≠ ∅ } (17.16) where m n = S A n − 1 (w) ∖ J ⊺ (w)S We define our bias factor F = 2, which is the expected number of validators which will be required to issue a judgment for a work-report given a single no-show in the tranche before. Modeling by Jeff Burdges, Cevallos, et al. 2024 shows that this is optimal. Later audits must be announced in a similar fashion to the first. If audit requirements lessen on the receipt of new information (i.e. a positive judgment being returned for a previous no-show), then any audits already announced are completed and judgments published. If audit requirements raise on the receipt of new information (i.e. an additional announcement being found without an accompanying judgment), then we announce the additional audit(s) we will undertake. As n increases with the passage of time a n becomes known and defines our auditing responsibilities. We must attempt to reconstruct all work-packages and their requisite data corresponding to each work-report we must audit. This may be done through requesting erasure-coded chunks from one-third of the validators. It may also be JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 31 short-cutted through asking a cooperative third-party (e.g. an original guarantor) for the preimages. Thus, for any such work-report w we are assured we will be able to fetch some candidate work-package encoding F (w) which comes either from reconstructing erasurecoded chunks verified through the erasure coding’s Merkle root, or alternatively from the preimage of the workpackage hash. We decode this candidate blob into a workpackage. In addition to the work-package, we also assume we are able to fetch all manifest data associated with it through requesting and reconstructing erasure-coded chunks from one-third of validators in the same way as above. We then attempt to reproduce the report on the core to give e n, a mapping from cores to evaluations: (17.17) ∀ (c, w) ∈ a n ∶ e n (w) ⇔ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ w = Ξ (p, c) if ∃ p ∈ P ∶ E (p) = F (w)  otherwise Note that a failure to decode implies an invalid workreport. From this mapping the validator issues a set of judgments j n : j n = { S κ [ v ] e (X e (w) ⌢ H (w)) S (c, w) ∈ a n } (17.18) All judgments j ∗ should be published to other validators in order that they build their view of J and in the case of a negative judgment arising, can form an extrinsic for E D. We consider a work-report as audited under two circumstances. Either, when it has no negative judgments and there exists some tranche in which we see a positive judgment from all validators who we believe are required to audit it; or when we see positive judgments for it from greater than two-thirds of the validator set. U (w) ⇔ ⋁  J  (w) = ∅ ∧ ∃ n ∶ A n (w) ⊂ J ⊺ (w) S J ⊺ (w)S > 2 ~ 3 V (17.19) Our block B may be considered audited, a condition denoted U, when all the work-reports which were made available are considered audited. Formally: U ⇔ ∀ w ∈ W ∶ U (w) (17.20) For any block we must judge it to be audited (i.e. U = ⊺) before we vote for the block to be finalized in Grandpa. See section 19 for more information here. Furthermore, we pointedly disregard chains which include the accumulation of a report which we know at least 1 ~ 3 of validators judge as being invalid. Any chains including such a block are not eligible for authoring on. The best block, i.e. that on which we build new blocks, is defined as the chain with the most regular Safrole blocks which does not contain any such disregarded block. Implementationwise, this may require reversion to an earlier head or alternative fork. As a block author, we include a judgment extrinsic which collects judgment signatures together and reports them on-chain. In the case of a non-valid judgment (i.e. one which is not two-thirds-plus-one of judgments confirming validity) then this extrinsic will be introduced in a block in which accumulation of the non-valid work-report is about to take place. The non-valid judgment extrinsic removes it from the pending work-reports, ρ. Refer to section 10 for more details on this.

20.1. Technical Characteristics
In total, with our stated target of 1,023 validators and three validators per core, along with requiring a mean of ten audits per validator per timeslot, and thus 30 audits per work-report, Jam is capable of trustlessly processing and integrating 341 work-packages per timeslot. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 32 We assume node hardware is a modern 16 core cpu with 64 gb ram, 8 tb secondary storage and 0.5 g be networking. Our performance models assume a rough split of cpu time as follows: Proportion Audits 10 ~ 16 Merklization 1 ~ 16 Block execution 2 ~ 16 Grandpa and Beefy 1 ~ 16 Erasure coding 1 ~ 16 Networking & misc 1 ~ 16 Estimates for network bandwidth requirements are as follows: Throughput, mb /slot Tx Rx Guaranteeing 106 48 Assuring 144 13 Auditing 0 133 Authoring 53 87 Grandpa and Beefy 4 4 Total 304 281 Implied bandwidth, m b/s 387 357 Thus, a connection able to sustain 500 m b/s should leave a sufficient margin of error and headroom to serve other validators as well as some public connections, though the burstiness of block publication would imply validators are best to ensure that peak bandwidth is higher. Under these conditions, we would expect an overall network-provided data availability capacity of 2 pb, with each node dedicating at most 6 tb to availability storage. Estimates for memory usage are as follows: gb Auditing 20 2 × 10 pvm instances Block execution 2 1 pvm instance State cache 40 Misc 2 Total 64 As a rough guide, each parachain has an average footprint of around 2 mb in the Polkadot Relay chain; a 40 gb state would allow 20,000 parachains’ information to be retained in state. What might be called the “virtual hardware” of a Jam core is essentially a regular cpu core executing at somewhere between 25% and 50% of regular speed for the whole six-second portion and which may draw and provide 2 mb /s average in general-purpose i/o and utilize up to 2 gb in ram. The i/o includes any trustless reads from the Jam chain state, albeit in the recent past. This virtual hardware also provides unlimited reads from a semi-static preimage-lookup database. Each work-package may occupy this hardware and execute arbitrary code on it in six-second segments to create some result of at most 48 kb. This work-result is then entitled to 10ms on the same machine, this time with no “external” i/o, but instead with full and immediate access to the Jam chain state and may alter the service(s) to which the results belong.

20.2. Illustrating Performance
In terms of pure processing power, the Jam machine architecture can deliver extremely high levels of homogeneous trustless computation. However, the core model of Jam is a classic parallelized compute architecture, and for solutions to be able to utilize the architecture well they must be designed with it in mind to some extent. Accordingly, until such usecases appear on Jam with similar semantics to existing ones, it is very difficult to make direct comparisons to existing systems. That said, if we indulge ourselves with some assumptions then we can make some crude comparisons. 20.2.1. Comparison to Polkadot. Polkadot is at present capable of validating at most 80 parachains each doing one second of native computation and 5 mb of i/o in every six. This corresponds to an aggregate compute performance of around 13x native cpu and a total 24-hour distributed availability of around 67 mb /s. Accumulation is beyond Polkadot’s capabilities and so not comparable. For comparison, in our basic models, Jam should be capable of attaining around 85x the computation load of a single native cpu core and a distributed availability of 682 mb /s. 20.2.2. Simple Transfers. We might also attempt to model a simple transactions-per-second amount, with each transaction requiring a signature verification and the modification of two account balances. Once again, until there are clear designs for precisely how this would work we must make some assumptions. Our most naive model would be to use the Jam cores (i.e. refinement) simply for transaction verification and account lookups. The Jam chain would then hold and alter the balances in its state. This is unlikely to give great performance since almost all the needed i/o would be synchronous, but it can serve as a basis. A 12 mb work-package can hold around 96k transactions at 128 bytes per transaction. However, a 48 kb work-result could only encode around 6k account updates when each update is given as a pair of a 4 byte account index and 4 byte balance, resulting in a limit of 3k transactions per package, or 171k tps in total. It is possible that the eight bytes could typically be compressed by a byte or two, increasing maximum throughput a little. Our expectations are that state updates, with highly parallelized Merklization, can be done at between 500k and 1 million reads/write per second, implying around 250k-350k tps, depending on which turns out to be the bottleneck. A more sophisticated model would be to use the Jam cores for balance updates as well as transaction verification. We would have to assume that state and the transactions which operate on them can be partitioned between work-packages with some degree of efficiency, and that the 12 mb of the work-package would be split between transaction data and state witness data. Our basic models predict that a 32-bit account system paginated into 2 10 accounts/page and 128 bytes per transaction could, assuming only around 1% of oraclized accounts were useful, average upwards of 1.4m tps depending on partitioning and usage characteristics. Partitioning could be done with a fixed fragmentation (essentially sharding state), a rotating partition pattern or a dynamic partitioning (which would require specialized sequencing). Interestingly, we expect neither model to be bottlenecked in computation, meaning that transactions JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 33 could be substantially more sophisticated, perhaps with more flexible cryptography or smart-contract functionality, without a significant impact on performance. 20.2.3. Computation Throughput. The tps metric does not lend itself well to measuring distributed systems’ computational performance, so we now turn to another slightly more compute-focussed benchmark: the evm. The basic YP Ethereum network, now approaching a decade old, is probably the best known example of general purpose decentralized computation and makes for a reasonable yardstick. It is able to sustain a computation and i/o rate of 1.25M gas/sec, with a peak throughput of twice that. The evm gas metric was designed to be a time-proportional metric for predicting and constraining program execution. Attempting to determine a concrete comparison to pvm throughput is non-trivial and necessarily opinionated owing to the disparity between the two platforms, including word size, endianness, stack/register architecture and memory model. However, we will attempt to determine a reasonable range of values. Evm gas does not directly translate into native execution as it also combines state reads and writes as well as transaction input data, implying it is able to process some combination of up to 595 storage reads, 57 storage writes and 1.25M computation-gas as well as 78 kb input data in each second, trading one against the other. 13 We cannot find any analysis of the typical breakdown between storage i/o and pure computation, so to make a very conservative estimate, we assume it does all four. In reality, we would expect it to be able to do on average 1 / 4 of each. Our experiments 14 show that on modern, high-end consumer hardware with a high-quality evm implementation, we can expect somewhere between 100 and 500 gas/μs in throughput on pure-compute workloads (we specifically utilized Odd-Product, Triangle-Number and several implementations of the Fibonacci calculation). To make a conservative comparison to pvm, we propose transpilation of the evm code into pvm code and then re-execution of it under the Polkavm prototype. 15 To help estimate a reasonable lower-bound of evm gas/μs, e.g. for workloads which are more memory and i/o intensive, we look toward real-world permissionless deployments of the evm and see that the Moonbeam network, after correcting for the slowdown of executing within the recompiled WebAssembly platform on the somewhat conservative Polkadot hardware platform, implies a throughput of around 100 gas/μs. We therefore assert that in terms of computation, 1μs approximates to around 100-500 evm gas on modern high-end consumer hardware. 16 Benchmarking and regression tests show that the prototype pvm engine has a fixed preprocessing overhead of around 5ns/byte of program code and, for arithmeticheavy tasks at least, a marginal factor of 1.6-2% compared to evm execution, implying an asymptotic speedup of around 50-60x. For machine code 1 mb in size expected to take of the order of a second to compute, the compilation cost becomes only 0.5% of the overall time. 17 For code not inherently suited to the 256-bit evm isa, we would expect substantially improved relative execution times on pvm, though more work must be done in order to gain confidence that these speed-ups are broadly applicable. If we allow for preprocessing to take up to the same component within execution as the marginal cost (owing to, for example, an extremely large but short-running program) and for the pvm metering to imply a safety overhead of 2x to execution speeds, then we can expect a Jam core to be able to process the equivalent of around 1,500 evm gas/μs. Owing to the crudeness of our analysis we might reasonably predict it to be somewhere within a factor of three either way—i.e. 500-5,000 evm gas/μs. Jam cores are each capable of 2 mb /s bandwidth, which must include any state i/o and data which must be newly introduced (e.g. transactions). While writes come at comparatively little cost to the core, only requiring hashing to determine an eventual updated Merkle root, reads must be witnessed, with each one costing around 640 bytes of witness conservatively assuming a one-million entry binary Merkle trie. This would result in a maximum of a little over 3k reads/second/core, with the exact amount dependent upon how much of the bandwidth is used for newly introduced input data. Aggregating everything across Jam, excepting accumulation which could add further throughput, numbers can be multiplied by 341 (with the caveat that each one’s computation cannot interfere with any of the others’ except through state oraclization and accumulation). Unlike for roll-up chain designs such as Polkadot and Ethereum, there is no need to have persistently fragmented state. Smart-contract state may be held in a coherent format on the Jam chain so long as any updates are made through the 8 kb /core/sec work-results, which would need to contain only the hashes of the altered contracts’ state roots. Under our modelling assumptions, we can therefore summarize: Eth. L1 Jam Core Jam Compute (evm gas/μs) 1. 25 † 500-5,000 0.15-1.5 m State writes (s − 1) 57 † n/a n/a State reads (s − 1) 595 † 4 k ‡ 1.4 m ‡ Input data (s − 1) 78 kb † 2 mb ‡ 682 mb ‡ What we can see is that Jam ’s overall predicted performance profile implies it could be comparable to many thousands of that of the basic Ethereum L1 chain. The large factor here is essentially due to three things: spacial 13 The latest “proto-danksharding” changes allow it to accept 87.3 kb /s in committed-to data though this is not directly available within state, so we exclude it from this illustration, though including it with the input data would change the results little. 14 This is detailed at https://hackmd.io/@XXX9CM1uSSCWVNFRYaSB5g/HJarTUhJA and intended to be updated as we get more information. 15 It is conservative since we don’t take into account that the source code was originally compiled into evm code and thus the pvm machine code will replicate architectural artifacts and thus is very likely to be pessimistic. As an example, all arithmetic operations in evm are 256-bit and 64-bit native pvm is being forced to honor this even if the source code only actually required 64-bit values. 16 We speculate that the substantial range could possibly be caused in part by the major architectural differences between the evm isa and typical modern hardware. 17 As an example, our odd-product benchmark, a very much pure-compute arithmetic task, execution takes 58s on evm, and 1.04s within our pvm prototype, including all preprocessing. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 34 parallelism, as Jam can host several hundred cores under its security apparatus; temporal parallelism, as Jam targets continuous execution for its cores and pipelines much of the computation between blocks to ensure a constant, optimal workload; and platform optimization by using a vm and gas model which closely fits modern hardware architectures. It must however be understood that this is a provisional and crude estimation only. It is included only for the purpose of expressing Jam ’s performance in tangible terms. Specifically, it does not take into account: ● that these numbers are based on real performance of Ethereum and performance modelling of Jam (though our models are based on real-world performance of the components); ● any L2 scaling which may be possible with either Jam or Ethereum; ● the state partitioning which uses of Jam would imply; ● the as-yet unfixed gas model for the pvm ; ● that pvm / evm comparisons are necessarily imprecise; ● (†) all figures for Ethereum L1 are drawn from the same resource: on average each figure will be only 1 ~ 4 of this maximum. ● (‡) the state reads and input data figures for Jam are drawn from the same resource: on average each figure will be only 1 ~ 2 of this maximum. We leave it as further work for an empirical analysis of performance and an analysis and comparison between Jam and the aggregate of a hypothetical Ethereum ecosystem which included some maximal amount of L2 deployments together with full Dank-sharding and any other additional consensus elements which they would require. This, however, is out of scope for the present work.

21.1. Further Work
While we are able to estimate theoretical computation possible given some basic assumptions and even make broad comparisons to existing systems, practical numbers are invaluable. We believe the model warrants further empirical research in order to better understand how these theoretical limits translate into real-world performance. We feel a proper cost analysis and comparison to pre-existing protocols would also be an excellent topic for further work. We can be reasonably confident that the design of Jam allows it to host a service under which Polkadot parachains could be validated, however further prototyping work is needed to understand the possible throughput which a pvm-powered metering system could support. We leave such a report as further work. Likewise, we have also intentionally omitted details of higher-level protocol elements including cryptocurrency, coretime sales, staking and regular smart-contract functionality. A number of potential alterations to the protocol described here are being considered in order to make practical utilization of the protocol easier. These include: ● Synchronous calls between services in accumulate. ● Restrictions on the transfer function in order to allow for substantial parallelism over accumulation. ● The possibility of reserving substantial additional computation capacity during accumulate under certain conditions. ● Introducing Merklization into the Work Package format in order to obviate the need to have the whole package downloaded in order to evaluate its authorization. The networking protocol is also left intentionally undefined at this stage and its description must be done in a follow-up proposal. Validator performance is not presently tracked onchain. We do expect this to be tracked on-chain in the final revision of the Jam protocol, but its specific format is not yet certain and it is therefore omitted at present.

A.1. Basic Definition
We declare the general pvm function Ψ. We assume a single-step invocation function define Ψ 1 and define the full pvm recursively as a sequence of such mutations up until the single-step mutation results in a halting condition. We additionally define the function deblob which extracts the instruction data, opcode bitmask and dynamic jump table from a program blob: Ψ ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (Y, N R, N G, ⟦ N R ⟧ 13, M) → ({ ∎, ☇, ∞ } ∪ { F, ̵ h } × N R, N R, Z G, ⟦ N R ⟧ 13, M) (p, ı, ϱ, ω, μ) ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ Ψ (p, ı ′, ϱ ′, ω ′, μ ′) if ε = ▸ (∞, ı ′, ϱ ′, ω ′, μ ′) if ϱ ′ < 0 (ε, 0, ϱ ′, ω ′, μ ′) if ε ∈ { ☇, ∎ } (ε, ı ′, ϱ ′, ω ′, μ ′) otherwise where (ε, ı ′, ϱ ′, ω ′, μ ′) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ Ψ 1 (c, k, j, ı, ϱ, ω, μ) if ⎧ ⎩ c, k, j ⎫ ⎭ = deblob (p) (☇, ı, ϱ, ω, μ) otherwise (A.1) deblob ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ Y → ⎧ ⎩ Y, B, ⟦ N R ⟧ ⎫ ⎭ ∪ ∇ p ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ⎧ ⎩ c, k, j ⎫ ⎭ if ∃ ! c, k, j ∶ p = E (S j S) ⌢ E 1 (z) ⌢ E (S c S) ⌢ E z (j) ⌢ E (c) ⌢ E (k), S k S = S c S ∇ otherwise (A.2) The pvm exit reason ε ∈ { ∎, ☇, ∞ } ∪ { F, ̵ h } × N R may be one of regular halt ∎, panic ☇ or out-of-gas ∞, or alternatively a host-call ̵ h, in which the host-call identifier is associated, or page-fault F in which case the address into ram is associated.

A.2. Instructions, Opcodes and Skip-distance
The program blob p is split into a series of octets which make up the instruction data c and the opcode bitmask k as well as the dynamic jump table, j. The former two imply an instruction sequence, and by extension a basic-block sequence, itself a sequence of indices of the instructions which follow a block-termination instruction. The latter, dynamic jump table, is a sequence of indices into the instruction data blob and is indexed into when dynamically-computed jumps are taken. It is encoded as a sequence of natural numbers (i.e. non-negative integers) each encoded with the same length in octets. This length, term z above, is itself encoded prior. The pvm counts instructions in octet terms (rather than in terms of instructions) and it is thus convenient to define which octets represent the beginning of an instruction, i.e. the opcode octet, and which do not. This is the purpose of k, the instruction-opcode bitmask. We assert that the length of the bitmask is equal to the length of the instruction blob. We define the Skip function skip which provides the number of octets, minus one, to the next instruction’s opcode, given the index of instruction’s opcode index into c (and by extension k): (A.3) skip ∶  N → N i ↦ min (24, j ∈ N ∶ (k ⌢ [ 1, 1 ,. .. ]) i + 1 + j = 1) The Skip function appends k with a sequence of set bits in order to ensure a well-defined result for the final instruction skip (S c S − 1). Given some instruction-index i, its opcode is readily expressed as c i and the distance in octets to move forward to the next instruction is 1 + skip (i). However, each instruction’s “length” (defined as the number of contiguous octets starting with the opcode which are needed to fully define the instruction’s semantics) is left implicit though limited to being at most 16. We define ζ as being equivalent to the instructions c except with an indefinite sequence of zeroes suffixed to ensure that no out-of-bounds access is possible. This effectively defines any otherwise-undefined arguments to the final instruction and ensures that a trap will occur if the program counter passes beyond the program code. Formally: (A.4) ζ ≡ c ⌢ [ 0, 0 ,. .. ]

A.3. Basic Blocks and Termination Instructions
Instructions of the following opcodes are considered basic-block termination instructions; other than trap & fallthrough, they correspond to instructions which may define the instructioncounter to be something other than its prior value plus the instruction’s skip amount: ● Trap and fallthrough: trap, fallthrough ● Jumps: jump, jump_ind ● Load-and-Jumps: load_imm_jump, load_imm_jump_ind ● Branches: branch_eq, branch_ne, branch_ge_u, branch_ge_s, branch_lt_u, branch_lt_s, branch_eq_imm, branch_ne_imm ● Immediate branches: branch_lt_u_imm, branch_lt_s_imm, branch_le_u_imm, branch_le_s_imm, branch_ge_u_imm, branch_ge_s_imm, branch_gt_u_imm, branch_gt_s_imm We denote this set, as opcode indices rather than names, as T. We define the instruction opcode indices denoting the beginning of basic-blocks as ϖ : (A.5) ϖ ≡ [ 0 ] ⌢ [ n + 1 + skip (n) S n < − N S c S ∧ k n = 1 ∧ c n ∈ T ] JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 36

A.4. Single-Step State Transition
We must now define the single-step pvm state-transition function Ψ 1 : (A.6) Ψ 1 ∶ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (Y, B, ⟦ N R ⟧, N R, N G, ⟦ N R ⟧ 13, M) → ({ ☇, ∎, ▸ } ∪ { F, ̵ h } × N R, N R, Z G, ⟦ N R ⟧ 13, M) (c, k, j, ı, ϱ, ω, μ) ↦ (ε, ı ′, ϱ ′, ω ′, μ ′) We define ε together with the posterior values (denoted as prime) of each of the items of the machine state as being in accordance with the table below. In general, when transitioning machine state for an instruction a number of conditions hold true and instructions are defined essentially by their exceptions to these rules. Specifically, the machine does not halt, the instruction counter increments by one, the gas remaining is reduced by the amount corresponding to the instruction type and ram & registers are unchanged. Formally: (A.7) ε = ▸, ı ′ = ı + 1 + skip (ı), ϱ ′ = ϱ − ϱ ∆, ω ′ = ω, μ ′ = μ except as indicated During the course of executing instructions, ram may be accessed. When an index of ram below 2 16 is required, the machine always panics immediately without further changes to its state regardless of the apparent (in)accessibility of the value. Otherwise, should the given index of ram not be accessible then machine state remains unchanged and the exit reason is a fault with the lowest inaccessible page address to be read. Similarly, where ram must be mutated and yet mutable access is not possible, then machine state is unchanged, and the exit reason is a fault with the lowest page address to be written which is inaccessible. Formally, let r and w be the set of indices by which μ must be subscripted for inspection and mutation respectively in order to calculate the result of Ψ 1. We define the memory-access exceptional execution state ε μ which shall, if not ▸, singly effect the returned return of Ψ 1 as following: let x = { x ∶ x ∈ r ∧ x mod 2 32 ~ ∈ V μ ∨ x ∈ w ∧ x mod 2 32 ~ ∈ V ∗ μ } (A.8) ε μ = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ ▸ if x = {} ☇ if min (x) mod 2 32 < 2 16 F × Z P min (x) mod 2 32 ÷ Z P  otherwise (A.9) We define signed/unsigned transitions for various octet widths: Z n ∈ N ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ N 2 8 n → Z − 2 8 n − 1 ... 2 8 n − 1 a ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ a if a < 2 8 n − 1 a − 2 8 n otherwise (A.10) Z − 1 n ∈ N ∶ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ Z − 2 8 n − 1 ... 2 8 n − 1 → N 2 8 n a ↦ (2 8 n + a) mod 2 8 n (A.11) B n ∈ N ∶ ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ N 2 8 n → B 8 n x ↦ y ∶ ∀ i ∈ N 8 n ∶ y [ i ] ⇔ x 2 i  mod 2 (A.12) B − 1 n ∈ N ∶ ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ B 8 n → N 2 8 n x ↦ y ∶ ∑ i ∈ N 8 n x i ⋅ 2 i (A.13) ← Ð B n ∈ N ∶ ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ N 2 8 n → B 8 n x ↦ y ∶ ∀ i ∈ N 8 n ∶ y [ 8 n − 1 − i ] ⇔ x 2 i  mod 2 (A.14) ← Ð B − 1 n ∈ N ∶ ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ B 8 n → N 2 8 n x ↦ y ∶ ∑ i ∈ N 8 n x 8 n − 1 − i ⋅ 2 i (A.15) Immediate arguments are encoded in little-endian format with the most-significant bit being the sign bit. They may be compactly encoded by eliding more significant octets. Elided octets are assumed to be zero if the msb of the value is zero, and 255 otherwise. This allows for compact representation of both positive and negative encoded values. We thus define the signed extension function operating on an input of n octets as X n : X n ∈ { 0, 1, 2, 3, 4, 8 } ∶ ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ N 2 8 n → N R x ↦ x + x 2 8 n − 1 (2 64 − 2 8 n) (A.16) Any alterations of the program counter stemming from a static jump, call or branch must be to the start of a basic block or else a panic occurs. Hypotheticals are not considered. Formally: (A.17) branch (b, C) Ô⇒ (ε, ı ′) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (▸, ı) if ¬ C (☇, ı) otherwise if b ~ ∈ ϖ (▸, b) otherwise JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 37 Jumps whose next instruction is dynamically computed must use an address which may be indexed into the jumptable j. Through a quirk of tooling 18, we define the dynamic address required by the instructions as the jump table index incremented by one and then multiplied by our jump alignment factor Z A = 2. As with other irregular alterations to the program counter, target code index must be the start of a basic block or else a panic occurs. Formally: (A.18) djump (a) Ô⇒ (ε, ı ′) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (∎, ı) if a = 2 32 − 2 16 (☇, ı) otherwise if a = 0 ∨ a > S j S ⋅ Z A ∨ a mod Z A ≠ 0 ∨ j (a ~ Z A) − 1 ~ ∈ ϖ (▸, j (a ~ Z A) − 1) otherwise

A.5. Instruction Tables
Note that in the case that the opcode is not defined in the following tables then the instruction is considered invalid, and it results in a panic; ε = ☇. We assume the skip length ℓ is well-defined: (A.19) ℓ ≡ skip (ı) A.5.1. Instructions without Arguments. ζ ı Name ϱ ∆ Mutations 0 trap 0 ε = ☇ 1 fallthrough 0 A.5.2. Instructions with Arguments of One Immediate. (A.20) let l X = min (4, ℓ), ν X ≡ X l X (E − 1 l X (ζ ı + 1 ⋅⋅⋅+ l X)) ζ ı Name ϱ ∆ Mutations 10 ecalli 0 ε = ̵ h × ν X A.5.3. Instructions with Arguments of One Register and One Extended Width Immediate. (A.21) let r A = min (12, ζ ı + 1 mod 16), ω ′ A ≡ ω ′ r A, ν X ≡ E − 1 8 (ζ ı + 2 ⋅⋅⋅+ 8) ζ ı Name ϱ ∆ Mutations 20 load_imm_64 0 ω ′ A = ν X A.5.4. Instructions with Arguments of Two Immediates. (A.22) let l X = min (4, ζ ı + 1 mod 8), ν X ≡ X l X (E − 1 l X (ζ ı + 2 ⋅⋅⋅+ l X)) let l Y = min (4, max (0, ℓ − l X − 1)), ν Y ≡ X l Y (E − 1 l Y (ζ ı + 2 + l X ⋅⋅⋅+ l Y)) ζ ı Name ϱ ∆ Mutations 30 store_imm_u8 0 μ ′ ↺ ν X = ν Y mod 2 8 31 store_imm_u16 0 μ ′ ↺ ν X ⋅⋅⋅+ 2 = E 2 (ν Y mod 2 16) 32 store_imm_u32 0 μ ′ ↺ ν X ⋅⋅⋅+ 4 = E 4 (ν Y mod 2 32) 33 store_imm_u64 0 μ ′ ↺ ν X ⋅⋅⋅+ 8 = E 8 (ν Y) A.5.5. Instructions with Arguments of One Offset. (A.23) let l X = min (4, ℓ), ν X ≡ ı + Z l X (E − 1 l X (ζ ı + 1 ⋅⋅⋅+ l X)) ζ ı Name ϱ ∆ Mutations 40 jump 0 branch (ν X, ⊺) 18 The popular code generation backend llvm requires and assumes in its code generation that dynamically computed jump destinations always have a certain memory alignment. Since at present we depend on this for our tooling, we must acquiesce to its assumptions. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 38 A.5.6. Instructions with Arguments of One Register & One Immediate. (A.24) let r A = min (12, ζ ı + 1 mod 16), ω A ≡ ω r A, ω ′ A ≡ ω ′ r A let l X = min (4, max (0, ℓ − 1)), ν X ≡ X l X (E − 1 l X (ζ ı + 2 ⋅⋅⋅+ l X)) ζ ı Name ϱ ∆ Mutations 50 jump_ind 0 djump ((ω A + ν X) mod 2 32) 51 load_imm 0 ω ′ A = ν X 52 load_u8 0 ω ′ A = μ ↺ ν X 53 load_i8 0 ω ′ A = X 1 (μ ↺ ν X) 54 load_u16 0 ω ′ A = E − 1 2 (μ ↺ ν X ⋅⋅⋅+ 2) 55 load_i16 0 ω ′ A = X 2 (E − 1 2 (μ ↺ ν X ⋅⋅⋅+ 2)) 56 load_u32 0 ω ′ A = E − 1 4 (μ ↺ ν X ⋅⋅⋅+ 4) 57 load_i32 0 ω ′ A = X 4 (E − 1 4 (μ ↺ ν X ⋅⋅⋅+ 4)) 58 load_u64 0 ω ′ A = E − 1 8 (μ ↺ ν X ⋅⋅⋅+ 8) 59 store_u8 0 μ ′ ↺ ν X = ω A mod 2 8 60 store_u16 0 μ ′ ↺ ν X ⋅⋅⋅+ 2 = E 2 (ω A mod 2 16) 61 store_u32 0 μ ′ ↺ ν X ⋅⋅⋅+ 4 = E 4 (ω A mod 2 32) 62 store_u64 0 μ ′ ↺ ν X ⋅⋅⋅+ 8 = E 8 (ω A) A.5.7. Instructions with Arguments of One Register & Two Immediates. (A.25) let r A = min (12, ζ ı + 1 mod 16), ω A ≡ ω r A, ω ′ A ≡ ω ′ r A let l X = min (4, ζ ı + 1 16  mod 8), ν X = X l X (E − 1 l X (ζ ı + 2 ⋅⋅⋅+ l X)) let l Y = min (4, max (0, ℓ − l X − 1)), ν Y = X l Y (E − 1 l Y (ζ ı + 2 + l X ⋅⋅⋅+ l Y)) ζ ı Name ϱ ∆ Mutations 70 store_imm_ind_u8 0 μ ′ ↺ ω A + ν X = ν Y mod 2 8 71 store_imm_ind_u16 0 μ ′ ↺ ω A + ν X ⋅⋅⋅+ 2 = E 2 (ν Y mod 2 16) 72 store_imm_ind_u32 0 μ ′ ↺ ω A + ν X ⋅⋅⋅+ 4 = E 4 (ν Y mod 2 32) 73 store_imm_ind_u64 0 μ ′ ↺ ω A + ν X ⋅⋅⋅+ 8 = E 8 (ν Y) A.5.8. Instructions with Arguments of One Register, One Immediate and One Offset. (A.26) let r A = min (12, ζ ı + 1 mod 16), ω A ≡ ω r A, ω ′ A ≡ ω ′ r A let l X = min (4, ζ ı + 1 16  mod 8), ν X = X l X (E − 1 l X (ζ ı + 2 ⋅⋅⋅+ l X)) let l Y = min (4, max (0, ℓ − l X − 1)), ν Y = ı + Z l Y (E − 1 l Y (ζ ı + 2 + l X ⋅⋅⋅+ l Y)) ζ ı Name ϱ ∆ Mutations 80 load_imm_jump 0 branch (ν Y, ⊺), ω ′ A = ν X 81 branch_eq_imm 0 branch (ν Y, ω A = ν X) 82 branch_ne_imm 0 branch (ν Y, ω A ≠ ν X) 83 branch_lt_u_imm 0 branch (ν Y, ω A < ν X) 84 branch_le_u_imm 0 branch (ν Y, ω A ≤ ν X) 85 branch_ge_u_imm 0 branch (ν Y, ω A ≥ ν X) 86 branch_gt_u_imm 0 branch (ν Y, ω A > ν X) JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 39 ζ ı Name ϱ ∆ Mutations 87 branch_lt_s_imm 0 branch (ν Y, Z 8 (ω A) < Z 8 (ν X)) 88 branch_le_s_imm 0 branch (ν Y, Z 8 (ω A) ≤ Z 8 (ν X)) 89 branch_ge_s_imm 0 branch (ν Y, Z 8 (ω A) ≥ Z 8 (ν X)) 90 branch_gt_s_imm 0 branch (ν Y, Z 8 (ω A) > Z 8 (ν X)) A.5.9. Instructions with Arguments of Two Registers. (A.27) let r D = min (12, (ζ ı + 1) mod 16), ω D ≡ ω r D, ω ′ D ≡ ω ′ r D let r A = min (12, ζ ı + 1 16 ), ω A ≡ ω r A, ω ′ A ≡ ω ′ r A ζ ı Name ϱ ∆ Mutations 100 move_reg 0 ω ′ D = ω A 101 sbrk 0 ω ′ D ≡ min (x ∈ N R) ∶ x ≥ h N x ⋅⋅⋅+ ω A ~ ⊆ V μ N x ⋅⋅⋅+ ω A ⊆ V ∗ μ ′ 102 count_set_bits_64 0 ω ′ D = 63 ∑ i = 0 B 8 (ω A) i 103 count_set_bits_32 0 ω ′ D = 31 ∑ i = 0 B 4 (ω A mod 2 32) i 104 leading_zero_bits_64 0 ω ′ D = max (n ∈ N 65) where i < n ∑ i = 0 B 8 (ω A) i = 0 105 leading_zero_bits_32 0 ω ′ D = max (n ∈ N 33) where i < n ∑ i = 0 B 4 (ω A mod 2 32) i = 0 106 trailing_zero_bits_64 0 ω ′ D = max (n ∈ N 65) where i < n ∑ i = 0 B 8 (ω A) 63 − i = 0 107 trailing_zero_bits_32 0 ω ′ D = max (n ∈ N 33) where i < n ∑ i = 0 B 4 (ω A mod 2 32) 31 − i = 0 108 sign_extend_8 0 ω ′ D = Z − 1 8 (Z 1 (ω A mod 2 8)) 109 sign_extend_16 0 ω ′ D = Z − 1 8 (Z 2 (ω A mod 2 16)) 110 zero_extend_16 0 ω ′ D = ω A mod 2 16 111 reverse_bytes 0 ∀ i ∈ N 8 ∶ E 8 (ω ′ D) i = E 8 (ω A) 7 − i Note, the term h above refers to the beginning of the heap, the second major section of memory as defined in equation A.41 as 2 Z Z + Z (S o S). If sbrk instruction is invoked on a pvm instance which does not have such a memory layout, then h = 0. A.5.10. Instructions with Arguments of Two Registers & One Immediate. (A.28) let r A = min (12, (ζ ı + 1) mod 16), ω A ≡ ω r A, ω ′ A ≡ ω ′ r A let r B = min (12, ζ ı + 1 16 ), ω B ≡ ω r B, ω ′ B ≡ ω ′ r B let l X = min (4, max (0, ℓ − 1)), ν X ≡ X l X (E − 1 l X (ζ ı + 2 ⋅⋅⋅+ l X)) ζ ı Name ϱ ∆ Mutations 120 store_ind_u8 0 μ ′ ↺ ω B + ν X = ω A mod 2 8 121 store_ind_u16 0 μ ′ ↺ ω B + ν X ⋅⋅⋅+ 2 = E 2 (ω A mod 2 16) 122 store_ind_u32 0 μ ′ ↺ ω B + ν X ⋅⋅⋅+ 4 = E 4 (ω A mod 2 32) 123 store_ind_u64 0 μ ′ ↺ ω B + ν X ⋅⋅⋅+ 8 = E 8 (ω A) JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 40 ζ ı Name ϱ ∆ Mutations 124 load_ind_u8 0 ω ′ A = μ ↺ ω B + ν X 125 load_ind_i8 0 ω ′ A = Z − 1 8 (Z 1 (μ ↺ ω B + ν X)) 126 load_ind_u16 0 ω ′ A = E − 1 2 (μ ↺ ω B + ν X ⋅⋅⋅+ 2) 127 load_ind_i16 0 ω ′ A = Z − 1 8 (Z 2 (E − 1 2 (μ ↺ ω B + ν X ⋅⋅⋅+ 2))) 128 load_ind_u32 0 ω ′ A = E − 1 4 (μ ↺ ω B + ν X ⋅⋅⋅+ 4) 129 load_ind_i32 0 ω ′ A = Z − 1 8 (Z 4 (E − 1 4 (μ ↺ ω B + ν X ⋅⋅⋅+ 4))) 130 load_ind_u64 0 ω ′ A = E − 1 8 (μ ↺ ω B + ν X ⋅⋅⋅+ 8) 131 add_imm_32 0 ω ′ A = X 4 ((ω B + ν X) mod 2 32) 132 and_imm 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ A) i = B 8 (ω B) i ∧ B 8 (ν X) i 133 xor_imm 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ A) i = B 8 (ω B) i ⊕ B 8 (ν X) i 134 or_imm 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ A) i = B 8 (ω B) i ∨ B 8 (ν X) i 135 mul_imm_32 0 ω ′ A = X 4 ((ω B ⋅ ν X) mod 2 32) 136 set_lt_u_imm 0 ω ′ A = ω B < ν X 137 set_lt_s_imm 0 ω ′ A = Z 8 (ω B) < Z 8 (ν X) 138 shlo_l_imm_32 0 ω ′ A = X 4 ((ω B ⋅ 2 ν X mod 32) mod 2 32) 139 shlo_r_imm_32 0 ω ′ A = X 4 (ω B mod 2 32 ÷ 2 ν X mod 32 ) 140 shar_r_imm_32 0 ω ′ A = Z − 1 8 (Z 4 (ω B mod 2 32) ÷ 2 ν X mod 32 ) 141 neg_add_imm_32 0 ω ′ A = X 4 ((ν X + 2 32 − ω B) mod 2 32) 142 set_gt_u_imm 0 ω ′ A = ω B > ν X 143 set_gt_s_imm 0 ω ′ A = Z 8 (ω B) > Z 8 (ν X) 144 shlo_l_imm_alt_32 0 ω ′ A = X 4 ((ν X ⋅ 2 ω B mod 32) mod 2 32) 145 shlo_r_imm_alt_32 0 ω ′ A = X 4 (ν X mod 2 32 ÷ 2 ω B mod 32 ) 146 shar_r_imm_alt_32 0 ω ′ A = Z − 1 8 (Z 4 (ν X mod 2 32) ÷ 2 ω B mod 32 ) 147 cmov_iz_imm 0 ω ′ A = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ν X if ω B = 0 ω A otherwise 148 cmov_nz_imm 0 ω ′ A = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ν X if ω B ≠ 0 ω A otherwise 149 add_imm_64 0 ω ′ A = (ω B + ν X) mod 2 64 150 mul_imm_64 0 ω ′ A = (ω B ⋅ ν X) mod 2 64 151 shlo_l_imm_64 0 ω ′ A = X 8 ((ω B ⋅ 2 ν X mod 64) mod 2 64) 152 shlo_r_imm_64 0 ω ′ A = X 8 (ω B ÷ 2 ν X mod 64 ) 153 shar_r_imm_64 0 ω ′ A = Z − 1 8 (Z 8 (ω B) ÷ 2 ν X mod 64 ) 154 neg_add_imm_64 0 ω ′ A = (ν X + 2 64 − ω B) mod 2 64 155 shlo_l_imm_alt_64 0 ω ′ A = (ν X ⋅ 2 ω B mod 64) mod 2 64 156 shlo_r_imm_alt_64 0 ω ′ A = ν X ÷ 2 ω B mod 64  157 shar_r_imm_alt_64 0 ω ′ A = Z − 1 8 (Z 8 (ν X) ÷ 2 ω B mod 64 ) 158 rot_r_64_imm 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ A) i = B 8 (ω B) (i + ν X) mod 64 159 rot_r_64_imm_alt 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ A) i = B 8 (ν X) (i + ω B) mod 64 160 rot_r_32_imm 0 ω ′ A = X 4 (x) where x ∈ N 2 32, ∀ i ∈ N 32 ∶ B 4 (x) i = B 4 (ω B) (i + ν X) mod 32 161 rot_r_32_imm_alt 0 ω ′ A = X 4 (x) where x ∈ N 2 32, ∀ i ∈ N 32 ∶ B 4 (x) i = B 4 (ν X) (i + ω B) mod 32 JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 41 A.5.11. Instructions with Arguments of Two Registers & One Offset. (A.29) let r A = min (12, (ζ ı + 1) mod 16), ω A ≡ ω r A, ω ′ A ≡ ω ′ r A let r B = min (12, ζ ı + 1 16 ), ω B ≡ ω r B, ω ′ B ≡ ω ′ r B let l X = min (4, max (0, ℓ − 1)), ν X ≡ ı + Z l X (E − 1 l X (ζ ı + 2 ⋅⋅⋅+ l X)) ζ ı Name ϱ ∆ Mutations 170 branch_eq 0 branch (ν X, ω A = ω B) 171 branch_ne 0 branch (ν X, ω A ≠ ω B) 172 branch_lt_u 0 branch (ν X, ω A < ω B) 173 branch_lt_s 0 branch (ν X, Z 8 (ω A) < Z 8 (ω B)) 174 branch_ge_u 0 branch (ν X, ω A ≥ ω B) 175 branch_ge_s 0 branch (ν X, Z 8 (ω A) ≥ Z 8 (ω B)) A.5.12. Instruction with Arguments of Two Registers and Two Immediates. (A.30) let r A = min (12, (ζ ı + 1) mod 16), ω A ≡ ω r A, ω ′ A ≡ ω ′ r A let r B = min (12, ζ ı + 1 16 ), ω B ≡ ω r B, ω ′ B ≡ ω ′ r B let l X = min (4, ζ ı + 2 mod 8), ν X = X l X (E − 1 l X (ζ ı + 3 ⋅⋅⋅+ l X)) let l Y = min (4, max (0, ℓ − l X − 2)), ν Y = X l Y (E − 1 l Y (ζ ı + 3 + l X ⋅⋅⋅+ l Y)) ζ ı Name ϱ ∆ Mutations 180 load_imm_jump_ind 0 djump ((ω B + ν Y) mod 2 32), ω ′ A = ν X A.5.13. Instructions with Arguments of Three Registers. (A.31) let r A = min (12, (ζ ı + 1) mod 16), ω A ≡ ω r A, ω ′ A ≡ ω ′ r A let r B = min (12, ζ ı + 1 16 ), ω B ≡ ω r B, ω ′ B ≡ ω ′ r B let r D = min (12, ζ ı + 2), ω D ≡ ω r D, ω ′ D ≡ ω ′ r D ζ ı Name ϱ ∆ Mutations 190 add_32 0 ω ′ D = X 4 ((ω A + ω B) mod 2 32) 191 sub_32 0 ω ′ D = X 4 ((ω A + 2 32 − (ω B mod 2 32)) mod 2 32) 192 mul_32 0 ω ′ D = X 4 ((ω A ⋅ ω B) mod 2 32) 193 div_u_32 0 ω ′ D = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 2 64 − 1 if ω B mod 2 32 = 0 X 4 ((ω A mod 2 32) ÷ (ω B mod 2 32)) otherwise 194 div_s_32 0 ω ′ D = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 2 64 − 1 if b = 0 Z − 1 8 (a) if a = − 2 31 ∧ b = − 1 Z − 1 8 (rtz (a ÷ b)) otherwise where a = Z 4 (ω A mod 2 32), b = Z 4 (ω B mod 2 32) 195 rem_u_32 0 ω ′ D = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ X 4 (ω A mod 2 32) if ω B mod 2 32 = 0 X 4 ((ω A mod 2 32) mod (ω B mod 2 32)) otherwise 196 rem_s_32 0 ω ′ D = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 0 if a = − 2 31 ∧ b = − 1 Z − 1 8 (smod (a, b)) otherwise where a = Z 4 (ω A mod 2 32), b = Z 4 (ω B mod 2 32) 197 shlo_l_32 0 ω ′ D = X 4 ((ω A ⋅ 2 ω B mod 32) mod 2 32) 198 shlo_r_32 0 ω ′ D = X 4 ((ω A mod 2 32) ÷ 2 ω B mod 32 ) 199 shar_r_32 0 ω ′ D = Z − 1 8 (Z 4 (ω A mod 2 32) ÷ 2 ω B mod 32 ) JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 42 ζ ı Name ϱ ∆ Mutations 200 add_64 0 ω ′ D = (ω A + ω B) mod 2 64 201 sub_64 0 ω ′ D = (ω A + 2 64 − ω B) mod 2 64 202 mul_64 0 ω ′ D = (ω A ⋅ ω B) mod 2 64 203 div_u_64 0 ω ′ D = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 2 64 − 1 if ω B = 0 ⌊ ω A ÷ ω B ⌋ otherwise 204 div_s_64 0 ω ′ D = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ 2 64 − 1 if ω B = 0 ω A if Z 8 (ω A) = − 2 63 ∧ Z 8 (ω B) = − 1 Z − 1 8 (rtz (Z 8 (ω A) ÷ Z 8 (ω B))) otherwise 205 rem_u_64 0 ω ′ D = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ω A if ω B = 0 ω A mod ω B otherwise 206 rem_s_64 0 ω ′ D = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0 if Z 8 (ω A) = − 2 63 ∧ Z 8 (ω B) = − 1 Z − 1 8 (smod (Z 8 (ω A), Z 8 (ω B))) otherwise 207 shlo_l_64 0 ω ′ D = (ω A ⋅ 2 ω B mod 64) mod 2 64 208 shlo_r_64 0 ω ′ D = ω A ÷ 2 ω B mod 64  209 shar_r_64 0 ω ′ D = Z − 1 8 (Z 8 (ω A) ÷ 2 ω B mod 64 ) 210 and 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ D) i = B 8 (ω A) i ∧ B 8 (ω B) i 211 xor 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ D) i = B 8 (ω A) i ⊕ B 8 (ω B) i 212 or 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ D) i = B 8 (ω A) i ∨ B 8 (ω B) i 213 mul_upper_s_s 0 ω ′ D = Z − 1 8 ((Z 8 (ω A) ⋅ Z 8 (ω B)) ÷ 2 64 ) 214 mul_upper_u_u 0 ω ′ D = (ω A ⋅ ω B) ÷ 2 64  215 mul_upper_s_u 0 ω ′ D = Z − 1 8 ((Z 8 (ω A) ⋅ ω B) ÷ 2 64 ) 216 set_lt_u 0 ω ′ D = ω A < ω B 217 set_lt_s 0 ω ′ D = Z 8 (ω A) < Z 8 (ω B) 218 cmov_iz 0 ω ′ D = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ω A if ω B = 0 ω D otherwise 219 cmov_nz 0 ω ′ D = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ω A if ω B ≠ 0 ω D otherwise 220 rot_l_64 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ D) (i + ω B) mod 64 = B 8 (ω A) i 221 rot_l_32 0 ω ′ D = X 4 (x) where x ∈ N 2 32, ∀ i ∈ N 32 ∶ B 4 (x) (i + ω B) mod 32 = B 4 (ω A) i 222 rot_r_64 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ D) i = B 8 (ω A) (i + ω B) mod 64 223 rot_r_32 0 ω ′ D = X 4 (x) where x ∈ N 2 32, ∀ i ∈ N 32 ∶ B 4 (x) i = B 4 (ω A) (i + ω B) mod 32 224 and_inv 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ D) i = B 8 (ω A) i ∧ ¬ B 8 (ω B) i 225 or_inv 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ D) i = B 8 (ω A) i ∨ ¬ B 8 (ω B) i 226 xnor 0 ∀ i ∈ N 64 ∶ B 8 (ω ′ D) i = ¬ (B 8 (ω A) i ⊕ B 8 (ω B) i) 227 max 0 ω ′ D = Z − 1 8 (max (Z 8 (ω A), Z 8 (ω B))) 228 max_u 0 ω ′ D = max (ω A, ω B) 229 min 0 ω ′ D = Z − 1 8 (min (Z 8 (ω A), Z 8 (ω B))) 230 min_u 0 ω ′ D = min (ω A, ω B) Note that the two signed modulo operations have an idiosyncratic definition, operating as the modulo of the absolute values, but with the sign of the numerator. Formally: (A.32) smod ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (Z, Z) → Z (a, b) ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ a if b = 0 sgn (a)(S a S mod S b S) otherwise JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 43 Division operations always round their result towards zero. Formally: (A.33) rtz ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ Z → Z x ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ⌈ x ⌉ if x < 0 ⌊ x ⌋ otherwise

A.6. Host Call Definition
An extended version of the pvm invocation which is able to progress an inner host-call state-machine in the case of a host-call halt condition is defined as Ψ H : Ψ H ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ Y, N R, N G, ⟦ N R ⟧ 13, M, Ω ⟨ X ⟩, X ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ → ⎧ ⎩ { ☇, ∞, ∎ } ∪ { F } × N R, N R, Z G, ⟦ N R ⟧ 13, M, X ⎫ ⎭ (c, ı, ϱ, ω, μ, f, x) ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ let (ε ′, ı ′, ϱ ′, ω ′, μ ′) = Ψ (c, ı, ϱ, ω, μ) ∶ (ε ′, ı ′, ϱ ′, ω ′, μ ′, x) if ε ′ ∈ { ∎, ☇, ∞ } ∪ { F } × N R (F × a, ı ′, ϱ ′, ω ′, μ ′, x) if ⋀ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ε ′ = ̵ h × h F × a = f (h, ϱ ′, ω ′, μ ′, x) Ψ H (c, ı ′, ϱ ′′, ω ′′, μ ′′, f, x ′′) if ⋀  ε ′ = ̵ h × h (▸, ϱ ′′, ω ′′, μ ′′, x ′′) = f (h, ϱ ′, ω ′, μ ′, x) (ε ′′, ı ′, ϱ ′′, ω ′′, μ ′′, x ′′) if ⋀ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ ε ′ = ̵ h × h (ε ′′, ϱ ′′, ω ′′, μ ′′, x ′′) = f (h, ϱ ′, ω ′, μ ′, x) ε ′′ ∈ { ☇, ∎, ∞ } (A.34) Ω ⟨ X ⟩ ≡ (N, N G, ⟦ N R ⟧ 13, M, X) → ⎧ ⎩ { ▸, ∎, ☇, ∞ }, N G, ⟦ N R ⟧ 13, M, X ⎫ ⎭ ∪ { F } × N R (A.35) On exit, the instruction counter ı ′ references the instruction which caused the exit. Should the machine be invoked again using this instruction counter and code, then the same instruction which caused the exit would be executed. This is sensible when the instruction is one which necessarily needs re-executing such as in the case of an out-of-gas or page fault reason. However, when the exit reason to Ψ is a host-call ̵ h, then the resultant instruction-counter has a value of the hostcall instruction and resuming with this state would immediately exit with the same result. Re-invoking would therefore require both the post-host-call machine state and the instruction counter value for the instruction following the one which resulted in the host-call exit reason. This is always one greater plus the relevant argument skip distance. Resuming the machine with this instruction counter will continue beyond the host-call instruction. We use both values of instruction-counter for the definition of Ψ H since if the host-call results in a page fault we need to allow the outer environment to resolve the fault and re-try the host-call. Conversely, if we successfully transition state according to the host-call, then on resumption we wish to begin with the instruction directly following the host-call.

A.7. Standard Program Initialization
The software programs which will run in each of the four instances where the pvm is utilized in the main document have a very typical setup pattern characteristic of an output of a compiler and linker. This means that ram has sections for program-specific read-only data, read-write (heap) data and the stack. An adjunct to this, very typical of our usage patterns is an extra read-only section via which invocation-specific data may be passed (i.e. arguments). It thus makes sense to define this properly in a single initializer function. These sections are quantized into major zones, and one major zone is always left unallocated between sections in order to reduce accidental overrun. Sections are padded with zeroes to the nearest pvm memory page boundary. We thus define the standard program code format p, which includes not only the instructions and jump table (previously represented by the term c), but also information on the state of the ram at program start. Given some p which is appropriately encoded together with some argument data a, we can define program code c, registers ω and ram μ through the standard initialization decoder function Y : (A.36) Y ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ Y → (Y, ⟦ N R ⟧ 13, M) ? p ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (c, ω, μ) if ∃ ! (c, o, w, z, s) which satisfy equation A.37 ∅ otherwise With conditions: let E 3 (S o S) ⌢ E 3 (S w S) ⌢ E 2 (z) ⌢ E 3 (s) ⌢ o ⌢ w ⌢ E 4 (S c S) ⌢ c ⌢ a = p (A.37) Z Z = 2 16, Z I = 2 24 (A.38) let P (x ∈ N) ≡ Z P  x Z P , Z (x ∈ N) ≡ Z Z  x Z Z  (A.39) 5 Z Z + Z (S o S) + Z (S w S + z Z P) + Z (s) + Z I ≤ 2 32 (A.40) JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 44 Thus, if the above conditions cannot be satisfied with unique values, then the result is ∅, otherwise it is a tuple of c as above and μ, ω such that: (A.41) ∀ i ∈ N 2 32 ∶ ((μ V) i, (μ A) ⌊ i ~ Z P ⌋) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ⎧ ⎩ V ▸ ▸ o i − Z Z, A ▸ ▸ R ⎫ ⎭ if Z Z ≤ i < Z Z + S o S (0, R) if Z Z + S o S ≤ i < Z Z + P (S o S) (w i − (2 Z Z + Z (S o S)), W) if 2 Z Z + Z (S o S) ≤ i < 2 Z Z + Z (S o S) + S w S (0, W) if 2 Z Z + Z (S o S) + S w S ≤ i < 2 Z Z + Z (S o S) + P (S w S) + z Z P (0, W) if 2 32 − 2 Z Z − Z I − P (s) ≤ i < 2 32 − 2 Z Z − Z I (a i − (2 32 − Z Z − Z I), R) if 2 32 − Z Z − Z I ≤ i < 2 32 − Z Z − Z I + S a S (0, R) if 2 32 − Z Z − Z I + S a S ≤ i < 2 32 − Z Z − Z I + P (S a S) (0, ∅) otherwise (A.42) ∀ i ∈ N 13 ∶ ω i = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 2 32 − 2 16 if i = 0 2 32 − 2 Z Z − Z I if i = 1 2 32 − Z Z − Z I if i = 7 S a S if i = 8 0 otherwise

A.8. Argument Invocation Definition
The four instances where the pvm is utilized each expect to be able to pass argument data in and receive some return data back. We thus define the common pvm program-argument invocation function Ψ M : (A.43) Ψ M ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (Y, N R, N G, Y ∶ Z I, Ω ⟨ X ⟩, X) → (N G, Y ∪ { ☇, ∞ }, X) (p, ı, ϱ, a, f, x) ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (0, ☇, x) if Y (p) = ∅ R (ϱ, Ψ H (c, ı, ϱ, ω, μ, f, x)) if Y (p) = (c, ω, μ) where R ∶ (ϱ, ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ε, ı ′, ϱ ′, ω ′, μ ′, x ′ ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭) ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (u, ∞, x ′) if ε = ∞ (u, μ ′ ω ′ 7 ⋅⋅⋅+ ω ′ 8, x ′) if ε = ∎ ∧ N ω ′ 7 ⋅⋅⋅+ ω ′ 8 ⊆ V μ ′ (u, [], x ′) if ε = ∎ ∧ N ω ′ 7 ⋅⋅⋅+ ω ′ 8 ~ ⊆ V μ ′ (u, ☇, x ′) otherwise where u = ϱ − max (ϱ ′, 0) Note that the first tuple item is the amount of gas consumed by the operation, but never greater than the amount of gas provided for the operation.

B.1. Host-Call Result Constants
NONE = 2 64 − 1 : The return value indicating an item does not exist. WHAT = 2 64 − 2 : Name unknown. OOB = 2 64 − 3 : The inner pvm memory index provided for reading/writing is not accessible. WHO = 2 64 − 4 : Index unknown. FULL = 2 64 − 5 : Storage full. CORE = 2 64 − 6 : Core index unknown. CASH = 2 64 − 7 : Insufficient funds. LOW = 2 64 − 8 : Gas limit too low. HUH = 2 64 − 9 : The item is already solicited or cannot be forgotten. OK = 0 : The return value indicating general success. Inner pvm invocations have their own set of result codes: HALT = 0 : The invocation completed and halted normally. PANIC = 1 : The invocation completed with a panic. FAULT = 2 : The invocation completed with a page fault. HOST = 3 : The invocation completed with a host-call fault. OOG = 4 : The invocation completed by running out of gas. Note return codes for a host-call-request exit are any non-zero value less than 2 64 − 13.

B.2. Is-Authorized Invocation
The Is-Authorized invocation is the first and simplest of the four, being totally stateless. It provides only a single host-call function, Ω G for determining the amount of gas remaining. It accepts as arguments the work-package as a whole, p and the core on which it should be executed, c. Formally, it is defined as Ψ I : Ψ I ∶  (P, N C) → ⎧ ⎩ Y ∪ J, N G ⎫ ⎭ (p, c) ↦ ⎧ ⎩ r, u ⎫ ⎭ where (u, r, ∅) = Ψ M (p c, 0, G I, E (p, c), F, ∅) (B.1) JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 45 F ∈ Ω ⟨{}⟩ ∶ (n, ϱ, ω, μ) ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ Ω G (ϱ, ω, μ) if n = gas (▸, ϱ − 10, [ ω 0 ,. .., ω 6, WHAT, ω 8 ,. .. ], μ) otherwise (B.2) Note for the Is-Authorized host-call dispatch function F in equation B.2, we elide the host-call context since, being essentially stateless, it is always ∅.

B.3. Refine Invocation
We define the Refine service-account invocation function as Ψ R. It has no general access to the state of the Jam chain, with the slight exception being the ability to make a historical lookup. Beyond this it is able to create inner instances of the pvm and dictate pieces of data to export. The historical-lookup host-call function, Ω H, is designed to give the same result regardless of the state of the chain for any time when auditing may occur (which we bound to be less than two epochs from being accumulated). The lookup anchor may be up to L timeslots before the recent history and therefore adds to the potential age at the time of audit. We therefore set D to have a safety margin of eight hours: (B.3) D ≡ L + 4, 800 = 19, 200 The inner pvm invocation host-calls, meanwhile, depend on an integrated pvm type, which we shall denote M. It holds some program code, instruction counter and ram : (B.4) M ≡ ⎧ ⎩ p ∈ Y, u ∈ M, i ∈ N R ⎫ ⎭ The Export host-call depends on two pieces of context; one sequence of segments (blobs of length W G) to which it may append, and the other an argument passed to the invocation function to dictate the number of segments prior which may assumed to have already been appended. The latter value ensures that an accurate segment index can be provided to the caller. Unlike the other invocation functions, the Refine invocation function implicitly draws upon some recent service account state item δ. The specific block from which this comes is not important, as long as it is no earlier than its work-package’s lookup-anchor block. It explicitly accepts the work-package p and the index of the work item to be refined, i. Additionally, the authorizer trace o is provided together with all work items’ import segments i and an export segment offset ς. It results in either some error J or a pair of the refinement output blob and the export sequence. Formally: Ψ R ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (N, P, Y, ⟦⟦ G ⟧⟧, N) → (Y ∪ J, ⟦ G ⟧, N G) (i, p, o, i, ς) ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (BAD, [], 0) if w s ~ ∈ K (δ) ∨ Λ (δ [ w s ], (p x) t, w h) = ∅ (BIG, [], 0) otherwise if S Λ (δ [ w s ], (p x) t, w h)S > W C otherwise ∶ let a = E (w s, w y, H (p), p x, p u), E (↕ m, c) = Λ (δ [ w s ], (p x) t, w h) and (u, r, (m, e)) = Ψ M (c, 0, w g, a, F, (∅, [])) ∶ (r, [], u) if r ∈ { ∞, ☇ } (r, e, u) otherwise where w = p w [ i ] (B.5) F ∈ Ω ⟨(D ⟨ N → M ⟩, ⟦ G ⟧)⟩ ∶ (n, ϱ, ω, μ, (m, e)) ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ Ω H (ϱ, ω, μ, (m, e), w s, δ, (p x) t) if n = historical _ lookup Ω Y (ϱ, ω, μ, (m, e), i, p, o, i) if n = fetch Ω E (ϱ, ω, μ, (m, e), ς) if n = export Ω G (ϱ, ω, μ, (m, e)) if n = gas Ω M (ϱ, ω, μ, (m, e)) if n = machine Ω P (ϱ, ω, μ, (m, e)) if n = peek Ω Z (ϱ, ω, μ, (m, e)) if n = zero Ω O (ϱ, ω, μ, (m, e)) if n = poke Ω V (ϱ, ω, μ, (m, e)) if n = void Ω K (ϱ, ω, μ, (m, e)) if n = invoke Ω X (ϱ, ω, μ, (m, e)) if n = expunge (▸, ϱ − 10, ω ′, μ) otherwise where ω ′ = ω except ω ′ 7 = WHAT (B.6)

B.4. Accumulate Invocation
Since this is a transition which can directly affect a substantial amount of on-chain state, our invocation context is accordingly complex. It is a tuple with elements for each of the aspects of state which can be altered through this invocation and beyond the account of the service itself includes the deferred transfer list and several dictionaries for alterations to preimage lookup state, core assignments, validator key assignments, newly created accounts and alterations to account privilege levels. Formally, we define our result context to be X, and our invocation context to be a pair of these contexts, X × X, with one dimension being the regular dimension and generally named x and the other being the exceptional dimension JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 46 and being named y. The only function which actually alters this second dimension is checkpoint, Ω C and so it is rarely seen. X ≡ ⎧ ⎩ s ∈ N S, u ∈ U, i ∈ N S, t ∈ ⟦ T ⟧, y ∈ H ?, p ∈ { ⎧ ⎩ N S, Y ⎫ ⎭ } ⎫ ⎭ (B.7) ∀ x ∈ X ∶ x s ≡ (x u) d [ x s ] (B.8) For all such contexts, we define a convenience equivalence, x s, which is the accumulating service account, as found in the dictionary of (x u) d at the index x s. We track both regular and exceptional dimensions within our context mutator, but collapse the result of the invocation to one or the other depending on whether the termination was regular or exceptional (i.e. out-of-gas or panic). We define Ψ A, the Accumulation invocation function as: Ψ A ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ⎧ ⎩ U, N T, N S, N G, ⟦ O ⟧ ⎫ ⎭ → ⎧ ⎩ U, ⟦ T ⟧, H ?, N G ⎫ ⎭ (u, t, s, g, o) ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ⎧ ⎩ u, [], ∅, 0 ⎫ ⎭ if u d [ s ] c = ∅ C (Ψ M (u d [ s ] c, 5, g, E (t, s, ↕ o), F, ⎧ ⎩ I (u, s), I (u, s) ⎫ ⎭)) otherwise (B.9) I ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (U, N S) → X (u, s) ↦ ⎧ ⎩ s, u, i, t ▸ ▸ [], y ▸ ▸ ∅, p ▸ ▸ [] ⎫ ⎭ where i = check ((E − 1 4 (H (E (s, η ′ 0, H t))) mod (2 32 − 2 9)) + 2 8) (B.10) F ∈ Ω ⟨(X, X)⟩ ∶ (n, ϱ, ω, μ, (x, y)) ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ G (Ω R (ϱ, ω, μ, s, x s, d), (x, y)) if n = read G (Ω W (ϱ, ω, μ, s, x s), (x, y)) if n = write G (Ω L (ϱ, ω, μ, s, x s, d), (x, y)) if n = lookup Ω G (ϱ, ω, μ, (x, y)) if n = gas G (Ω I (ϱ, ω, μ, x s, d), (x, y)) if n = info Ω B (ϱ, ω, μ, (x, y)) if n = bless Ω A (ϱ, ω, μ, (x, y)) if n = assign Ω D (ϱ, ω, μ, (x, y)) if n = designate Ω C (ϱ, ω, μ, (x, y)) if n = checkpoint Ω N (ϱ, ω, μ, (x, y)) if n = new Ω U (ϱ, ω, μ, (x, y)) if n = upgrade Ω T (ϱ, ω, μ, (x, y)) if n = transfer Ω J (ϱ, ω, μ, (x, y), H t) if n = eject Ω Q (ϱ, ω, μ, (x, y)) if n = query Ω S (ϱ, ω, μ, (x, y), H t) if n = solicit Ω F (ϱ, ω, μ, (x, y), H t) if n = forget Ω Q (ϱ, ω, μ, (x, y)) if n = yield Ω P (ϱ, ω, μ, (x, y), x s) if n = provide (▸, ϱ − 10, [ ω 0 ,. .., ω 6, WHAT, ω 8 ,. .. ], μ, x) otherwise where d = (x u) d, s = (x u) d [ x s ] (B.11) G ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (({ ▸, ∎, ☇, ∞ }, N G, ⟦ N R ⟧ 13, M, A), (X, X)) → ({ ▸, ∎, ☇, ∞ }, N G, ⟦ N R ⟧ 13, M, (X, X)) ((ε, ϱ, ω, μ, s), (x, y)) ↦ (ε, ϱ, ω, μ, (x ∗, y)) where x ∗ = x except (x ∗ u) d [ x ∗ s ] = s (B.12) C ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (N G, Y ∪ { ∞, ☇ }, (X, X)) → (U, ⟦ T ⟧, H ?, N G, ⟦ ⎧ ⎩ N S, Y ⎫ ⎭ ⟧) (u, o, (x, y)) ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ ⎧ ⎩ y u, y t, y y, u, y p ⎫ ⎭ if o ∈ { ∞, ☇ } ⎧ ⎩ x u, x t, o, u, x p ⎫ ⎭ otherwise if o ∈ H ⎧ ⎩ x u, x t, x y, u, x p ⎫ ⎭ otherwise (B.13) The mutator F governs how this context will alter for any given parameterization, and the collapse function C selects one of the two dimensions of context depending on whether the virtual machine’s halt was regular or exceptional. The initializer function I maps some service account s along with its index s to yield a mutator context such that no alterations to state are implied (beyond those already inherent in s) in either exit scenario. Note that the component a utilizes the random accumulator η 0 and the block’s timeslot H t to create a deterministic sequence of identifiers which are extremely likely to be unique. Concretely, we create the identifier from the Blake2 hash of the identifier of the creating service, the current random accumulator η 0 and the block’s timeslot. Thus, within a service’s accumulation it is almost certainly unique, but it is not necessarily unique across all services, nor at all times in the past. We utilize a check function to find the first such JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 47 index in this sequence which does not already represent a service: (B.14) check (i ∈ N S) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ i if i ~ ∈ K (u d) check ((i − 2 8 + 1) mod (2 32 − 2 9) + 2 8) otherwise nb In the highly unlikely event that a block executes to find that a single service index has inadvertently been attached to two different services, then the block is considered invalid. Since no service can predict the identifier sequence ahead of time, they cannot intentionally disadvantage the block author.

B.5. On-Transfer Invocation
We define the On-Transfer service-account invocation function as Ψ T ; it is somewhat similar to the Accumulation Invocation except that the only state alteration it facilitates are basic alteration to the storage of the subject account. No further transfers may be made, no privileged operations are possible, no new accounts may be created nor other operations done on the subject account itself. The function is defined as: Ψ T ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (D ⟨ N S → A ⟩, N T, N S, ⟦ T ⟧) → ⎧ ⎩ A, N G ⎫ ⎭ (d, t, s, t) ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ⎧ ⎩ s, 0 ⎫ ⎭ if s c = ∅ ∨ t = [] ⎧ ⎩ s ′, u ⎫ ⎭ otherwise where (u, r, s ′) = Ψ M (s c, 10, ∑ r ∈ t (r g), E (t, s, ↕ t), F, s) and s = d [ s ] except s b = d [ s ] b + ∑ r ∈ t r a (B.15) and F ∈ Ω ⟨ A ⟩ ∶ (n, ϱ, ω, μ, s) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ Ω L (ϱ, ω, μ, s, s, d) if n = lookup Ω R (ϱ, ω, μ, s, s, d) if n = read Ω W (ϱ, ω, μ, s, s) if n = write Ω G (ϱ, ω, μ) if n = gas Ω I (ϱ, ω, μ, s, d) if n = info (▸, ϱ − 10, [ ω 0 ,. .., ω 6, WHAT, ω 8 ,. .. ], μ, s) otherwise (B.16)

I.3. Utilities, Externalities and Standard Functions
A (. ..) : The Merkle mountain range append function. See equation E.8. B n (. ..) : The octets-to-bits function for n octets. Superscripted − 1 to denote the inverse. See equation A.12. C (. ..) : The group of erasure-coding functions. C n (. ..) : The erasure-coding functions for n chunks. See equation H.6. E (. ..) : The octet-sequence encode function. Superscripted − 1 to denote the inverse. See appendix C. F (. ..) : The Fisher-Yates shuffle function. See equation F.1. H (. ..) : The Blake 2b 256-bit hash function. See section 3.8. H K (. ..) : The Keccak 256-bit hash function. See section 3.8. J x : The justification path to a specific 2 x size page of a constant-depth Merkle tree. See equation E.5. K (. ..) : The domain, or set of keys, of a dictionary. See section 3.5. L x : The 2 x size page function for a constant-depth Merkle tree. See equation E.6. M (. ..) : The constant-depth binary Merklization function. See appendix E. M B (. ..) : The well-balanced binary Merklization function. See appendix E. M σ (. ..) : The state Merklization function. See appendix D. N (. ..) : The erasure-coding chunks function. See appendix H. O (. ..) : The Bandersnatch ring root function. See section 3.8 and appendix G. P n (. ..) : The octet-array zero-padding function. See equation 14.17. Q (. ..) : The numeric-sequence-from-hash function. See equation F.3. R : The group of erasure-coding piece-recovery functions. S k (. ..) : The general signature function. See section 3.8. T : The current time expressed in seconds after the start of the Jam Common Era. See section 4.4. U (. ..) : The substitute-if-nothing function. See equation 3.2. V (. ..) : The range, or set of values, of a dictionary or sequence. See section 3.5. X n (. ..) : The signed-extension function for a value in N 2 8 n. See equation A.16. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 64 Y (. ..) : The alias/output/entropy function of a Bandersnatch vrf signature/proof. See section 3.8 and appendix G. Z n (. ..) : The into-signed function for a value in N 2 8 n. Superscripted with − 1 to denote the inverse. See equation A.10. ℘ ⟨. .. ⟩ : Power set function.

I.4. Values
I.4.1. Block-context Terms. These terms are all contextualized to a single block. They may be superscripted with some other term to alter the context and reference some other block. A : The ancestor set of the block. See equation 5.3. B : The block. See equation 4.2. C : The service accumulation-commitment, used to form the Beefy root. See equation ??. E : The block extrinsic. See equation 4.3. F v : The Beefy signed commitment of validator v. See equation 18.1. G : The mapping from cores to guarantor keys. See section 11.3. G ∗ : The mapping from cores to guarantor keys for the previous rotation. See section 11.3. H : The block header. See equation 5.1. I : The sequence of work-reports which were accumulated this in this block. See equation ??. Q : The selection of ready work-reports which a validator determined they must audit. See equation 17.1. R : The set of Ed25519 guarantor keys who made a work-report. See equation 11.26. S : The set of indices of services which have been accumulated (“progressed”) in the block. See equation ??. T : The ticketed condition, true if the block was sealed with a ticket signature rather than a fallback. See equations 6.15 and 6.16. U : The audit condition, equal to ⊺ once the block is audited. See section 17. V : The set of verdicts in the present block. See equation 10.11. W : The sequence of work-reports which have now become available and ready for accumulation. See equation 11.16. X : The sequence of transfers implied by the block’s accumulations. See equation 12.30. Without any superscript, the block is assumed to the block being imported or, if no block is being imported, the head of the best chain (see section 19). Explicit block-contextualizing superscripts include: B ♮ : The latest finalized block. See equation 19. B ♭ : The block at the head of the best chain. See equation 19. I.4.2. State components. Here, the prime annotation indicates posterior state. Individual components may be identified with a letter subscript. α : The core α uthorizations pool. See equation 8.1. β : Information on the most recent β locks. γ : State concerning Safrole. See equation 6.3. γ a : The sealing lottery ticket accumulator. γ k : The keys for the validators of the next epoch, equivalent to those keys which constitute γ z. γ s : The sealing-key sequence of the current epoch. γ z : The Bandersnatch root for the current epoch’s ticket submissions. δ : The (prior) state of the service accounts. δ † : The post-accumulation, pre-transfer intermediate state. δ ‡ : The post-transfer, pre-preimage integration intermediate state. η : The e η tropy accumulator and epochal ra η domness. ι : The validator keys and metadata to be drawn from next. κ : The validator κ eys and metadata currently active. λ : The validator keys and metadata which were active in the prior epoch. ρ : The ρ ending reports, per core, which are being made available prior to accumulation. ρ † : The post-judgment, pre-guarantees-extrinsic intermediate state. ρ ‡ : The post-guarantees-extrinsic, pre-assurances-extrinsic, intermediate state. σ : The σ verall state of the system. See equations 4.1, 4.4. τ : The most recent block’s τ imeslot. φ : The authorization queue. ψ : Past judgments on work-reports and validators. ψ b : Work-reports judged to be incorrect. ψ g : Work-reports judged to be correct. ψ w : Work-reports whose validity is judged to be unknowable. ψ o : Validators who made a judgment found to be incorrect. χ : The privileged service indices. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 65 χ m : The index of the blessed service. χ v : The index of the designate service. χ a : The index of the assign service. χ g : The always-accumulate service indices and their basic gas allowance. π : The activity statistics for the validators. ϑ : The accumulation queue. ξ : The accumulation history. I.4.3. Virtual Machine components. ε : The exit-reason resulting from all machine state transitions. ν : The immediate values of an instruction. μ : The memory sequence; a member of the set M. ϱ : The gas counter. ω : The registers. ζ : The instruction sequence. ϖ : The sequence of basic blocks of the program. ı : The instruction counter. I.4.4. Constants. A = 8 : The period, in seconds, between audit tranches. B I = 10 : The additional minimum balance required per item of elective service state. B L = 1 : The additional minimum balance required per octet of elective service state. B S = 100 : The basic minimum balance which all services require. C = 341 : The total number of cores. D = 19, 200 : The period in timeslots after which an unreferenced preimage may be expunged. E = 600 : The length of an epoch in timeslots. F = 2 : The audit bias factor, the expected number of additional validators who will audit a work-report in the following tranche for each no-show in the previous. G A = 10, 000, 000 : The gas allocated to invoke a work-report’s Accumulation logic. G I = 50, 000, 000 : The gas allocated to invoke a work-package’s Is-Authorized logic. G R = 5, 000, 000, 000 : The gas allocated to invoke a work-package’s Refine logic. G T = 3, 500, 000, 000 : The total gas allocated across for all Accumulation. Should be no smaller than G A ⋅ C + ∑ g ∈ V (χ g) (g). H = 8 : The size of recent history, in blocks. I = 16 : The maximum amount of work items in a package. J = 8 : The maximum sum of dependency items in a work-report. K = 16 : The maximum number of tickets which may be submitted in a single extrinsic. L = 14, 400 : The maximum age in timeslots of the lookup anchor. N = 2 : The number of ticket entries per validator. O = 8 : The maximum number of items in the authorizations pool. P = 6 : The slot period, in seconds. Q = 80 : The number of items in the authorizations queue. R = 10 : The rotation period of validator-core assignments, in timeslots. S = 1024 : The maximum number of entries in the accumulation queue. T = 128 : The maximum number of extrinsics in a work-package. U = 5 : The period in timeslots after which reported but unavailable work may be replaced. V = 1023 : The total number of validators. W B = 12 ⋅ 2 20 : The maximum size of an encoded work-package together with its extrinsic data and import implications, in octets. W C = 4, 000, 000 : The maximum size of service code in octets. W E = 684 : The basic size of erasure-coded pieces in octets. See equation H.6. W G = W P W E = 4104 : The size of a segment in octets. W M = 3, 072 : The maximum number of imports in a work-package. W P = 6 : The number of erasure-coded pieces in a segment. W R = 48 ⋅ 2 10 : The maximum total size of all unbounded blobs in a work-report, in octets. W T = 128 : The size of a transfer memo in octets. W X = 3, 072 : The maximum number of exports in a work-package. X : Context strings, see below. Y = 500 : The number of slots into an epoch at which ticket-submission ends. Z A = 2 : The pvm dynamic address alignment factor. See equation A.18. Z I = 2 24 : The standard pvm program initialization input data size. See equation A.7. Z P = 2 12 : The pvm memory page size. See equation 4.24. Z Z = 2 16 : The standard pvm program initialization zone size. See section A.7. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 66 I.4.5. Signing Contexts. X A = $jam_available : Ed25519 Availability assurances. X B = $jam_beefy : bls Accumulate-result-rootmmr commitment. X E = $jam_entropy : On-chain entropy generation. X F = $jam_fallback_seal : Bandersnatch Fallback block seal. X G = $jam_guarantee : Ed25519 Guarantee statements. X I = $jam_announce : Ed25519 Audit announcement statements. X T = $jam_ticket_seal : Bandersnatch Ring vrf Ticket generation and regular block seal. X U = $jam_audit : Bandersnatch Audit selection entropy. X ⊺ = $jam_valid : Ed25519 Judgments for valid work-reports. X  = $jam_invalid : Ed25519 Judgments for invalid work-reports. REFERENCES 67

B.6. General Functions
We come now to defining the host functions which are utilized by the pvm invocations. Generally, these map some pvm state, including invocation context, possibly together with some additional parameters, to a new pvm state. The general functions are all broadly of the form (ϱ ′ ∈ Z G, ω ′ ∈ ⟦ N R ⟧ 13, μ ′, s ′) = Ω ◻ (ϱ ∈ N G, ω ∈ ⟦ N R ⟧ 13, μ ∈ M, s ∈ A ,. ..). Functions which have a result component which is equivalent to the corresponding argument may have said components elided in the description. Functions may also depend upon particular additional parameters. Unlike the Accumulate functions in appendix B.7, these do not mutate an accumulation context, but merely a service account s. The gas function, Ω G has a parameter list suffixed with an ellipsis to denote that any additional parameters may be taken and are provided transparently into its result. This allows it to be easily utilized in multiple pvm invocations. Other than the gas-counter which is explicitly defined, elements of pvm state are each assumed to remain unchanged by the host-call unless explicitly specified. ϱ ′ ≡ ϱ − g (B.17) (ε ′, ω ′, μ ′, s ′) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (∞, ω, μ, s) if ϱ < g (▸, ω, μ, s) except as indicated below otherwise (B.18) Function Identifier Gas usage Mutations Ω G (ϱ, ω,. ..) gas = 0 g = 10 ω ′ 7 ≡ ϱ ′ JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 48 Function Identifier Gas usage Mutations Ω L (ϱ, ω, μ, s, s, d) lookup = 1 g = 10 let a = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ s if ω 7 ∈ { s, 2 64 − 1 } d [ ω 7 ] otherwise if ω 7 ∈ K (d) ∅ otherwise let [ h, o ] = ω 8 ⋅⋅⋅+ 2 let v = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ ∇ if N h ⋅⋅⋅+ 32 ~ ⊆ V μ ∅ otherwise if a = ∅ ∨ μ h ⋅⋅⋅+ 32 ~ ∈ K (a p) a p [ μ h ⋅⋅⋅+ 32 ] otherwise let f = min (ω 10, S v S) let l = min (ω 11, S v S − f) (ε ′, ω ′ 7, μ ′ o ⋅⋅⋅+ l) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, μ o ⋅⋅⋅+ l) if v = ∇ ∨ N o ⋅⋅⋅+ l ~ ⊆ V ∗ μ (▸, NONE, μ o ⋅⋅⋅+ l) otherwise if v = ∅ (▸, S v S, v f ⋅⋅⋅+ l) otherwise Ω R (ϱ, ω, μ, s, s, d) read = 2 g = 10 let s ∗ = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ s if ω 7 = 2 64 − 1 ω 7 otherwise let a = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ s if s ∗ = s d [ s ∗ ] otherwise if s ∗ ∈ K (d) ∅ otherwise let [ k o, k z, o ] = ω 8 ⋅⋅⋅+ 3 let v = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ ∇ if N k o ⋅⋅⋅+ k z ~ ⊆ V μ a s [ k ] otherwise if a ≠ ∅ ∧ k ∈ K (a s), where k = H (E 4 (s ∗) ⌢ μ k o ⋅⋅⋅+ k z) ∅ otherwise let f = min (ω 11, S v S) let l = min (ω 12, S v S − f) (ε ′, ω ′ 7, μ ′ o ⋅⋅⋅+ l) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, μ o ⋅⋅⋅+ l) if v = ∇ ∨ N o ⋅⋅⋅+ l ~ ⊆ V ∗ μ (▸, NONE, μ o ⋅⋅⋅+ l) otherwise if v = ∅ (▸, S v S, v f ⋅⋅⋅+ l) otherwise Ω W (ϱ, ω, μ, s, s) write = 3 g = 10 let [ k o, k z, v o, v z ] = ω 7 ⋅⋅⋅+ 4 let k = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ H (E 4 (s) ⌢ μ k o ⋅⋅⋅+ k z) if N k o ⋅⋅⋅+ k z ⊆ V μ ∇ otherwise let a = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ s, except K (a s) = K (a s) ∖ { k } if v z = 0 s, except a s [ k ] = μ v o ⋅⋅⋅+ v z otherwise if N v o ⋅⋅⋅+ v z ⊆ V μ ∇ otherwise let l = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ S s s [ k ]S if k ∈ K (s s) NONE otherwise (ε ′, ω ′ 7, s ′) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, s) if k = ∇ ∨ a = ∇ (▸, FULL, s) otherwise if a t > a b (▸, l, a) otherwise JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 49 Function Identifier Gas usage Mutations Ω I (ϱ, ω, μ, s, d) info = 4 g = 10 let t = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ d [ s ] if ω 7 = 2 64 − 1 d [ ω 7 ] otherwise let o = ω 8 let m = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ E (t c, t b, t t, t g, t m, t o, t i) if t ≠ ∅ ∅ otherwise ∀ i ∈ N S m S ∶ μ ′ o + i ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ m i if m ≠ ∅ ∧ N o ⋅⋅⋅+ S m S ⊆ V ∗ μ μ o + i otherwise (ε ′, ω ′ 7) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7) if N o ⋅⋅⋅+ S m S ~ ⊆ V ∗ μ (▸, NONE) otherwise if m = ∅ (▸, OK) otherwise

B.7. Accumulate Functions
This defines a number of functions broadly of the form (ϱ ′ ∈ Z G, ω ′ ∈ ⟦ N R ⟧ 13, μ ′, (x ′, y ′)) = Ω ◻ (ϱ ∈ N G, ω ∈ ⟦ N R ⟧ 13, μ ∈ M, (x ∈ X, y ∈ X) ,. ..). Functions which have a result component which is equivalent to the corresponding argument may have said components elided in the description. Functions may also depend upon particular additional parameters. Other than the gas-counter which is explicitly defined, elements of pvm state are each assumed to remain unchanged by the host-call unless explicitly specified. ϱ ′ ≡ ϱ − g (B.19) (ε ′, ω ′, μ ′, x ′, y ′) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (∞, ω, μ, x, y) if ϱ < g (▸, ω, μ, x, y) except as indicated below otherwise (B.20) Function Identifier Gas usage Mutations Ω B (ϱ, ω, μ, (x, y)) bless = 5 g = 10 let [ m, a, v, o, n ] = ω 7 ⋅⋅⋅+ 5 let g = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ {(s ↦ g) where E 4 (s) ⌢ E 8 (g) = μ o + 12 i ⋅⋅⋅+ 12 S i ∈ N n } if N o ⋅⋅⋅+ 12 n ⊆ V μ ∇ otherwise (ε ′, ω ′ 7, (x ′ u) x) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, (x u) x) if g = ∇ (▸, WHO, (x u) x) otherwise if (m, a, v) ~ ∈ (N S, N S, N S) (▸, OK, ⎧ ⎩ m, a, v, g ⎫ ⎭) otherwise Ω A (ϱ, ω, μ, (x, y)) assign = 6 g = 10 let o = ω 8 let c = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ [ μ o + 32 i ⋅⋅⋅+ 32 S i < − N Q ] if N o ⋅⋅⋅+ 32 Q ⊆ V μ ∇ otherwise (ε ′, ω ′ 7, (x ′ u) q [ ω 7 ]) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, (x u) q [ ω 7 ]) if c = ∇ (▸, CORE, (x u) q [ ω 7 ]) otherwise if ω 7 ≥ C (▸, OK, c) otherwise Ω D (ϱ, ω, μ, (x, y)) designate = 7 g = 10 let o = ω 7 let v = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ [ μ o + 336 i ⋅⋅⋅+ 336 S i < − N V ] if N o ⋅⋅⋅+ 336 V ⊆ V μ ∇ otherwise (ε ′, ω ′ 7, (x ′ u) i) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (☇, ω 7, (x u) i) if v = ∇ (▸, OK, v) otherwise Ω C (ϱ, ω, μ, (x, y)) checkpoint = 8 g = 10 y ′ ≡ x ω ′ 7 ≡ ϱ ′ JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 50 Function Identifier Gas usage Mutations Ω N (ϱ, ω, μ, (x, y)) new = 9 g = 10 let [ o, l, g, m ] = ω 7 ⋅⋅⋅+ 4 let c = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ μ o ⋅⋅⋅+ 32 if N o ⋅⋅⋅+ 32 ⊆ V μ ∧ l ∈ N 2 32 ∇ otherwise let a ∈ A ∪ { ∇ } = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ⎧ ⎩ c, s ▸ ▸ {}, l ▸ ▸ {(c, l) ↦ []}, b ▸ ▸ a t, g, m, p ▸ ▸ {} ⎫ ⎭ if c ≠ ∇ ∇ otherwise let s = x s except s b = (x s) b − a t (ε ′, ω ′ 7, x ′ i, (x ′ u) d) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, x i, (x u) d) if c = ∇ (▸, CASH, x i, (x u) d) otherwise if s b < (x s) t (▸, x i, check (i), (x u) d ∪ { x i ↦ a, x s ↦ s }) otherwise where i = 2 8 + (x i − 2 8 + 42) mod (2 32 − 2 9) Ω U (ϱ, ω, μ, (x, y)) upgrade = 10 g = 10 let [ o, g, m ] = ω 7 ⋅⋅⋅+ 3 let c = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ μ o ⋅⋅⋅+ 32 if N o ⋅⋅⋅+ 32 ⊆ V μ ∇ otherwise (ε ′, ω ′ 7, (x ′ s) c, (x ′ s) g, (x ′ s) m) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (☇, ω 7, (x s) c, (x s) g, (x s) m) if c = ∇ (▸, OK, c, g, m) otherwise Ω T (ϱ, ω, μ, (x, y)) transfer = 11 g = 10 + ω 9 let [ d, a, l, o ] = ω 7 ⋅⋅⋅+ 4, let d = (x u) d let t ∈ T ∪ { ∇ } = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ⎧ ⎩ s ▸ ▸ x s, d, a, m ▸ ▸ μ o ⋅⋅⋅+ W T, g ▸ ▸ l ⎫ ⎭ if N o ⋅⋅⋅+ W T ⊆ V μ ∇ otherwise let b = (x s) b − a (ε ′, ω ′ 7, x ′ t, (x ′ s) b) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, x t, (x s) b) if t = ∇ (▸, WHO, x t, (x s) b) otherwise if d ~ ∈ K (d) (▸, LOW, x t, (x s) b) otherwise if l < d [ d ] m (▸, CASH, x t, (x s) b) otherwise if b < (x s) t (▸, OK, x t t, b) otherwise Ω J (ϱ, ω, μ, (x, y), t) eject = 12 g = 10 let [ d, o ] = ω 7, 8 let h = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ μ o ⋅⋅⋅+ 32 if N o ⋅⋅⋅+ 32 ⊆ V μ ∇ otherwise let d = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ((x u) d)[ d ] if d ≠ x s ∧ d ∈ K ((x u) d) ∇ otherwise let l = max (81, d o) − 81 let s ′ = ((x u) d)[ x s ] except s ′ b = ((x u) d)[ x s ] b + d b (ε ′, ω ′ 7, (x ′ u) d) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, (x u) d) if h = ∇ (▸, WHO, (x u) d) otherwise if d = ∇ ∨ d c ≠ E 32 (x s) (▸, HUH, (x u) d) otherwise if d i ≠ 2 ∨ (h, l) ~ ∈ d l (▸, OK, (x u) d ∖ { d } ∪ { x s ↦ s ′ }) otherwise if d l [ h, l ] = [ x, y ], y < t − D (▸, HUH, (x u) d) otherwise JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 51 Function Identifier Gas usage Mutations Ω Q (ϱ, ω, μ, (x, y)) query = 13 g = 10 let [ o, z ] = ω 7, 8 let h = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ μ o ⋅⋅⋅+ 32 if N o ⋅⋅⋅+ 32 ⊆ V μ ∇ otherwise let a = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (x s) l [ h, z ] if (h, z) ∈ K ((x s) l) ∇ otherwise (ε ′, ω ′ 7, ω ′ 8) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, ω 8) if h = ∇ (▸, NONE, 0) otherwise if a = ∇ (▸, 0, 0) otherwise if a = [] (▸, 1 + 2 32 x, 0) otherwise if a = [ x ] (▸, 2 + 2 32 x, y) otherwise if a = [ x, y ] (▸, 3 + 2 32 x, y + 2 32 z) otherwise if a = [ x, y, z ] Ω S (ϱ, ω, μ, (x, y), t) solicit = 14 g = 10 let [ o, z ] = ω 7, 8 let h = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ μ o ⋅⋅⋅+ 32 if N o ⋅⋅⋅+ 32 ⊆ V μ ∇ otherwise let a = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ x s except: a l [ ⎧ ⎩ h, z ⎫ ⎭ ] = [] if h ≠ ∇ ∧ (h, z) ~ ∈ K ((x s) l) a l [ ⎧ ⎩ h, z ⎫ ⎭ ] = (x s) l [ ⎧ ⎩ h, z ⎫ ⎭ ] t if (x s) l [ ⎧ ⎩ h, z ⎫ ⎭ ] = [ x, y ] ∇ otherwise (ε ′, ω ′ 7, x ′ s) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, x s) if h = ∇ (▸, HUH, x s) otherwise if a = ∇ (▸, FULL, x s) otherwise if a b < a t (▸, OK, a) otherwise Ω F (ϱ, ω, μ, (x, y), t) forget = 15 g = 10 let [ o, z ] = ω 7, 8 let h = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ μ o ⋅⋅⋅+ 32 if N o ⋅⋅⋅+ 32 ⊆ V μ ∇ otherwise let a = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ x s except: K (a l) = K ((x s) l) ∖ { ⎧ ⎩ h, z ⎫ ⎭ }, K (a p) = K ((x s) p) ∖ { h } ¡ if (x s) l [ h, z ] ∈ {[], [ x, y ]}, y < t − D a l [ h, z ] = [ x, t ] if (x s) l [ h, z ] = [ x ] a l [ h, z ] = [ w, t ] if (x s) l [ h, z ] = [ x, y, w ], y < t − D ∇ otherwise (ε ′, ω ′ 7, x ′ s) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, x s) if h = ∇ (▸, HUH, x s) otherwise if a = ∇ (▸, OK, a) otherwise Ω Q (ϱ, ω, μ, (x, y)) yield = 16 g = 10 let o = ω 7 let h = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ μ o ⋅⋅⋅+ 32 if N o ⋅⋅⋅+ 32 ⊆ V μ ∇ otherwise (ε ′, ω ′ 7, x ′ y) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (☇, ω 7, x y) if h = ∇ (▸, OK, h) otherwise JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 52 Function Identifier Gas usage Mutations Ω P (ϱ, ω, μ, (x, y), s) provide = _ g = 10 let [ o, z ] = ω 8 ⋅⋅⋅+ 3 let s ∗ = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ s if ω 7 = 2 64 − 1 ω 7 otherwise let i = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ μ o ⋅⋅⋅+ z if N o ⋅⋅⋅+ z ⊆ V μ ∇ otherwise let a = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ d [ s ∗ ] if s ∗ ∈ K (d) ∅ otherwise (ε ′, ω ′ 7, x ′ p) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, x p) if i = ∇ (▸, WHO, x p) otherwise if a = ∅ (▸, HUH, x p) otherwise if a l [ ⎧ ⎩ H (i), z ⎫ ⎭ ] ≠ [] (▸, HUH, x p) otherwise if ⎧ ⎩ s ∗, i ⎫ ⎭ ∈ x p (▸, OK, x p ∪ { ⎧ ⎩ s ∗, i ⎫ ⎭ }) otherwise

B.8. Refine Functions
These assume some refine context pair (m, e) ∈ (D ⟨ N → M ⟩, ⟦ G ⟧), which are both initially empty. Other than the gas-counter which is explicitly defined, elements of pvm state are each assumed to remain unchanged by the host-call unless explicitly specified. ϱ ′ ≡ ϱ − g (B.21) (ε ′, ω ′, μ ′) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (∞, ω, μ) if ϱ < g (▸, ω, μ) except as indicated below otherwise (B.22) Function Identifier Gas usage Mutations Ω H (ϱ, ω, μ, (m, e), s, d, t) historical_lookup = 17 g = 10 let a = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ d [ s ] if ω 7 = 2 64 − 1 ∧ s ∈ K (d) d [ ω 7 ] if ω 7 ∈ K (d) ∅ otherwise let [ h, o ] = ω 8 ⋅⋅⋅+ 2 let v = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ ∇ if N h ⋅⋅⋅+ 32 ~ ⊆ V μ ∅ otherwise if a = ∅ Λ (a, t, μ h ⋅⋅⋅+ 32) otherwise let f = min (ω 10, S v S) let l = min (ω 11, S v S − f) (ε ′, ω ′ 7, μ ′ o ⋅⋅⋅+ l) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, μ o ⋅⋅⋅+ l) if v = ∇ ∨ N o ⋅⋅⋅+ l ~ ⊆ V ∗ μ (▸, NONE, μ o ⋅⋅⋅+ l) otherwise if v = ∅ (▸, S v S, v f ⋅⋅⋅+ l) otherwise JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 53 Function Identifier Gas usage Mutations Ω Y (ϱ, ω, μ, (m, e), i, p, o, i) fetch = 18 g = 10 let v = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ E (p) if ω 10 = 0 o if ω 10 = 1 p w [ ω 11 ] y if ω 10 = 2 ∧ ω 11 < S p w S x if ω 10 = 3 ∧ ω 11 < S p w S ∧ ω 12 < S p w [ ω 11 ] x S ∧ (H (x), S x S) = p w [ ω 11 ] x [ ω 12 ] x if ω 10 = 4 ∧ ω 11 < S p w [ i ] x S ∧ (H (x), S x S) = p w [ i ] x [ ω 11 ] i [ ω 11 ] ω 12 if ω 10 = 5 ∧ ω 11 < S i S ∧ ω 12 < S i [ ω 11 ]S i [ i ] ω 11 if ω 10 = 6 ∧ ω 11 < S i [ i ]S p p if ω 10 = 7 ∅ otherwise let o = ω 7 let f = min (ω 8, S v S) let l = min (ω 9, S v S − f) (ε ′, ω ′ 7, μ ′ o ⋅⋅⋅+ l) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, μ o ⋅⋅⋅+ l) if N o ⋅⋅⋅+ l ~ ⊆ V ∗ μ (▸, NONE, μ o ⋅⋅⋅+ l) otherwise if v = ∅ (▸, S v S, v f ⋅⋅⋅+ l) otherwise Ω E (ϱ, ω, μ, (m, e), ς) export = 19 g = 10 let p = ω 7 let z = min (ω 8, W G) let x = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ P W G (μ p ⋅⋅⋅+ z) if N p ⋅⋅⋅+ z ⊆ V μ ∇ otherwise (ε ′, ω ′ 7, e ′) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, e) if x = ∇ (▸, FULL, e) otherwise if ς + S e S ≥ W X (▸, ς + S e S, e x) otherwise Ω M (ϱ, ω, μ, (m, e)) machine = 20 g = 10 let [ p o, p z, i ] = ω 7 ⋅⋅⋅+ 3 let p = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ μ p o ⋅⋅⋅+ p z if N p o ⋅⋅⋅+ p z ⊆ V μ ∇ otherwise let n = min (n ∈ N, n ~ ∈ K (m)) let u = ⎧ ⎩ V ▸ ▸ [ 0, 0 ,. .. ], A ▸ ▸ [ ∅, ∅ ,. .. ] ⎫ ⎭ (ε ′, ω ′ 7, m) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, m) if p = ∇ (▸, HUH, m) otherwise if deblob (p) = ∇ (▸, n, m ∪ { n ↦ ⎧ ⎩ p, u, i ⎫ ⎭ }) otherwise Ω P (ϱ, ω, μ, (m, e)) peek = 21 g = 10 let [ n, o, s, z ] = ω 7 ⋅⋅⋅+ 4 (ε ′, ω ′ 7, μ ′) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, μ) if N o ⋅⋅⋅+ z ~ ⊆ V ∗ μ (▸, WHO, μ) otherwise if n ~ ∈ K (m) (▸, OOB, μ) otherwise if N s ⋅⋅⋅+ z ~ ⊆ V m [ n ] u (▸, OK, μ ′) otherwise where μ ′ = μ except μ o ⋅⋅⋅+ z = (m [ n ] u) s ⋅⋅⋅+ z Ω O (ϱ, ω, μ, (m, e)) poke = 22 g = 10 let [ n, s, o, z ] = ω 7 ⋅⋅⋅+ 4 (ε ′, ω ′ 7, m ′) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, m) if N s ⋅⋅⋅+ z ~ ⊆ V μ (▸, WHO, m) otherwise if n ~ ∈ K (m) (▸, OOB, m) otherwise if N o ⋅⋅⋅+ z ~ ⊆ V ∗ m [ n ] u (▸, OK, m ′) otherwise where m ′ = m except (m ′ [ n ] u) o ⋅⋅⋅+ z = μ s ⋅⋅⋅+ z JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 54 Function Identifier Gas usage Mutations Ω Z (ϱ, ω, μ, (m, e)) zero = 23 g = 10 let [ n, p, c ] = ω 7 ⋅⋅⋅+ 3 let u = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ m [ n ] u if n ∈ K (m) ∇ otherwise let u ′ = u except ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (u ′ V) p Z P ⋅⋅⋅+ c Z P = [ 0, 0 ,. .. ] (u ′ A) p ⋅⋅⋅+ c = [ W, W ,. .. ] (ω ′ 7, m ′) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ (HUH, m) if p < 16 ∨ p + c ≥ 2 32 ~ Z P (WHO, m) if u = ∇ (OK, m ′), where m ′ = m except m ′ [ n ] u = u ′ otherwise Ω V (ϱ, ω, μ, (m, e)) void = 24 g = 10 let [ n, p, c ] = ω 7 ⋅⋅⋅+ 3 let u = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ m [ n ] u if n ∈ K (m) ∇ otherwise let u ′ = u except ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (u ′ V) p Z P ⋅⋅⋅+ c Z P = [ 0, 0 ,. .. ] (u ′ A) p ⋅⋅⋅+ c = [ ∅, ∅ ,. .. ] (ω ′ 7, m ′) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (WHO, m) if u = ∇ (HUH, m) otherwise if p < 16 ∨ p + c ≥ 2 32 ~ Z P ∨ ∃ i ∈ N p ⋅⋅⋅+ c ∶ (u A) i = ∅ (OK, m ′) otherwise where m ′ = m except m ′ [ n ] u = u ′ Ω K (ϱ, ω, μ, (m, e)) invoke = 25 g = 10 let [ n, o ] = ω 7, 8 let (g, w) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (g, w) ∶ E 8 (g) ⌢ Ð E # 8 (w) = μ o ⋅⋅⋅+ 112 if N o ⋅⋅⋅+ 112 ⊆ V ∗ μ (∇, ∇) otherwise let (c, i ′, g ′, w ′, u ′) = Ψ (m [ n ] p, m [ n ] i, g, w, m [ n ] u) let μ ∗ = μ except μ ∗ o ⋅⋅⋅+ 112 = E 8 (g ′) ⌢ Ð E # 8 (w ′) let m ∗ = m except ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ m ∗ [ n ] u = u ′ m ∗ [ n ] i = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ i ′ + 1 if c ∈ {̵ h } × N R i ′ otherwise (ε ′, ω ′ 7, ω ′ 8, μ ′, m ′) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (☇, ω 7, ω 8, μ, m) if g = ∇ (▸, WHO, ω 8, μ, m) otherwise if n ~ ∈ m (▸, HOST, h, μ ∗, m ∗) otherwise if c = ̵ h × h (▸, FAULT, x, μ ∗, m ∗) otherwise if c = F × x (▸, OOG, ω 8, μ ∗, m ∗) otherwise if c = ∞ (▸, PANIC, ω 8, μ ∗, m ∗) otherwise if c = ☇ (▸, HALT, ω 8, μ ∗, m ∗) otherwise if c = ∎ Ω X (ϱ, ω, μ, (m, e)) expunge = 26 g = 10 let n = ω 7 (ω ′ 7, m ′) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (WHO, m) if n ≠ K (m) (m [ n ] i, m ∖ n) otherwise

C.1. Common Terms
Our codec function E is used to serialize some term into a sequence of octets. We define the deserialization function E − 1 as the inverse of E and able to decode some sequence into the original value. The codec is designed such that exactly one value is encoded into any given sequence of octets, and in cases where this is not desirable then we use special codec functions. C.1.1. Trivial Encodings. We define the serialization of ∅ as the empty sequence: (C.1) E (∅) ≡ [] JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 55 We also define the serialization of an octet-sequence as itself: (C.2) E (x ∈ Y) ≡ x We define anonymous tuples to be encoded as the concatenation of their encoded elements: (C.3) E (⎧ ⎩ a, b,. .. ⎫ ⎭) ≡ E (a) ⌢ E (b) ⌢. .. Passing multiple arguments to the serialization functions is equivalent to passing a tuple of those arguments. Formally: E (a, b,. ..) ≡ E (⎧ ⎩ a, b,. .. ⎫ ⎭) (C.4) C.1.2. Integer Encoding. We first define the trivial natural number serialization functions which are subscripted by the number of octets of the final sequence. Values are encoded in a regular little-endian fashion. This is utilized for almost all integer encoding across the protocol. Formally: (C.5) E l ∈ N ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ N 2 8 l → Y l x ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ [] if l = 0 [ x mod 256 ] ⌢ E l − 1  x 256  otherwise We define general natural number serialization, able to encode naturals of up to 2 64, as: (C.6) E ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ N 2 64 → Y 1 ∶ 9 x ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ [ 0 ] if x = 0  2 8 − 2 8 − l + x 2 8 l  ⌢ E l (x mod 2 8 l) if ∃ l ∈ N 8 ∶ 2 7 l ≤ x < 2 7 (l + 1) [ 2 8 − 1 ] ⌢ E 8 (x) otherwise if x < 2 64 C.1.3. Sequence Encoding. We define the sequence serialization function E (⟦ T ⟧) for any T which is itself a subset of the domain of E. We simply concatenate the serializations of each element in the sequence in turn: (C.7) E ([ i 0, i 1, ... ]) ≡ E (i 0) ⌢ E (i 1) ⌢. .. Thus, conveniently, fixed length octet sequences (e.g. hashes H and its variants) have an identity serialization. C.1.4. Discriminator Encoding. When we have sets of heterogeneous items such as a union of different kinds of tuples or sequences of different length, we require a discriminator to determine the nature of the encoded item for successful deserialization. Discriminators are encoded as a natural and are encoded immediately prior to the item. We generally use a length discriminator when serializing sequence terms which have variable length (e.g. general blobs Y or unbound numeric sequences ⟦ N ⟧) (though this is omitted in the case of fixed-length terms such as hashes H). 19 In this case, we simply prefix the term its length prior to encoding. Thus, for some term y ∈ ⎧ ⎩ x ∈ Y ,. .. ⎫ ⎭, we would generally define its serialized form to be E (S x S) ⌢ E (x) ⌢. .. . To avoid repetition of the term in such cases, we define the notation ↕ x to mean that the term of value x is variable in size and requires a length discriminator. Formally: (C.8) ↕ x ≡ ⎧ ⎩ S x S, x ⎫ ⎭ thus E (↕ x) ≡ E (S x S) ⌢ E (x) We also define a convenient discriminator operator ¿ x specifically for terms defined by some serializable set in union with ∅ (generally denoted for some set S as S ?): ¿ x ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0 if x = ∅ (1, x) otherwise (C.9) C.1.5. Bit Sequence Encoding. A sequence of bits b ∈ B is a special case since encoding each individual bit as an octet would be very wasteful. We instead pack the bits into octets in order of least significant to most, and arrange into an octet stream. In the case of a variable length sequence, then the length is prefixed as in the general case. E (b ∈ B) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ [] if b = []  min (8, S b S) ∑ i = 0 b i ⋅ 2 i ⌢ E (b 8 ...) otherwise (C.10) C.1.6. Dictionary Encoding. In general, dictionaries are placed in the Merkle trie directly (see appendix E for details). However, small dictionaries may reasonably be encoded as a sequence of pairs ordered by the key. Formally: (C.11) ∀ K, V ∶ E (d ∈ D ⟨ K → V ⟩) ≡ E (↕ [ k ^ ^ ⎧ ⎩ E (k), E (d [ k ]) ⎫ ⎭ S k ∈ K (d)]) C.1.7. Set Encoding. For any values which are sets and don’t already have a defined encoding above, we define the serialization of a set as the serialization of the set’s elements in proper order. Formally: (C.12) E ({ a, b, c,. .. }) ≡ E (a) ⌢ E (b) ⌢ E (c) ⌢. .. where a < b < c <. .. 19 Note that since specific values may belong to both sets which would need a discriminator and those that would not then we are sadly unable to introduce a function capable of serializing corresponding to the term ’s limitation. A more sophisticated formalism than basic set-theory would be needed, capable of taking into account not simply the value but the term from which or to which it belongs in order to do this succinctly. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 56

C.2. Block Serialization
A block B is serialized as a tuple of its elements in regular order, as implied in equations 4.2, 4.3 and 5.1. For the header, we define both the regular serialization and the unsigned serialization E U. Formally: E (B) = E (H, E T (E T), E P (E P), E G (E G), E A (E A), E D (E D)) (C.13) E T (E T) = E (↕ E T) (C.14) E P (E P) = E (↕ [ ⎧ ⎩ s, ↕ p ⎫ ⎭ S ⎧ ⎩ s, p ⎫ ⎭ < − E P ]) (C.15) E G (E G) = E (↕ [ ⎧ ⎩ w, E 4 (t), ↕ a ⎫ ⎭ S ⎧ ⎩ w, t, a ⎫ ⎭ < − E G ]) (C.16) E A (E A) = E (↕ [ ⎧ ⎩ a, f, E 2 (v), s ⎫ ⎭ S ⎧ ⎩ a, f, v, s ⎫ ⎭ < − E A ]) (C.17) E D ((v, c, f)) = E (↕ [(r, E 4 (a), [(v, E 2 (i), s) S (v, i, s) < − j ]) S (r, a, j) < − v ], ↕ c, ↕ f) (C.18) E (H) = E U (H) ⌢ E (H s) (C.19) E U (H) = E (H p, H r, H x) ⌢ E 4 (H t) ⌢ E (¿ H e, ¿ H w, ↕ H o, E 2 (H i), H v) (C.20) E (x ∈ X) ≡ E (x a, x s, x b, x l) ⌢ E 4 (x t) ⌢ E (↕ x p) (C.21) E (x ∈ S) ≡ E (x h) ⌢ E 4 (x l) ⌢ E (x u, x e) ⌢ E 2 (x n) (C.22) E (x ∈ L) ≡ E (E 4 (x s), x c, x y, E 8 (x g), O (x d), x u, x i, x x, x z, x e) (C.23) E (x ∈ W) ≡ E (x s, x x, x c, x a, ↕ x o, ↕ x l, ↕ x r, x g) (C.24) E (x ∈ P) ≡ E (↕ x j, E 4 (x h), x u, ↕ x p, x x, ↕ x w) (C.25) E (x ∈ I) ≡ E (E 4 (x s), x c, ↕ x y, E 8 (x g), E 8 (x a), Õ × Ö E # I (x i), ↕ [(h, E 4 (i)) S (h, i) < − x x ], E 2 (x e)) (C.26) E (x ∈ C) ≡ E (x y, x r) (C.27) E (x ∈ T) ≡ E (E 4 (x s), E 4 (x d), E 8 (x a), E (x m), E 8 (x g)) (C.28) E (x ∈ O) ≡ E (x h, x e, x a, ↕ x o, x y, x g, O (x d)) (C.29) O (o ∈ J ∪ Y) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (0, ↕ o) if o ∈ Y 1 if o = ∞ 2 if o = ☇ 3 if o = ⊚ 4 if o = BAD 5 if o = BIG (C.30) E I (h ∈ H ∪ H ⊞, i ∈ N 2 15) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (h, E 2 (i)) if h ∈ H (r, E 2 (i + 2 15)) if ∃ r ∈ H, h = r ⊞ (C.31) Note the use of O above to succinctly encode the result of a work item and the slight transformations of E G and E P to take account of the fact their inner tuples contain variable-length sequence terms a and p which need length discriminators.

D.1. Serialization
The serialization of state primarily involves placing all the various components of σ into a single mapping from 32-octet sequence state-keys to octet sequences of indefinite length. The state-key is constructed from a hash component and a chapter component, equivalent to either the index of a state component or, in the case of the inner dictionaries of δ, a service index. We define the state-key constructor functions C as: (D.1) C ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ N 2 8 ∪ (N 2 8, N S) ∪ ⎧ ⎩ N S, Y ⎫ ⎭ → H i ∈ N 2 8 ↦ [ i, 0, 0 ,. .. ] (i, s ∈ N S) ↦ [ i, n 0, 0, n 1, 0, n 2, 0, n 3, 0, 0 ,. .. ] where n = E 4 (s) (s, h) ↦ [ n 0, h 0, n 1, h 1, n 2, h 2, n 3, h 3, h 4, h 5 ,. .., h 27 ] where n = E 4 (s) The state serialization is then defined as the dictionary built from the amalgamation of each of the components. Cryptographic hashing ensures that there will be no duplicate state-keys given that there are no duplicate inputs to C. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 57 Formally, we define T which transforms some state σ into its serialized form: (D.2) T (σ) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ C (1) ↦ E ([ ↕ x S x < − α ]), C (2) ↦ E (φ), C (3) ↦ E (↕ [(h, E M (b), s, ↕ p) S (h, b, s, p) < − β ]), C (4) ↦ E  ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ γ k, γ z,  0 if γ s ∈ ⟦ C ⟧ E 1 if γ s ∈ ⟦ H B ⟧ E ¡, γ s, ↕ γ a ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ , C (5) ↦ E (↕ [ x ^ ^ x ∈ ψ g ], ↕ [ x ^ ^ x ∈ ψ b ], ↕ [ x ^ ^ x ∈ ψ w ], ↕ [ x ^ ^ x ∈ ψ o ]), C (6) ↦ E (η), C (7) ↦ E (ι), C (8) ↦ E (κ), C (9) ↦ E (λ), C (10) ↦ E ([ ¿ (w, E 4 (t)) S (w, t) < − ρ ]), C (11) ↦ E 4 (τ), C (12) ↦ E 4 (χ m, χ a, χ v) ⌢ E (χ g), C (13) ↦ E (E # 4 (π V), E # 4 (π L), π C, π S), C (14) ↦ E ([ ↕ (w, ↕ d) S (w, d) < − i S i < − ϑ ]), C (15) ↦ E ([ ↕ i S i < − ξ ]), ∀ (s ↦ a) ∈ δ ∶ C (255, s) ↦ a c ⌢ E 8 (a b, a g, a m, a o) ⌢ E 4 (a i), ∀ (s ↦ a) ∈ δ, (k ↦ v) ∈ a s ∶ C (s, E 4 (2 32 − 1) ⌢ k 0 ... 28) ↦ v, ∀ (s ↦ a) ∈ δ, (h ↦ p) ∈ a p ∶ C (s, E 4 (2 32 − 2) ⌢ h 1 ... 29) ↦ p, ∀ (s ↦ a) ∈ δ, (⎧ ⎩ h, l ⎫ ⎭ ↦ t) ∈ a l ∶ C (s, E 4 (l) ⌢ H (h) 2 ... 30) ↦ E (↕ [ E 4 (x) S x < − t ]) Note that most rows describe a single mapping between key derived from a natural and the serialization of a state component. However, the final four rows each define sets of mappings since these items act over all service accounts and in the case of the final three rows, the keys of a nested dictionary with the service. Also note that all non-discriminator numeric serialization in state is done in fixed-length according to the size of the term.

D.2. Merklization
With T defined, we now define the rest of M σ which primarily involves transforming the serialized mapping into a cryptographic commitment. We define this commitment as the root of the binary Patricia Merkle Trie with a format optimized for modern compute hardware, primarily by optimizing sizes to fit succinctly into typical memory layouts and reducing the need for unpredictable branching. D.2.1. Node Encoding and Trie Identification. We identify (sub-)tries as the hash of their root node, with one exception: empty (sub-)tries are identified as the zero-hash, H 0. Nodes are fixed in size at 512 bit (64 bytes). Each node is either a branch or a leaf. The first bit discriminate between these two types. In the case of a branch, the remaining 511 bits are split between the two child node hashes, using the last 255 bits of the 0-bit (left) sub-trie identity and the full 256 bits of the 1-bit (right) sub-trie identity. Leaf nodes are further subdivided into embedded-value leaves and regular leaves. The second bit of the node discriminates between these. In the case of an embedded-value leaf, the remaining 6 bits of the first byte are used to store the embedded value size. The following 31 bytes are dedicated to the first 31 bytes of the key. The last 32 bytes are defined as the value, filling with zeroes if its length is less than 32 bytes. In the case of a regular leaf, the remaining 6 bits of the first byte are zeroed. The following 31 bytes store the first 31 bytes of the key. The last 32 bytes store the hash of the value. Formally, we define the encoding functions B and L : B ∶  (H, H) → B 512 (l, r) ↦ [ 0 ] ⌢ bits (l) 1 ... ⌢ bits (r) (D.3) L ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (H, Y) → B 512 (k, v) ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ [ 1, 0 ] ⌢ bits (E 1 (S v S)) 2 ... ⌢ bits (k) ... 248 ⌢ bits (v) ⌢ [ 0, 0 ,. .. ] if S v S ≤ 32 [ 1, 1, 0, 0, 0, 0, 0, 0 ] ⌢ bits (k) ... 248 ⌢ bits (H (v)) otherwise (D.4) We may then define the basic Merklization function M σ as: M σ (σ) ≡ M ({(bits (k) ↦ ⎧ ⎩ k, v ⎫ ⎭) S (k ↦ v) ∈ T (σ)}) (D.5) JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 58 M (d ∶ D ⟨ B → (H, Y)⟩) ≡ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ H 0 if S d S = 0 H (bits − 1 (L (k, v))) if V (d) = {(k, v)} H (bits − 1 (B (M (l), M (r)))) otherwise where ∀ b, p ∶ (b ↦ p) ∈ d ⇔ (b 1 ... ↦ p) ∈ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ l if b 0 = 0 r if b 0 = 1 (D.6)

E.1. Binary Merkle Trees
The Merkle tree is a cryptographic data structure yielding a hash commitment to a specific sequence of values. It provides O (N) computation and O (log (N)) proof size for inclusion. This well-balanced formulation ensures that the maximum depth of any leaf is minimal and that the number of leaves at that depth is also minimal. The underlying function for our Merkle trees is the node function N, which accepts some sequence of blobs of some length n and provides either such a blob back or a hash: (E.1) N ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (⟦ Y n ⟧, Y → H) → Y n ∪ H (v, H) ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ H 0 if S v S = 0 v 0 if S v S = 1 H ($node ⌢ N (v ... ⌈ S v S ~ 2 ⌉, H) ⌢ N (v ⌈ S v S ~ 2 ⌉ ..., H)) otherwise The astute reader will realize that if our Y n happens to be equivalent H then this function will always evaluate into H. That said, for it to be secure care must be taken to ensure there is no possibility of preimage collision. For this purpose we include the hash prefix $node to minimize the chance of this; simply ensure any items are hashed with a different prefix and the system can be considered secure. We also define the trace function T, which returns each opposite node from top to bottom as the tree is navigated to arrive at some leaf corresponding to the item of a given index into the sequence. It is useful in creating justifications of data inclusion. (E.2) T ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (⟦ Y n ⟧, N S v S, Y → H) → ⟦ Y n ∪ H ⟧ (v, i, H) ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ [ N (P  (v, i), H)] ⌢ T (P ⊺ (v, i), i − P I (v, i), H) if S v S > 1 [] otherwise where P s (v, i) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ v ... ⌈ S v S ~ 2 ⌉ if (i < ⌈ S v S ~ 2 ⌉) = s v ⌈ S v S ~ 2 ⌉ ... otherwise and P I (v, i) ≡ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0 if i < ⌈ S v S ~ 2 ⌉ ⌈ S v S ~ 2 ⌉ otherwise From this we define our other Merklization functions. E.1.1. Well-Balanced Tree. We define the well-balanced binary Merkle function as M B : (E.3) M B ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (⟦ Y ⟧, Y → H) → H (v, H) ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ H (v 0) if S v S = 1 N (v, H) otherwise This is suitable for creating proofs on data which is not much greater than 32 octets in length since it avoids hashing each item in the sequence. For sequences with larger data items, it is better to hash them beforehand to ensure proof-size is minimal since each proof will generally contain a data item. Note: In the case that no hash function argument H is supplied, we may assume the Blake 2b hash function, H. E.1.2. Constant-Depth Tree. We define the constant-depth binary Merkle function as M. We define two corresponding functions for working with subtree pages, J x and L x. The latter provides a single page of leaves, themselves hashed, prefixed data. The former provides the Merkle path to a single page. Both assume size-aligned pages of size 2 x and accept page indices. M ∶  (⟦ Y ⟧, Y → H) → H (v, H) ↦ N (C (v, H), H) (E.4) J x ∶  (⟦ Y ⟧, N S v S, Y → H) → ⟦ H ⟧ (v, i, H) ↦ T (C (v, H), 2 x i, H) ... max (0, ⌈ log 2 (max (1, S v S)) − x ⌉) (E.5) L x ∶  (⟦ Y ⟧, N S v S, Y → H) → ⟦ H ⟧ (v, i, H) ↦ [ H ($leaf ⌢ l) S l < − v 2 x i... min (2 x i + 2 x, S v S) ] (E.6) For the latter justification J x to be acceptable, we must assume the target observer also knows not merely the value of the item at the given index, but also all other leaves within its 2 x size subtree, given by L x. As above, we may assume a default value for H of the Blake 2b hash function, H. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 59 For justifications and Merkle root calculations, a constancy preprocessor function C is applied which hashes all data items with a fixed prefix ”leaf” and then pads the overall size to the next power of two with the zero hash H 0 : (E.7) C ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (⟦ Y ⟧, Y → H) → ⟦ H ⟧ (v, H) ↦ v ′ where ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ S v ′ S = 2 ⌈ log 2 (max (1, S v S))⌉ v ′ i = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ H ($leaf ⌢ v i) if i < S v S H 0 otherwise

E.2. Merkle Mountain Ranges
The Merkle Mountain Range (mmr) is an append-only cryptographic data structure which yields a commitment to a sequence of values. Appending to an mmr and proof of inclusion of some item within it are both O (log (N)) in time and space for the size of the set. We define a Merkle Mountain Range as being within the set ⟦ H ? ⟧, a sequence of peaks, each peak the root of a Merkle tree containing 2 i items where i is the index in the sequence. Since we support set sizes which are not always powers-of-two-minus-one, some peaks may be empty, ∅ rather than a Merkle root. Since the sequence of hashes is somewhat unwieldy as a commitment, Merkle Mountain Ranges are themselves generally hashed before being published. Hashing them removes the possibility of further appending so the range itself is kept on the system which needs to generate future proofs. We define the append function A as: (E.8) A ∶ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (⟦ H ? ⟧, H, Y → H) → ⟦ H ? ⟧ (r, l, H) ↦ P (r, l, 0, H) where P ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (⟦ H ? ⟧, H, N, Y → H) → ⟦ H ? ⟧ (r, l, n, H) ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ r l if n ≥ S r S R (r, n, l) if n < S r S ∧ r n = ∅ P (R (r, n, ∅), H (r n ⌢ l), n + 1, H) otherwise and R ∶ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (⟦ T ⟧, N, T) → ⟦ T ⟧ (s, i, v) ↦ s ′ where s ′ = s except s ′ i = v We define the mmr encoding function as E M : (E.9) E M ∶ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ⟦ H ? ⟧ → Y b ↦ E (↕ [ ¿ x S x < − b ]) We define the mmr super-peak function as M R : (E.10) M R ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ⟦ H ? ⟧ → H b ↦ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ H 0 if S h S = 0 h 0 if S h S = 1 H K ($peak ⌢ M R (h ... S h S − 1) ⌢ h S h S − 1) otherwise where h = [ h S h < − b, h ≠ ∅ ]

H.1. Blob Encoding and Recovery
We assume some data blob d ∈ Y 684 k, k ∈ N. We are able to express this as a whole number of k pieces each of a sequence of 684 octets. We denote these (data-parallel) pieces p ∈ ⟦ Y 684 ⟧ = unzip 684 (d). Each piece is then reformed as 342 octet pairs and erasure-coded using C as above to give 1,023 octet pairs per piece. The resulting matrix is grouped by its pair-index and concatenated to form 1,023 chunks, each of k octet-pairs. Any 342 of these chunks may then be used to reconstruct the original data d. Formally we begin by defining four utility functions for splitting some large sequence into a number of equal-sized sub-sequences and for reconstituting such subsequences back into a single large sequence: ∀ n ∈ N, k ∈ N ∶ split n (d ∈ Y k ⋅ n) ∈ ⟦ Y n ⟧ k ≡  d 0 ⋅⋅⋅+ n, d n ⋅⋅⋅+ n, ⋯, d (k − 1) n ⋅⋅⋅+ n  (H.1) ∀ n ∈ N, k ∈ N ∶ join n (c ∈ ⟦ Y n ⟧ k) ∈ Y k ⋅ n ≡ c 0 ⌢ c 1 ⌢. .. (H.2) ∀ n ∈ N, k ∈ N ∶ unzip n (d ∈ Y k ⋅ n) ∈ ⟦ Y n ⟧ k ≡ [[ d j ⋅ k + i S j < − N n ] S i < − N k ] (H.3) ∀ n ∈ N, k ∈ N ∶ lace n (c ∈ ⟦ Y n ⟧ k) ∈ Y k ⋅ n ≡ d where ∀ i ∈ N k, j ∈ N n ∶ d j ⋅ k + i = (c i) j (H.4) We define the transposition operator hence: (H.5) T [[ x 0, 0, x 0, 1, x 0, 2 ,. .. ], [ x 1, 0, x 1, 1 ,. .. ] ,. .. ] ≡ [[ x 0, 0, x 1, 0, x 2, 0 ,. .. ], [ x 0, 1, x 1, 1 ,. .. ] ,. .. ] We may then define our erasure-code chunking function which accepts an arbitrary sized data blob whose length divides wholly into 684 octets and results in 1,023 sequences of sequences each of smaller blobs: (H.6) C k ∈ N ∶ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ Y 684 k → ⟦ Y 2 k ⟧ 1023 d ↦ [ join (c) S c < − T [ C (p) S p < − unzip 684 (d)]] The original data may be reconstructed with any 342 of the 1,023 resultant items (along with their indices). If the original 342 items are known then reconstruction is just their concatenation. (H.7) R k ∈ N ∶ ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ {(Y 2 k, N 1023)} 342 → Y 684 k c ↦ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ E ([ x S (x, i) < − c ]) if [ i S (x, i) < − c ] = [ 0, 1 ,. .. 341 ] lace k ([ R ([(split 2 (x) p, i) S (x, i) < − c ]) S p ∈ N k always ]) JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 61 Segment encoding/decoding may be done using the same functions albeit with a constant k = 6.

H.2. Code Word representation
For the sake of brevity we call each octet pair a word. The code words (including the message words) are treated as element of F 2 16 finite field. The field is generated as an extension of F 2 using the irreducible polynomial: (H.8) x 16 + x 5 + x 3 + x 2 + 1 Hence: (H.9) F 16 ≡ F 2 [ x ] x 16 + x 5 + x 3 + x 2 + 1 We name the generator of F 16 F 2, the root of the above polynomial, α as such: F 16 = F 2 (α). Instead of using the standard basis { 1, α, α 2 ,. .., α 15 }, we opt for a representation of F 16 which performs more efficiently for the encoding and the decoding process. To that aim, we name this specific representation of F 16 as ˜ F 16 and define it as a vector space generated by the following Cantor basis: v 0 1 v 1 α 15 + α 13 + α 11 + α 10 + α 7 + α 6 + α 3 + α v 2 α 13 + α 12 + α 11 + α 10 + α 3 + α 2 + α v 3 α 12 + α 10 + α 9 + α 5 + α 4 + α 3 + α 2 + α v 4 α 15 + α 14 + α 10 + α 8 + α 7 + α v 5 α 15 + α 14 + α 13 + α 11 + α 10 + α 8 + α 5 + α 3 + α 2 + α v 6 α 15 + α 12 + α 8 + α 6 + α 3 + α 2 v 7 α 14 + α 4 + α v 8 α 14 + α 13 + α 11 + α 10 + α 7 + α 4 + α 3 v 9 α 12 + α 7 + α 6 + α 4 + α 3 v 10 α 14 + α 13 + α 11 + α 9 + α 6 + α 5 + α 4 + α v 11 α 15 + α 13 + α 12 + α 11 + α 8 v 12 α 15 + α 14 + α 13 + α 12 + α 11 + α 10 + α 8 + α 7 + α 5 + α 4 + α 3 v 13 α 15 + α 14 + α 13 + α 12 + α 11 + α 9 + α 8 + α 5 + α 4 + α 2 v 14 α 15 + α 14 + α 13 + α 12 + α 11 + α 10 + α 9 + α 8 + α 5 + α 4 + α 3 v 15 α 15 + α 12 + α 11 + α 8 + α 4 + α 3 + α 2 + α Every message word m i = m i, 15. .. m i, 0 consists of 16 bits. As such it could be regarded as binary vector of length 16: (H.10) m i = (m i, 0. .. m i, 15) Where m i, 0 is the least significant bit of message word m i. Accordingly we consider the field element ˜ m i = ∑ 15 j = 0 m i,j v j to represent that message word. Similarly, we assign a unique index to each validator between 0 and 1,022 and we represent validator i with the field element: (H.11) ˜ i = 15 ∑ j = 0 i j v j where i = i 15. .. i 0 is the binary representation of i.

H.3. The Generator Polynomial
To erasure code a message of 342 words into 1023 code words, we represent each message as a field element as described in previous section and we interpolate the polynomial p (y) of maximum 341 degree which satisfies the following equalities: (H.12) p (˜ 0) = È m 0 p (˜ 1) = È m 1 ⋮ p (É 341) = Ê m 341 After finding p (y) with such properties, we evaluate p at the following points: (H.13) É r 342 ∶ = p (É 342) É r 343 ∶ = p (É 343) ⋮ Ê r 1022 ∶ = p (Ê 1022) We then distribute the message words and the extra code words among the validators according to their corresponding indices. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 62

I.1. Sets
I.1.1. Regular Notation. N : The set of non-negative integers. Subscript denotes one greater than the maximum. See section 3.4. N + : The set of positive integers (not including zero). N B : The set of balance values. Equivalent to N 2 64. See equation 4.21. N G : The set of unsigned gas values. Equivalent to N 2 64. See equation 4.23. N L : The set of blob length values. Equivalent to N 2 32. See section 3.4. N R : The set of register values. Equivalent to N 2 64. See equation 4.23. N S : The set from which service indices are drawn. Equivalent to N 2 32. See section 9.1. N T : The set of timeslot values. Equivalent to N 2 32. See equation 4.28. Q : The set of rational numbers. Unused. R : The set of real numbers. Unused. Z : The set of integers. Subscript denotes range. See section 3.4. Z G : The set of signed gas values. Equivalent to Z − 2 63 ... 2 63. See equation 4.23. I.1.2. Custom Notation. A : The set of service accounts. See equation 9.3. B : The set of Boolean sequences/bitstrings. Subscript denotes length. See section 3.7. C : The set of seal-key tickets. See equation 6.6. Not used as the set of complex numbers. D : The set of dictionaries. See section 3.5. D ⟨ K → V ⟩ : The set of dictionaries making a partial bijection of domain K to range V. See section 3.5. E : The set of valid Ed25519 signatures. A subset of Y 64. See section 3.8. E K ⟨ M ⟩ : The set of valid Ed25519 signatures of the key K and message M. A subset of E. See section 3.8. F : The set of Bandersnatch signatures. A subset of Y 64. See section 3.8. NOTE: Not used as finite fields. F M K ⟨ C ⟩ : The set of Bandersnatch signatures of the public key K, context C and message M. A subset of F. See section 3.8. ¯ F : The set of Bandersnatch Ring vrf proofs. See section 3.8. ¯ F M R ⟨ C ⟩ : The set of Bandersnatch Ring vrf proofs of the root R, context C and message M. A subset of ¯ F. See section 3.8. G : The set of data segments, equivalent to octet sequences of length W G. See equation 14.1. H : The set of 32-octet cryptographic values. A subset of Y 32. H without a subscript generally implies a hash function result. See section 3.8. NOTE: Not used as quaternions. H B : The set of Bandersnatch public keys. A subset of Y 32. See section 3.8 and appendix G. H E : The set of Ed25519 public keys. A subset of Y 32. See section 3.8.2. I : The set of work items. See equation 14.3. J : The set of work execution errors. K : The set of validator key-sets. See equation 6.7. L : The set of work-digests. M : The set of pvm ram states. See equation 4.24. O : The accumulation operand element, corresponding to a single work-item. P : The set of work-packages. See equation 14.2. S : The set of availability specifications. T : The set of deferred transfers. U : The set of partial state, used during accumulation. See equation 12.13. V μ : The set of validly readable indices for pvm ram μ. See appendix A. V ∗ μ : The set of validly writable indices for pvm ram μ. See appendix A. W : The set of work-reports. X : The set of refinement contexts. Y : The set of octet strings/“blobs”. Subscript denotes length. See section 3.7. Y BLS : The set of BLS public keys. A subset of Y 144. See section 3.8.2. Y R : The set of Bandersnatch ring roots. A subset of Y 144. See section 3.8 and appendix G.

I.2. Functions
∆ : The accumulation function; certain subscripts are used to denote helper functions: ∆ 1 : The single-step accumulation function. ∆ ∗ : The parallel accumulation function. ∆ + : The full sequential accumulation function. Λ : The historical lookup function. See equation 9.7. Ξ : The work-digest computation function. See equation 14.11. Υ : The general state transition function. See equations 4.1, 4.5. Φ : The key-nullifier function. See equation 6.14. JAM: JOIN-ACCUMULATE MACHINE DRAFT 0.6.5 - April 22, 2025 63 Ψ : The whole-program pvm machine state-transition function. See equation A. Ψ 1 : The single-step (pvm) machine state-transition function. See appendix A. Ψ A : The Accumulate pvm invocation function. See appendix B. Ψ H : The host-function invocation (pvm) with host-function marshalling. See appendix A. Ψ I : The Is-Authorized pvm invocation function. See appendix B. Ψ M : The marshalling whole-program pvm machine state-transition function. See appendix A. Ψ R : The Refine pvm invocation function. See appendix B. Ψ T : The On-Transfer pvm invocation function. See appendix B. Ω : Virtual machine host-call functions. See appendix B. Ω A : Assign-core host-call. Ω B : Empower-service host-call. Ω C : Checkpoint host-call. Ω D : Designate-validators host-call. Ω E : Export segment host-call. Ω F : Forget-preimage host-call. Ω G : Gas-remaining host-call. Ω H : Historical-lookup-preimage host-call. Ω I : Information-on-service host-call. Ω J : Eject-service host-call. Ω K : Kickoffpvm host-call. Ω L : Lookup-preimage host-call. Ω M : Makepvm host-call. Ω N : New-service host-call. Ω O : Pokepvm host-call. Ω P : Peekpvm host-call. Ω Q : Query-preimage host-call. Ω R : Read-storage host-call. Ω S : Solicit-preimage host-call. Ω T : Transfer host-call. Ω U : Upgrade-service host-call. Ω V : Void innerpvm memory host-call. Ω W : Write-storage host-call. Ω X : Expungepvm host-call. Ω Y : Fetch data host-call. Ω Z : Zero innerpvm memory host-call. Ω Q : Yield accumulation trie result host-call.